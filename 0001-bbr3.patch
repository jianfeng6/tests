From aa2165cbb8e235becd5e3f2696b1d878b5e29010 Mon Sep 17 00:00:00 2001
From: ferstar <zhangjianfei3@gmail.com>
Date: Mon, 4 Nov 2024 09:59:35 +0800
Subject: [PATCH] bbr3:
 https://github.com/zxcvbnmv/testnano/tree/main/PATCH/BBRv3

---
 Documentation/networking/ip-sysctl.rst |   75 +
 arch/arm64/configs/gki_defconfig       |   12 +-
 include/linux/netdevice.h              |    4 +-
 include/linux/random.h                 |   40 +
 include/net/inet_connection_sock.h     |    2 +-
 include/net/tcp.h                      |   59 +-
 include/uapi/linux/inet_diag.h         |   40 +-
 include/uapi/linux/rtnetlink.h         |    4 +-
 include/uapi/linux/tcp.h               |    1 +
 net/bpf/test_run.c                     |    2 +-
 net/core/dev.c                         |    2 +-
 net/ipv4/Gconfig.bbr2                  |    2 +-
 net/ipv4/Kconfig                       |   37 +-
 net/ipv4/Makefile                      |    2 +-
 net/ipv4/tcp.c                         |    2 +
 net/ipv4/tcp_bbr.c                     | 2224 +++++++++++++++-----
 net/ipv4/tcp_bbr2.c                    | 2692 ------------------------
 net/ipv4/tcp_ipv4.c                    |    3 +-
 net/ipv4/tcp_minisocks.c               |    2 +
 net/ipv4/tcp_output.c                  |   31 +-
 net/ipv4/tcp_plb.c                     |   53 +-
 net/ipv4/tcp_rate.c                    |    4 +
 net/sctp/output.c                      |    3 +-
 23 files changed, 2001 insertions(+), 3295 deletions(-)
 delete mode 100644 net/ipv4/tcp_bbr2.c

diff --git a/Documentation/networking/ip-sysctl.rst b/Documentation/networking/ip-sysctl.rst
index 7806a695b962..56efcc246af7 100644
--- a/Documentation/networking/ip-sysctl.rst
+++ b/Documentation/networking/ip-sysctl.rst
@@ -980,6 +980,81 @@ tcp_plb_suspend_rto_sec - INTEGER
 
 	Default: 60
 
+tcp_plb_enabled - BOOLEAN
+	If set and the underlying congestion control (e.g. DCTCP) supports
+	and enables PLB feature, TCP PLB (Protective Load Balancing) is
+	enabled. PLB is described in the following paper:
+	https://doi.org/10.1145/3544216.3544226. Based on PLB parameters,
+	upon sensing sustained congestion, TCP triggers a change in
+	flow label field for outgoing IPv6 packets. A change in flow label
+	field potentially changes the path of outgoing packets for switches
+	that use ECMP/WCMP for routing.
+
+	PLB changes socket txhash which results in a change in IPv6 Flow Label
+	field, and currently no-op for IPv4 headers. It is possible
+	to apply PLB for IPv4 with other network header fields (e.g. TCP
+	or IPv4 options) or using encapsulation where outer header is used
+	by switches to determine next hop. In either case, further host
+	and switch side changes will be needed.
+
+	When set, PLB assumes that congestion signal (e.g. ECN) is made
+	available and used by congestion control module to estimate a
+	congestion measure (e.g. ce_ratio). PLB needs a congestion measure to
+	make repathing decisions.
+
+	Default: FALSE
+
+tcp_plb_idle_rehash_rounds - INTEGER
+	Number of consecutive congested rounds (RTT) seen after which
+	a rehash can be performed, given there are no packets in flight.
+	This is referred to as M in PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	Possible Values: 0 - 31
+
+	Default: 3
+
+tcp_plb_rehash_rounds - INTEGER
+	Number of consecutive congested rounds (RTT) seen after which
+	a forced rehash can be performed. Be careful when setting this
+	parameter, as a small value increases the risk of retransmissions.
+	This is referred to as N in PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	Possible Values: 0 - 31
+
+	Default: 12
+
+tcp_plb_suspend_rto_sec - INTEGER
+	Time, in seconds, to suspend PLB in event of an RTO. In order to avoid
+	having PLB repath onto a connectivity "black hole", after an RTO a TCP
+	connection suspends PLB repathing for a random duration between 1x and
+	2x of this parameter. Randomness is added to avoid concurrent rehashing
+	of multiple TCP connections. This should be set corresponding to the
+	amount of time it takes to repair a failed link.
+
+	Possible Values: 0 - 255
+
+	Default: 60
+
+tcp_plb_cong_thresh - INTEGER
+	Fraction of packets marked with congestion over a round (RTT) to
+	tag that round as congested. This is referred to as K in the PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	The 0-1 fraction range is mapped to 0-256 range to avoid floating
+	point operations. For example, 128 means that if at least 50% of
+	the packets in a round were marked as congested then the round
+	will be tagged as congested.
+
+	Setting threshold to 0 means that PLB repaths every RTT regardless
+	of congestion. This is not intended behavior for PLB and should be
+	used only for experimentation purpose.
+
+	Possible Values: 0 - 256
+
+	Default: 128
+
 UDP variables
 =============
 
diff --git a/arch/arm64/configs/gki_defconfig b/arch/arm64/configs/gki_defconfig
index 4eafc1f13b5b..8301e0d338f8 100644
--- a/arch/arm64/configs/gki_defconfig
+++ b/arch/arm64/configs/gki_defconfig
@@ -1025,9 +1025,9 @@ CONFIG_INET_UDP_DIAG=y
 CONFIG_INET_DIAG_DESTROY=y
 # CONFIG_TCP_CONG_ADVANCED is not set
 CONFIG_TCP_CONG_BRUTAL=y
-CONFIG_TCP_CONG_BBR2=y
+CONFIG_TCP_CONG_BBR=y
 CONFIG_TCP_CONG_CUBIC=y
-CONFIG_DEFAULT_TCP_CONG="cubic"
+CONFIG_DEFAULT_TCP_CONG="bbr"
 # CONFIG_TCP_MD5SIG is not set
 CONFIG_IPV6=y
 CONFIG_IPV6_ROUTER_PREF=y
@@ -1416,7 +1416,13 @@ CONFIG_NET_SCH_FQ=y
 CONFIG_NET_SCH_INGRESS=y
 # CONFIG_NET_SCH_PLUG is not set
 # CONFIG_NET_SCH_ETS is not set
-# CONFIG_NET_SCH_DEFAULT is not set
+CONFIG_NET_SCH_DEFAULT=y
+# CONFIG_DEFAULT_FQ is not set
+# CONFIG_DEFAULT_CODEL is not set
+CONFIG_DEFAULT_FQ_CODEL=y
+# CONFIG_DEFAULT_SFQ is not set
+# CONFIG_DEFAULT_PFIFO_FAST is not set
+CONFIG_DEFAULT_NET_SCH="fq_codel"
 
 #
 # Classification
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index a347061c01ef..92edda0dd29f 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2170,7 +2170,9 @@ struct net_device {
 	const struct rtnl_link_ops *rtnl_link_ops;
 
 	/* for setting kernel sock attribute on TCP connection setup */
-#define GSO_MAX_SIZE		65536
+#define GSO_LEGACY_MAX_SIZE	65536u
+#define GSO_MAX_SIZE		UINT_MAX
+
 	unsigned int		gso_max_size;
 #define GSO_MAX_SEGS		65535
 	u16			gso_max_segs;
diff --git a/include/linux/random.h b/include/linux/random.h
index f45b8be3e3c4..0b684f9aa99f 100644
--- a/include/linux/random.h
+++ b/include/linux/random.h
@@ -64,6 +64,46 @@ static inline unsigned long get_random_long(void)
 #endif
 }
 
+u32 __get_random_u32_below(u32 ceil);
+
+/*
+ * Returns a random integer in the interval [0, ceil), with uniform
+ * distribution, suitable for all uses. Fastest when ceil is a constant, but
+ * still fast for variable ceil as well.
+ */
+static inline u32 get_random_u32_below(u32 ceil)
+{
+	if (!__builtin_constant_p(ceil))
+		return __get_random_u32_below(ceil);
+
+	/*
+	 * For the fast path, below, all operations on ceil are precomputed by
+	 * the compiler, so this incurs no overhead for checking pow2, doing
+	 * divisions, or branching based on integer size. The resultant
+	 * algorithm does traditional reciprocal multiplication (typically
+	 * optimized by the compiler into shifts and adds), rejecting samples
+	 * whose lower half would indicate a range indivisible by ceil.
+	 */
+	BUILD_BUG_ON_MSG(!ceil, "get_random_u32_below() must take ceil > 0");
+	if (ceil <= 1)
+		return 0;
+	for (;;) {
+		if (ceil <= 1U << 8) {
+			u32 mult = ceil * (get_random_u32() & 0xff);
+			if (likely(is_power_of_2(ceil) || (u8)mult >= (1U << 8) % ceil))
+				return mult >> 8;
+		} else if (ceil <= 1U << 16) {
+			u32 mult = ceil * (get_random_u32() & 0xffff);
+			if (likely(is_power_of_2(ceil) || (u16)mult >= (1U << 16) % ceil))
+				return mult >> 16;
+		} else {
+			u64 mult = (u64)ceil * get_random_u32();
+			if (likely(is_power_of_2(ceil) || (u32)mult >= -ceil % ceil))
+				return mult >> 32;
+		}
+	}
+}
+
 /*
  * On 64-bit architectures, protect against non-terminated C string overflows
  * by zeroing out the first byte of the canary; this leaves 56 bits of entropy.
diff --git a/include/net/inet_connection_sock.h b/include/net/inet_connection_sock.h
index a126c94607dd..17b47e1277fc 100644
--- a/include/net/inet_connection_sock.h
+++ b/include/net/inet_connection_sock.h
@@ -140,7 +140,7 @@ struct inet_connection_sock {
 	ANDROID_KABI_RESERVE(1);
 
 /* XXX inflated by temporary internal debugging info */
-#define ICSK_CA_PRIV_SIZE      (224)
+#define ICSK_CA_PRIV_SIZE      (144)
 	u64			  icsk_ca_priv[ICSK_CA_PRIV_SIZE / sizeof(u64)];
 };
 
diff --git a/include/net/tcp.h b/include/net/tcp.h
index bcd2eff74fbc..702ba35e7740 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -368,7 +368,8 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 #define	TCP_ECN_QUEUE_CWR	2
 #define	TCP_ECN_DEMAND_CWR	4
 #define	TCP_ECN_SEEN		8
-#define	TCP_ECN_ECT_PERMANENT	16
+#define	TCP_ECN_LOW		16
+#define	TCP_ECN_ECT_PERMANENT	32
 
 enum tcp_tw_status {
 	TCP_TW_SUCCESS = 0,
@@ -702,6 +703,15 @@ static inline void tcp_fast_path_check(struct sock *sk)
 		tcp_fast_path_on(tp);
 }
 
+static inline void tcp_set_ecn_low_from_dst(struct sock *sk,
+					    const struct dst_entry *dst)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (dst_feature(dst, RTAX_FEATURE_ECN_LOW))
+		tp->ecn_flags |= TCP_ECN_LOW;
+}
+
 /* Compute the actual rto_min value */
 static inline u32 tcp_rto_min(struct sock *sk)
 {
@@ -1199,6 +1209,21 @@ void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
 		  bool is_sack_reneg, struct rate_sample *rs);
 void tcp_rate_check_app_limited(struct sock *sk);
 
+/* If a retransmit failed due to local qdisc congestion or other local issues,
+ * then we may have called tcp_set_skb_tso_segs() to increase the number of
+ * segments in the skb without increasing the tx.in_flight. In all other cases,
+ * the tx.in_flight should be at least as big as the pcount of the sk_buff.  We
+ * do not have the state to know whether a retransmit failed due to local qdisc
+ * congestion or other local issues, so to avoid spurious warnings we consider
+ * that any skb marked lost may have suffered that fate.
+ */
+static inline bool tcp_skb_tx_in_flight_is_suspicious(u32 skb_pcount,
+						      u32 skb_sacked_flags,
+						      u32 tx_in_flight)
+{
+	return (skb_pcount > tx_in_flight) && !(skb_sacked_flags & TCPCB_LOST);
+}
+
 /* These functions determine how the current flow behaves in respect of SACK
  * handling. SACK is negotiated with the peer, and therefore it can vary
  * between different flows.
@@ -1242,6 +1267,17 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 
 #define TCP_INFINITE_SSTHRESH	0x7fffffff
 
+static inline u32 tcp_snd_cwnd(const struct tcp_sock *tp)
+{
+	return tp->snd_cwnd;
+}
+
+static inline void tcp_snd_cwnd_set(struct tcp_sock *tp, u32 val)
+{
+	WARN_ON_ONCE((int)val <= 0);
+	tp->snd_cwnd = val;
+}
+
 static inline bool tcp_in_slow_start(const struct tcp_sock *tp)
 {
 	return tp->snd_cwnd < tp->snd_ssthresh;
@@ -2120,16 +2156,27 @@ extern void tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs);
 
 /* tcp_plb.c */
 
-#define TCP_PLB_SCALE 8	/* scaling factor for fractions in PLB (e.g. ce_ratio) */
+/*
+ * Scaling factor for fractions in PLB. For example, tcp_plb_update_state
+ * expects cong_ratio which represents fraction of traffic that experienced
+ * congestion over a single RTT. In order to avoid floating point operations,
+ * this fraction should be mapped to (1 << TCP_PLB_SCALE) and passed in.
+ */
+#define TCP_PLB_SCALE 8
 
 /* State for PLB (Protective Load Balancing) for a single TCP connection. */
 struct tcp_plb_state {
 	u8	consec_cong_rounds:5, /* consecutive congested rounds */
-		enabled:1,	/* Check if PLB is enabled */
-		unused:2;
-	u32	pause_until; /* jiffies32 when PLB can resume repathing */
-};
+		unused:3;
+	u32	pause_until; /* jiffies32 when PLB can resume rerouting */
+} __attribute__ ((__packed__));
 
+static inline void tcp_plb_init(const struct sock *sk,
+				struct tcp_plb_state *plb)
+{
+	plb->consec_cong_rounds = 0;
+	plb->pause_until = 0;
+}
 void tcp_plb_update_state(const struct sock *sk, struct tcp_plb_state *plb,
 			  const int cong_ratio);
 void tcp_plb_check_rehash(struct sock *sk, struct tcp_plb_state *plb);
diff --git a/include/uapi/linux/inet_diag.h b/include/uapi/linux/inet_diag.h
index 96d52dd9c48a..06fd41ea1a8c 100644
--- a/include/uapi/linux/inet_diag.h
+++ b/include/uapi/linux/inet_diag.h
@@ -229,27 +229,6 @@ struct tcp_bbr_info {
 	__u32	bbr_min_rtt;		/* min-filtered RTT in uSec */
 	__u32	bbr_pacing_gain;	/* pacing gain shifted left 8 bits */
 	__u32	bbr_cwnd_gain;		/* cwnd gain shifted left 8 bits */
-};
-
-/* Phase as reported in netlink/ss stats. */
-enum tcp_bbr2_phase {
-	BBR2_PHASE_INVALID		= 0,
-	BBR2_PHASE_STARTUP		= 1,
-	BBR2_PHASE_DRAIN		= 2,
-	BBR2_PHASE_PROBE_RTT		= 3,
-	BBR2_PHASE_PROBE_BW_UP		= 4,
-	BBR2_PHASE_PROBE_BW_DOWN	= 5,
-	BBR2_PHASE_PROBE_BW_CRUISE	= 6,
-	BBR2_PHASE_PROBE_BW_REFILL	= 7
-};
-
-struct tcp_bbr2_info {
-	/* u64 bw: bandwidth (app throughput) estimate in Byte per sec: */
-	__u32	bbr_bw_lsb;		/* lower 32 bits of bw */
-	__u32	bbr_bw_msb;		/* upper 32 bits of bw */
-	__u32	bbr_min_rtt;		/* min-filtered RTT in uSec */
-	__u32	bbr_pacing_gain;	/* pacing gain shifted left 8 bits */
-	__u32	bbr_cwnd_gain;		/* cwnd gain shifted left 8 bits */
 	__u32	bbr_bw_hi_lsb;		/* lower 32 bits of bw_hi */
 	__u32	bbr_bw_hi_msb;		/* upper 32 bits of bw_hi */
 	__u32	bbr_bw_lo_lsb;		/* lower 32 bits of bw_lo */
@@ -257,16 +236,27 @@ struct tcp_bbr2_info {
 	__u8	bbr_mode;		/* current bbr_mode in state machine */
 	__u8	bbr_phase;		/* current state machine phase */
 	__u8	unused1;		/* alignment padding; not used yet */
-	__u8	bbr_version;		/* MUST be at this offset in struct */
-	__u32	bbr_inflight_lo;	/* lower/short-term data volume bound */
-	__u32	bbr_inflight_hi;	/* higher/long-term data volume bound */
+	__u8	bbr_version;		/* BBR algorithm version */
+	__u32	bbr_inflight_lo;	/* lower short-term data volume bound */
+	__u32	bbr_inflight_hi;	/* higher long-term data volume bound */
 	__u32	bbr_extra_acked;	/* max excess packets ACKed in epoch */
 };
 
+/* TCP BBR congestion control bbr_phase as reported in netlink/ss stats. */
+enum tcp_bbr_phase {
+	BBR_PHASE_INVALID		= 0,
+	BBR_PHASE_STARTUP		= 1,
+	BBR_PHASE_DRAIN			= 2,
+	BBR_PHASE_PROBE_RTT		= 3,
+	BBR_PHASE_PROBE_BW_UP		= 4,
+	BBR_PHASE_PROBE_BW_DOWN		= 5,
+	BBR_PHASE_PROBE_BW_CRUISE	= 6,
+	BBR_PHASE_PROBE_BW_REFILL	= 7,
+};
+
 union tcp_cc_info {
 	struct tcpvegas_info	vegas;
 	struct tcp_dctcp_info	dctcp;
 	struct tcp_bbr_info	bbr;
-	struct tcp_bbr2_info	bbr2;
 };
 #endif /* _UAPI_INET_DIAG_H_ */
diff --git a/include/uapi/linux/rtnetlink.h b/include/uapi/linux/rtnetlink.h
index 9b814c92de12..0f7bff0c6e98 100644
--- a/include/uapi/linux/rtnetlink.h
+++ b/include/uapi/linux/rtnetlink.h
@@ -482,9 +482,11 @@ enum {
 #define RTAX_FEATURE_SACK	(1 << 1)
 #define RTAX_FEATURE_TIMESTAMP	(1 << 2)
 #define RTAX_FEATURE_ALLFRAG	(1 << 3)
+#define RTAX_FEATURE_ECN_LOW	(1 << 4)
 
 #define RTAX_FEATURE_MASK	(RTAX_FEATURE_ECN | RTAX_FEATURE_SACK | \
-				 RTAX_FEATURE_TIMESTAMP | RTAX_FEATURE_ALLFRAG)
+				 RTAX_FEATURE_TIMESTAMP | RTAX_FEATURE_ALLFRAG \
+				 | RTAX_FEATURE_ECN_LOW)
 
 struct rta_session {
 	__u8	proto;
diff --git a/include/uapi/linux/tcp.h b/include/uapi/linux/tcp.h
index cfcb10b75483..1e7922d10167 100644
--- a/include/uapi/linux/tcp.h
+++ b/include/uapi/linux/tcp.h
@@ -170,6 +170,7 @@ enum tcp_fastopen_client_fail {
 #define TCPI_OPT_ECN		8 /* ECN was negociated at TCP session init */
 #define TCPI_OPT_ECN_SEEN	16 /* we received at least one packet with ECT */
 #define TCPI_OPT_SYN_DATA	32 /* SYN-ACK acked data in SYN sent or rcvd */
+#define TCPI_OPT_ECN_LOW	64 /* Low-latency ECN configured at init */
 
 /*
  * Sender's congestion state indicating normal or abnormal situations
diff --git a/net/bpf/test_run.c b/net/bpf/test_run.c
index 72d424a5a142..1d673fcda2b8 100644
--- a/net/bpf/test_run.c
+++ b/net/bpf/test_run.c
@@ -452,7 +452,7 @@ static int convert___skb_to_skb(struct sk_buff *skb, struct __sk_buff *__skb)
 		cb->pkt_len = skb->len;
 	} else {
 		if (__skb->wire_len < skb->len ||
-		    __skb->wire_len > GSO_MAX_SIZE)
+		    __skb->wire_len > GSO_LEGACY_MAX_SIZE)
 			return -EINVAL;
 		cb->pkt_len = __skb->wire_len;
 	}
diff --git a/net/core/dev.c b/net/core/dev.c
index e8449d3cc8d6..e0b6f5b7c6d3 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -10625,7 +10625,7 @@ struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
 
 	dev_net_set(dev, &init_net);
 
-	dev->gso_max_size = GSO_MAX_SIZE;
+	dev->gso_max_size = GSO_LEGACY_MAX_SIZE;
 	dev->gso_max_segs = GSO_MAX_SEGS;
 	dev->upper_level = 1;
 	dev->lower_level = 1;
diff --git a/net/ipv4/Gconfig.bbr2 b/net/ipv4/Gconfig.bbr2
index 1cdc3953f2be..b5ffe0f620ba 100644
--- a/net/ipv4/Gconfig.bbr2
+++ b/net/ipv4/Gconfig.bbr2
@@ -1 +1 @@
-CONFIG_TCP_CONG_BBR2=y
+CONFIG_TCP_CONG_BBR=y
diff --git a/net/ipv4/Kconfig b/net/ipv4/Kconfig
index c4a5fe1b9ce4..4c5b1608748a 100644
--- a/net/ipv4/Kconfig
+++ b/net/ipv4/Kconfig
@@ -659,22 +659,25 @@ config TCP_CONG_BBR
 	default n
 	help
 
-	  BBR (Bottleneck Bandwidth and RTT) TCP congestion control aims to
-	  maximize network utilization and minimize queues. It builds an explicit
-	  model of the bottleneck delivery rate and path round-trip propagation
-	  delay. It tolerates packet loss and delay unrelated to congestion. It
-	  can operate over LAN, WAN, cellular, wifi, or cable modem links. It can
-	  coexist with flows that use loss-based congestion control, and can
-	  operate with shallow buffers, deep buffers, bufferbloat, policers, or
-	  AQM schemes that do not provide a delay signal. It requires the fq
-	  ("Fair Queue") pacing packet scheduler.
-
-config TCP_CONG_BBR2
-	tristate "BBR2 TCP"
+	  BBR (Bottleneck Bandwidth and RTT) TCP congestion control is a
+	  model-based congestion control algorithm that aims to maximize
+	  network utilization, keep queues and retransmit rates low, and to be
+	  able to coexist with Reno/CUBIC in common scenarios. It builds an
+	  explicit model of the network path.  It tolerates a targeted degree
+	  of random packet loss and delay. It can operate over LAN, WAN,
+	  cellular, wifi, or cable modem links, and can use shallow-threshold
+	  ECN signals. It can coexist to some degree with flows that use
+	  loss-based congestion control, and can operate with shallow buffers,
+	  deep buffers, bufferbloat, policers, or AQM schemes that do not
+	  provide a delay signal. It requires pacing, using either TCP internal
+	  pacing or the fq ("Fair Queue") pacing packet scheduler.
+
+config TCP_CONG_BBR
+	tristate "BBR TCP"
 	default y
 	help
 
-	BBR2 TCP congestion control is a model-based congestion control
+	BBR TCP congestion control is a model-based congestion control
 	algorithm that aims to maximize network utilization, keep queues and
 	retransmit rates low, and to be able to coexist with Reno/CUBIC in
 	common scenarios. It builds an explicit model of the network path.  It
@@ -703,7 +706,7 @@ config TCP_CONG_BRUTAL
 
 choice
 	prompt "Default TCP congestion control"
-	default DEFAULT_BBR2
+	default DEFAULT_BBR
 	help
 	  Select the TCP congestion control that will be used by default
 	  for all connections.
@@ -738,9 +741,6 @@ choice
 	config DEFAULT_BBR
 		bool "BBR" if TCP_CONG_BBR=y
 
-	config DEFAULT_BBR2
-		bool "BBR2" if TCP_CONG_BBR2=y
-
 	config DEFAULT_BRUTAL
 		bool "BRUTAL" if TCP_CONG_BRUTAL=y
 
@@ -768,9 +768,8 @@ config DEFAULT_TCP_CONG
 	default "dctcp" if DEFAULT_DCTCP
 	default "cdg" if DEFAULT_CDG
 	default "bbr" if DEFAULT_BBR
-	default "bbr2" if DEFAULT_BBR2
 	default "brutal" if DEFAULT_BRUTAL
-	default "bbr2"
+	default "bbr"
 
 config TCP_MD5SIG
 	bool "TCP: MD5 Signature Option support (RFC2385)"
diff --git a/net/ipv4/Makefile b/net/ipv4/Makefile
index ed43f95c858f..2c404e4fc34a 100644
--- a/net/ipv4/Makefile
+++ b/net/ipv4/Makefile
@@ -16,7 +16,7 @@ obj-y     := route.o inetpeer.o protocol.o \
 	     inet_fragment.o ping.o ip_tunnel_core.o gre_offload.o \
 	     metrics.o netlink.o nexthop.o udp_tunnel_stub.o
 
-obj-y += tcp_bbr2.o
+obj-y += tcp_bbr.o
 obj-y += tcp_brutal.o
 
 obj-$(CONFIG_BPFILTER) += bpfilter/
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 9e2b44ffd802..bfb09cdd3781 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -3447,6 +3447,8 @@ void tcp_get_info(struct sock *sk, struct tcp_info *info)
 		info->tcpi_options |= TCPI_OPT_ECN;
 	if (tp->ecn_flags & TCP_ECN_SEEN)
 		info->tcpi_options |= TCPI_OPT_ECN_SEEN;
+	if (tp->ecn_flags & TCP_ECN_LOW)
+		info->tcpi_options |= TCPI_OPT_ECN_LOW;
 	if (tp->syn_data_acked)
 		info->tcpi_options |= TCPI_OPT_SYN_DATA;
 
diff --git a/net/ipv4/tcp_bbr.c b/net/ipv4/tcp_bbr.c
index c0d5a4211fc1..32b13d05c535 100644
--- a/net/ipv4/tcp_bbr.c
+++ b/net/ipv4/tcp_bbr.c
@@ -1,18 +1,19 @@
-/* Bottleneck Bandwidth and RTT (BBR) congestion control
+/* BBR (Bottleneck Bandwidth and RTT) congestion control
  *
- * BBR congestion control computes the sending rate based on the delivery
- * rate (throughput) estimated from ACKs. In a nutshell:
+ * BBR is a model-based congestion control algorithm that aims for low queues,
+ * low loss, and (bounded) Reno/CUBIC coexistence. To maintain a model of the
+ * network path, it uses measurements of bandwidth and RTT, as well as (if they
+ * occur) packet loss and/or shallow-threshold ECN signals. Note that although
+ * it can use ECN or loss signals explicitly, it does not require either; it
+ * can bound its in-flight data based on its estimate of the BDP.
  *
- *   On each ACK, update our model of the network path:
- *      bottleneck_bandwidth = windowed_max(delivered / elapsed, 10 round trips)
- *      min_rtt = windowed_min(rtt, 10 seconds)
- *   pacing_rate = pacing_gain * bottleneck_bandwidth
- *   cwnd = max(cwnd_gain * bottleneck_bandwidth * min_rtt, 4)
- *
- * The core algorithm does not react directly to packet losses or delays,
- * although BBR may adjust the size of next send per ACK when loss is
- * observed, or adjust the sending rate if it estimates there is a
- * traffic policer, in order to keep the drop rate reasonable.
+ * The model has both higher and lower bounds for the operating range:
+ *   lo: bw_lo, inflight_lo: conservative short-term lower bound
+ *   hi: bw_hi, inflight_hi: robust long-term upper bound
+ * The bandwidth-probing time scale is (a) extended dynamically based on
+ * estimated BDP to improve coexistence with Reno/CUBIC; (b) bounded by
+ * an interactive wall-clock time-scale to be more scalable and responsive
+ * than Reno and CUBIC.
  *
  * Here is a state transition diagram for BBR:
  *
@@ -63,6 +64,13 @@
 #include <linux/random.h>
 #include <linux/win_minmax.h>
 
+#include <trace/events/tcp.h>
+#include "tcp_dctcp.h"
+
+#define BBR_VERSION		3
+
+#define bbr_param(sk,name)	(bbr_ ## name)
+
 /* Scale factor for rate in pkt/uSec unit to avoid truncation in bandwidth
  * estimation. The rate unit ~= (1500 bytes / 1 usec / 2^24) ~= 715 bps.
  * This handles bandwidths from 0.06pps (715bps) to 256Mpps (3Tbps) in a u32.
@@ -83,36 +91,41 @@ enum bbr_mode {
 	BBR_PROBE_RTT,	/* cut inflight to min to probe min_rtt */
 };
 
+/* How does the incoming ACK stream relate to our bandwidth probing? */
+enum bbr_ack_phase {
+	BBR_ACKS_INIT,		  /* not probing; not getting probe feedback */
+	BBR_ACKS_REFILLING,	  /* sending at est. bw to fill pipe */
+	BBR_ACKS_PROBE_STARTING,  /* inflight rising to probe bw */
+	BBR_ACKS_PROBE_FEEDBACK,  /* getting feedback from bw probing */
+	BBR_ACKS_PROBE_STOPPING,  /* stopped probing; still getting feedback */
+};
+
 /* BBR congestion control block */
 struct bbr {
 	u32	min_rtt_us;	        /* min RTT in min_rtt_win_sec window */
 	u32	min_rtt_stamp;	        /* timestamp of min_rtt_us */
 	u32	probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
-	struct minmax bw;	/* Max recent delivery rate in pkts/uS << 24 */
-	u32	rtt_cnt;	    /* count of packet-timed rounds elapsed */
+	u32	probe_rtt_min_us;	/* min RTT in probe_rtt_win_ms win */
+	u32	probe_rtt_min_stamp;	/* timestamp of probe_rtt_min_us*/
 	u32     next_rtt_delivered; /* scb->tx.delivered at end of round */
 	u64	cycle_mstamp;	     /* time of this cycle phase start */
-	u32     mode:3,		     /* current bbr_mode in state machine */
+	u32     mode:2,		     /* current bbr_mode in state machine */
 		prev_ca_state:3,     /* CA state on previous ACK */
-		packet_conservation:1,  /* use packet conservation? */
 		round_start:1,	     /* start of packet-timed tx->ack round? */
+		ce_state:1,          /* If most recent data has CE bit set */
+		bw_probe_up_rounds:5,   /* cwnd-limited rounds in PROBE_UP */
+		try_fast_path:1,	/* can we take fast path? */
 		idle_restart:1,	     /* restarting after idle? */
 		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
-		unused:13,
-		lt_is_sampling:1,    /* taking long-term ("LT") samples now? */
-		lt_rtt_cnt:7,	     /* round trips in long-term interval */
-		lt_use_bw:1;	     /* use lt_bw as our bw estimate? */
-	u32	lt_bw;		     /* LT est delivery rate in pkts/uS << 24 */
-	u32	lt_last_delivered;   /* LT intvl start: tp->delivered */
-	u32	lt_last_stamp;	     /* LT intvl start: tp->delivered_mstamp */
-	u32	lt_last_lost;	     /* LT intvl start: tp->lost */
+		init_cwnd:7,         /* initial cwnd */
+		unused_1:10;
 	u32	pacing_gain:10,	/* current gain for setting pacing rate */
 		cwnd_gain:10,	/* current gain for setting cwnd */
 		full_bw_reached:1,   /* reached full bw in Startup? */
 		full_bw_cnt:2,	/* number of rounds without large bw gains */
-		cycle_idx:3,	/* current index in pacing_gain cycle array */
+		cycle_idx:2,	/* current index in pacing_gain cycle array */
 		has_seen_rtt:1, /* have we seen an RTT sample yet? */
-		unused_b:5;
+		unused_2:6;
 	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */
 	u32	full_bw;	/* recent bw, to estimate if pipe is full */
 
@@ -122,19 +135,67 @@ struct bbr {
 	u32	ack_epoch_acked:20,	/* packets (S)ACKed in sampling epoch */
 		extra_acked_win_rtts:5,	/* age of extra_acked, in round trips */
 		extra_acked_win_idx:1,	/* current index in extra_acked array */
-		unused_c:6;
+	/* BBR v3 state: */
+		full_bw_now:1,		/* recently reached full bw plateau? */
+		startup_ecn_rounds:2,	/* consecutive hi ECN STARTUP rounds */
+		loss_in_cycle:1,	/* packet loss in this cycle? */
+		ecn_in_cycle:1,		/* ECN in this cycle? */
+		unused_3:1;
+	u32	loss_round_delivered; /* scb->tx.delivered ending loss round */
+	u32	undo_bw_lo;	     /* bw_lo before latest losses */
+	u32	undo_inflight_lo;    /* inflight_lo before latest losses */
+	u32	undo_inflight_hi;    /* inflight_hi before latest losses */
+	u32	bw_latest;	 /* max delivered bw in last round trip */
+	u32	bw_lo;		 /* lower bound on sending bandwidth */
+	u32	bw_hi[2];	 /* max recent measured bw sample */
+	u32	inflight_latest; /* max delivered data in last round trip */
+	u32	inflight_lo;	 /* lower bound of inflight data range */
+	u32	inflight_hi;	 /* upper bound of inflight data range */
+	u32	bw_probe_up_cnt; /* packets delivered per inflight_hi incr */
+	u32	bw_probe_up_acks;  /* packets (S)ACKed since inflight_hi incr */
+	u32	probe_wait_us;	 /* PROBE_DOWN until next clock-driven probe */
+	u32	prior_rcv_nxt;	/* tp->rcv_nxt when CE state last changed */
+	u32	ecn_eligible:1,	/* sender can use ECN (RTT, handshake)? */
+		ecn_alpha:9,	/* EWMA delivered_ce/delivered; 0..256 */
+		bw_probe_samples:1,    /* rate samples reflect bw probing? */
+		prev_probe_too_high:1, /* did last PROBE_UP go too high? */
+		stopped_risky_probe:1, /* last PROBE_UP stopped due to risk? */
+		rounds_since_probe:8,  /* packet-timed rounds since probed bw */
+		loss_round_start:1,    /* loss_round_delivered round trip? */
+		loss_in_round:1,       /* loss marked in this round trip? */
+		ecn_in_round:1,	       /* ECN marked in this round trip? */
+		ack_phase:3,	       /* bbr_ack_phase: meaning of ACKs */
+		loss_events_in_round:4,/* losses in STARTUP round */
+		initialized:1;	       /* has bbr_init() been called? */
+	u32	alpha_last_delivered;	 /* tp->delivered    at alpha update */
+	u32	alpha_last_delivered_ce; /* tp->delivered_ce at alpha update */
+
+	u8	unused_4;		/* to preserve alignment */
+	struct tcp_plb_state plb;
 };
 
-#define CYCLE_LEN	8	/* number of phases in a pacing gain cycle */
+struct bbr_context {
+	u32 sample_bw;
+};
 
-/* Window length of bw filter (in rounds): */
-static const int bbr_bw_rtts = CYCLE_LEN + 2;
 /* Window length of min_rtt filter (in sec): */
 static const u32 bbr_min_rtt_win_sec = 10;
 /* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode: */
 static const u32 bbr_probe_rtt_mode_ms = 200;
-/* Skip TSO below the following bandwidth (bits/sec): */
-static const int bbr_min_tso_rate = 1200000;
+/* Window length of probe_rtt_min_us filter (in ms), and consequently the
+ * typical interval between PROBE_RTT mode entries. The default is 5000ms.
+ * Note that bbr_probe_rtt_win_ms must be <= bbr_min_rtt_win_sec * MSEC_PER_SEC
+ */
+static const u32 bbr_probe_rtt_win_ms = 5000;
+/* Proportion of cwnd to estimated BDP in PROBE_RTT, in units of BBR_UNIT: */
+static const u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2;
+
+/* Use min_rtt to help adapt TSO burst size, with smaller min_rtt resulting
+ * in bigger TSO bursts. We cut the RTT-based allowance in half
+ * for every 2^9 usec (aka 512 us) of RTT, so that the RTT-based allowance
+ * is below 1500 bytes after 6 * ~500 usec = 3ms.
+ */
+static const u32 bbr_tso_rtt_shift = 9;
 
 /* Pace at ~1% below estimated bw, on average, to reduce queue at bottleneck.
  * In order to help drive the network toward lower queues and low latency while
@@ -144,13 +205,15 @@ static const int bbr_min_tso_rate = 1200000;
  */
 static const int bbr_pacing_margin_percent = 1;
 
-/* We use a high_gain value of 2/ln(2) because it's the smallest pacing gain
+/* We use a startup_pacing_gain of 4*ln(2) because it's the smallest value
  * that will allow a smoothly increasing pacing rate that will double each RTT
  * and send the same number of packets per RTT that an un-paced, slow-starting
  * Reno or CUBIC flow would:
  */
-static const int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1;
-/* The pacing gain of 1/high_gain in BBR_DRAIN is calculated to typically drain
+static const int bbr_startup_pacing_gain = BBR_UNIT * 277 / 100 + 1;
+/* The gain for deriving startup cwnd: */
+static const int bbr_startup_cwnd_gain = BBR_UNIT * 2;
+/* The pacing gain in BBR_DRAIN is calculated to typically drain
  * the queue created in BBR_STARTUP in a single round:
  */
 static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
@@ -158,13 +221,17 @@ static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
 static const int bbr_cwnd_gain  = BBR_UNIT * 2;
 /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
 static const int bbr_pacing_gain[] = {
-	BBR_UNIT * 5 / 4,	/* probe for more available bw */
-	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
+	BBR_UNIT * 5 / 4,	/* UP: probe for more available bw */
+	BBR_UNIT * 91 / 100,	/* DOWN: drain queue and/or yield bw */
+	BBR_UNIT,		/* CRUISE: try to use pipe w/ some headroom */
+	BBR_UNIT,		/* REFILL: refill pipe to estimated 100% */
+};
+enum bbr_pacing_gain_phase {
+	BBR_BW_PROBE_UP		= 0,  /* push up inflight to probe for bw/vol */
+	BBR_BW_PROBE_DOWN	= 1,  /* drain excess inflight from the queue */
+	BBR_BW_PROBE_CRUISE	= 2,  /* use pipe, w/ headroom in queue/pipe */
+	BBR_BW_PROBE_REFILL	= 3,  /* v2: refill the pipe again to 100% */
 };
-/* Randomize the starting gain cycling phase over N phases: */
-static const u32 bbr_cycle_rand = 7;
 
 /* Try to keep at least this many packets in flight, if things go smoothly. For
  * smooth functioning, a sliding window protocol ACKing every other packet
@@ -172,24 +239,12 @@ static const u32 bbr_cycle_rand = 7;
  */
 static const u32 bbr_cwnd_min_target = 4;
 
-/* To estimate if BBR_STARTUP mode (i.e. high_gain) has filled pipe... */
+/* To estimate if BBR_STARTUP or BBR_BW_PROBE_UP has filled pipe... */
 /* If bw has increased significantly (1.25x), there may be more bw available: */
 static const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
 /* But after 3 rounds w/o significant bw growth, estimate pipe is full: */
 static const u32 bbr_full_bw_cnt = 3;
 
-/* "long-term" ("LT") bandwidth estimator parameters... */
-/* The minimum number of rounds in an LT bw sampling interval: */
-static const u32 bbr_lt_intvl_min_rtts = 4;
-/* If lost/delivered ratio > 20%, interval is "lossy" and we may be policed: */
-static const u32 bbr_lt_loss_thresh = 50;
-/* If 2 intervals have a bw ratio <= 1/8, their bw is "consistent": */
-static const u32 bbr_lt_bw_ratio = BBR_UNIT / 8;
-/* If 2 intervals have a bw diff <= 4 Kbit/sec their bw is "consistent": */
-static const u32 bbr_lt_bw_diff = 4000 / 8;
-/* If we estimate we're policed, use lt_bw for this many round trips: */
-static const u32 bbr_lt_bw_max_rtts = 48;
-
 /* Gain factor for adding extra_acked to target cwnd: */
 static const int bbr_extra_acked_gain = BBR_UNIT;
 /* Window length of extra_acked window. */
@@ -199,8 +254,121 @@ static const u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
 /* Time period for clamping cwnd increment due to ack aggregation */
 static const u32 bbr_extra_acked_max_us = 100 * 1000;
 
+/* Flags to control BBR ECN-related behavior... */
+
+/* Ensure ACKs only ACK packets with consistent ECN CE status? */
+static const bool bbr_precise_ece_ack = true;
+
+/* Max RTT (in usec) at which to use sender-side ECN logic.
+ * Disabled when 0 (ECN allowed at any RTT).
+ */
+static const u32 bbr_ecn_max_rtt_us = 5000;
+
+/* On losses, scale down inflight and pacing rate by beta scaled by BBR_SCALE.
+ * No loss response when 0.
+ */
+static const u32 bbr_beta = BBR_UNIT * 30 / 100;
+
+/* Gain factor for ECN mark ratio samples, scaled by BBR_SCALE (1/16 = 6.25%) */
+static const u32 bbr_ecn_alpha_gain = BBR_UNIT * 1 / 16;
+
+/* The initial value for ecn_alpha; 1.0 allows a flow to respond quickly
+ * to congestion if the bottleneck is congested when the flow starts up.
+ */
+static const u32 bbr_ecn_alpha_init = BBR_UNIT;
+
+/* On ECN, cut inflight_lo to (1 - ecn_factor * ecn_alpha) scaled by BBR_SCALE.
+ * No ECN based bounding when 0.
+ */
+static const u32 bbr_ecn_factor = BBR_UNIT * 1 / 3;	 /* 1/3 = 33% */
+
+/* Estimate bw probing has gone too far if CE ratio exceeds this threshold.
+ * Scaled by BBR_SCALE. Disabled when 0.
+ */
+static const u32 bbr_ecn_thresh = BBR_UNIT * 1 / 2;  /* 1/2 = 50% */
+
+/* If non-zero, if in a cycle with no losses but some ECN marks, after ECN
+ * clears then make the first round's increment to inflight_hi the following
+ * fraction of inflight_hi.
+ */
+static const u32 bbr_ecn_reprobe_gain = BBR_UNIT * 1 / 2;
+
+/* Estimate bw probing has gone too far if loss rate exceeds this level. */
+static const u32 bbr_loss_thresh = BBR_UNIT * 2 / 100;  /* 2% loss */
+
+/* Slow down for a packet loss recovered by TLP? */
+static const bool bbr_loss_probe_recovery = true;
+
+/* Exit STARTUP if number of loss marking events in a Recovery round is >= N,
+ * and loss rate is higher than bbr_loss_thresh.
+ * Disabled if 0.
+ */
+static const u32 bbr_full_loss_cnt = 6;
+
+/* Exit STARTUP if number of round trips with ECN mark rate above ecn_thresh
+ * meets this count.
+ */
+static const u32 bbr_full_ecn_cnt = 2;
+
+/* Fraction of unutilized headroom to try to leave in path upon high loss. */
+static const u32 bbr_inflight_headroom = BBR_UNIT * 15 / 100;
+
+/* How much do we increase cwnd_gain when probing for bandwidth in
+ * BBR_BW_PROBE_UP? This specifies the increment in units of
+ * BBR_UNIT/4. The default is 1, meaning 0.25.
+ * The min value is 0 (meaning 0.0); max is 3 (meaning 0.75).
+ */
+static const u32 bbr_bw_probe_cwnd_gain = 1;
+
+/* Max number of packet-timed rounds to wait before probing for bandwidth.  If
+ * we want to tolerate 1% random loss per round, and not have this cut our
+ * inflight too much, we must probe for bw periodically on roughly this scale.
+ * If low, limits Reno/CUBIC coexistence; if high, limits loss tolerance.
+ * We aim to be fair with Reno/CUBIC up to a BDP of at least:
+ *  BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
+ */
+static const u32 bbr_bw_probe_max_rounds = 63;
+
+/* Max amount of randomness to inject in round counting for Reno-coexistence.
+ */
+static const u32 bbr_bw_probe_rand_rounds = 2;
+
+/* Use BBR-native probe time scale starting at this many usec.
+ * We aim to be fair with Reno/CUBIC up to an inter-loss time epoch of at least:
+ *  BDP*RTT = 25Mbps * .030sec /(1514bytes) * 0.030sec = 1.9 secs
+ */
+static const u32 bbr_bw_probe_base_us = 2 * USEC_PER_SEC;  /* 2 secs */
+
+/* Use BBR-native probes spread over this many usec: */
+static const u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC;  /* 1 secs */
+
+/* Use fast path if app-limited, no loss/ECN, and target cwnd was reached? */
+static const bool bbr_fast_path = true;
+
+/* Use fast ack mode? */
+static const bool bbr_fast_ack_mode = true;
+
+static u32 bbr_max_bw(const struct sock *sk);
+static u32 bbr_bw(const struct sock *sk);
+static void bbr_exit_probe_rtt(struct sock *sk);
+static void bbr_reset_congestion_signals(struct sock *sk);
+static void bbr_run_loss_probe_recovery(struct sock *sk);
+
 static void bbr_check_probe_rtt_done(struct sock *sk);
 
+/* This connection can use ECN if both endpoints have signaled ECN support in
+ * the handshake and the per-route settings indicated this is a
+ * shallow-threshold ECN environment, meaning both:
+ *  (a) ECN CE marks indicate low-latency/shallow-threshold congestion, and
+ *  (b) TCP endpoints provide precise ACKs that only ACK data segments
+ *      with consistent ECN CE status
+ */
+static bool bbr_can_use_ecn(const struct sock *sk)
+{
+	return (tcp_sk(sk)->ecn_flags & TCP_ECN_OK) &&
+	       (tcp_sk(sk)->ecn_flags & TCP_ECN_LOW);
+}
+
 /* Do we estimate that STARTUP filled the pipe? */
 static bool bbr_full_bw_reached(const struct sock *sk)
 {
@@ -212,17 +380,17 @@ static bool bbr_full_bw_reached(const struct sock *sk)
 /* Return the windowed max recent bandwidth sample, in pkts/uS << BW_SCALE. */
 static u32 bbr_max_bw(const struct sock *sk)
 {
-	struct bbr *bbr = inet_csk_ca(sk);
+	const struct bbr *bbr = inet_csk_ca(sk);
 
-	return minmax_get(&bbr->bw);
+	return max(bbr->bw_hi[0], bbr->bw_hi[1]);
 }
 
 /* Return the estimated bandwidth of the path, in pkts/uS << BW_SCALE. */
 static u32 bbr_bw(const struct sock *sk)
 {
-	struct bbr *bbr = inet_csk_ca(sk);
+	const struct bbr *bbr = inet_csk_ca(sk);
 
-	return bbr->lt_use_bw ? bbr->lt_bw : bbr_max_bw(sk);
+	return min(bbr_max_bw(sk), bbr->bw_lo);
 }
 
 /* Return maximum extra acked in past k-2k round trips,
@@ -239,15 +407,23 @@ static u16 bbr_extra_acked(const struct sock *sk)
  * The order here is chosen carefully to avoid overflow of u64. This should
  * work for input rates of up to 2.9Tbit/sec and gain of 2.89x.
  */
-static u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain)
+static u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain,
+				  int margin)
 {
 	unsigned int mss = tcp_sk(sk)->mss_cache;
 
 	rate *= mss;
 	rate *= gain;
 	rate >>= BBR_SCALE;
-	rate *= USEC_PER_SEC / 100 * (100 - bbr_pacing_margin_percent);
-	return rate >> BW_SCALE;
+	rate *= USEC_PER_SEC / 100 * (100 - margin);
+	rate >>= BW_SCALE;
+	rate = max(rate, 1ULL);
+	return rate;
+}
+
+static u64 bbr_bw_bytes_per_sec(struct sock *sk, u64 rate)
+{
+	return bbr_rate_bytes_per_sec(sk, rate, BBR_UNIT, 0);
 }
 
 /* Convert a BBR bw and gain factor to a pacing rate in bytes per second. */
@@ -255,12 +431,13 @@ static unsigned long bbr_bw_to_pacing_rate(struct sock *sk, u32 bw, int gain)
 {
 	u64 rate = bw;
 
-	rate = bbr_rate_bytes_per_sec(sk, rate, gain);
+	rate = bbr_rate_bytes_per_sec(sk, rate, gain,
+				      bbr_pacing_margin_percent);
 	rate = min_t(u64, rate, sk->sk_max_pacing_rate);
 	return rate;
 }
 
-/* Initialize pacing rate to: high_gain * init_cwnd / RTT. */
+/* Initialize pacing rate to: startup_pacing_gain * init_cwnd / RTT. */
 static void bbr_init_pacing_rate_from_rtt(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -276,7 +453,8 @@ static void bbr_init_pacing_rate_from_rtt(struct sock *sk)
 	}
 	bw = (u64)tp->snd_cwnd * BW_UNIT;
 	do_div(bw, rtt_us);
-	sk->sk_pacing_rate = bbr_bw_to_pacing_rate(sk, bw, bbr_high_gain);
+	sk->sk_pacing_rate =
+	  bbr_bw_to_pacing_rate(sk, bw, bbr_param(sk, startup_pacing_gain));
 }
 
 /* Pace using current bw estimate and a gain factor. */
@@ -292,30 +470,38 @@ static void bbr_set_pacing_rate(struct sock *sk, u32 bw, int gain)
 		sk->sk_pacing_rate = rate;
 }
 
-static u32 bbr_min_tso_segs(struct sock *sk)
-{
-	return sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
-}
-
-/* Return the number of segments BBR would like in a TSO/GSO skb, given
- * a particular max gso size as a constraint.
+/* Return the number of segments BBR would like in a TSO/GSO skb, given a
+ * particular max gso size as a constraint. TODO: make this simpler and more
+ * consistent by switching bbr to just call tcp_tso_autosize().
  */
 static u32 bbr_tso_segs_generic(struct sock *sk, unsigned int mss_now,
 				u32 gso_max_size)
 {
-	u32 segs;
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 segs, r;
 	u64 bytes;
 
 	/* Budget a TSO/GSO burst size allowance based on bw (pacing_rate). */
 	bytes = sk->sk_pacing_rate >> sk->sk_pacing_shift;
 
+	/* Budget a TSO/GSO burst size allowance based on min_rtt. For every
+	 * K = 2^tso_rtt_shift microseconds of min_rtt, halve the burst.
+	 * The min_rtt-based burst allowance is: 64 KBytes / 2^(min_rtt/K)
+	 */
+	if (bbr_param(sk, tso_rtt_shift)) {
+		r = bbr->min_rtt_us >> bbr_param(sk, tso_rtt_shift);
+		if (r < BITS_PER_TYPE(u32))   /* prevent undefined behavior */
+			bytes += GSO_LEGACY_MAX_SIZE >> r;
+	}
+
 	bytes = min_t(u32, bytes, gso_max_size - 1 - MAX_TCP_HEADER);
-	segs = max_t(u32, bytes / mss_now, bbr_min_tso_segs(sk));
+	segs = max_t(u32, bytes / mss_now,
+		     sock_net(sk)->ipv4.sysctl_tcp_min_tso_segs);
 	return segs;
 }
 
 /* Custom tcp_tso_autosize() for BBR, used at transmit time to cap skb size. */
-static u32  bbr_tso_segs(struct sock *sk, unsigned int mss_now)
+static u32 bbr_tso_segs(struct sock *sk, unsigned int mss_now)
 {
 	return bbr_tso_segs_generic(sk, mss_now, sk->sk_gso_max_size);
 }
@@ -325,7 +511,7 @@ static u32 bbr_tso_segs_goal(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	return  bbr_tso_segs_generic(sk, tp->mss_cache, GSO_MAX_SIZE);
+	return  bbr_tso_segs_generic(sk, tp->mss_cache, GSO_LEGACY_MAX_SIZE);
 }
 
 /* Save "last known good" cwnd so we can restore it after losses or PROBE_RTT */
@@ -345,7 +531,9 @@ static void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 
-	if (event == CA_EVENT_TX_START && tp->app_limited) {
+	if (event == CA_EVENT_TX_START) {
+		if (!tp->app_limited)
+			return;
 		bbr->idle_restart = 1;
 		bbr->ack_epoch_mstamp = tp->tcp_mstamp;
 		bbr->ack_epoch_acked = 0;
@@ -356,6 +544,16 @@ static void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)
 			bbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);
 		else if (bbr->mode == BBR_PROBE_RTT)
 			bbr_check_probe_rtt_done(sk);
+	} else if ((event == CA_EVENT_ECN_IS_CE ||
+		    event == CA_EVENT_ECN_NO_CE) &&
+		   bbr_can_use_ecn(sk) &&
+		   bbr_param(sk, precise_ece_ack)) {
+		u32 state = bbr->ce_state;
+		dctcp_ece_ack_update(sk, event, &bbr->prior_rcv_nxt, &state);
+		bbr->ce_state = state;
+	} else if (event == CA_EVENT_TLP_RECOVERY &&
+		   bbr_param(sk, loss_probe_recovery)) {
+		bbr_run_loss_probe_recovery(sk);
 	}
 }
 
@@ -378,10 +576,10 @@ static u32 bbr_bdp(struct sock *sk, u32 bw, int gain)
 	 * default. This should only happen when the connection is not using TCP
 	 * timestamps and has retransmitted all of the SYN/SYNACK/data packets
 	 * ACKed so far. In this case, an RTO can cut cwnd to 1, in which
-	 * case we need to slow-start up toward something safe: TCP_INIT_CWND.
+	 * case we need to slow-start up toward something safe: initial cwnd.
 	 */
 	if (unlikely(bbr->min_rtt_us == ~0U))	 /* no valid RTT samples yet? */
-		return TCP_INIT_CWND;  /* be safe: cap at default initial cwnd*/
+		return bbr->init_cwnd;  /* be safe: cap at initial cwnd */
 
 	w = (u64)bw * bbr->min_rtt_us;
 
@@ -398,23 +596,23 @@ static u32 bbr_bdp(struct sock *sk, u32 bw, int gain)
  *   - one skb in sending host Qdisc,
  *   - one skb in sending host TSO/GSO engine
  *   - one skb being received by receiver host LRO/GRO/delayed-ACK engine
- * Don't worry, at low rates (bbr_min_tso_rate) this won't bloat cwnd because
- * in such cases tso_segs_goal is 1. The minimum cwnd is 4 packets,
+ * Don't worry, at low rates this won't bloat cwnd because
+ * in such cases tso_segs_goal is small. The minimum cwnd is 4 packets,
  * which allows 2 outstanding 2-packet sequences, to try to keep pipe
  * full even with ACK-every-other-packet delayed ACKs.
  */
 static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
+	u32 tso_segs_goal;
 
-	/* Allow enough full-sized skbs in flight to utilize end systems. */
-	cwnd += 3 * bbr_tso_segs_goal(sk);
-
-	/* Reduce delayed ACKs by rounding up cwnd to the next even number. */
-	cwnd = (cwnd + 1) & ~1U;
+	tso_segs_goal = 3 * bbr_tso_segs_goal(sk);
 
+	/* Allow enough full-sized skbs in flight to utilize end systems. */
+	cwnd = max_t(u32, cwnd, tso_segs_goal);
+	cwnd = max_t(u32, cwnd, bbr_param(sk, cwnd_min_target));
 	/* Ensure gain cycling gets inflight above BDP even for small BDPs. */
-	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == 0)
+	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == BBR_BW_PROBE_UP)
 		cwnd += 2;
 
 	return cwnd;
@@ -469,10 +667,10 @@ static u32 bbr_ack_aggregation_cwnd(struct sock *sk)
 {
 	u32 max_aggr_cwnd, aggr_cwnd = 0;
 
-	if (bbr_extra_acked_gain && bbr_full_bw_reached(sk)) {
+	if (bbr_param(sk, extra_acked_gain)) {
 		max_aggr_cwnd = ((u64)bbr_bw(sk) * bbr_extra_acked_max_us)
 				/ BW_UNIT;
-		aggr_cwnd = (bbr_extra_acked_gain * bbr_extra_acked(sk))
+		aggr_cwnd = (bbr_param(sk, extra_acked_gain) * bbr_extra_acked(sk))
 			     >> BBR_SCALE;
 		aggr_cwnd = min(aggr_cwnd, max_aggr_cwnd);
 	}
@@ -480,66 +678,27 @@ static u32 bbr_ack_aggregation_cwnd(struct sock *sk)
 	return aggr_cwnd;
 }
 
-/* An optimization in BBR to reduce losses: On the first round of recovery, we
- * follow the packet conservation principle: send P packets per P packets acked.
- * After that, we slow-start and send at most 2*P packets per P packets acked.
- * After recovery finishes, or upon undo, we restore the cwnd we had when
- * recovery started (capped by the target cwnd based on estimated BDP).
- *
- * TODO(ycheng/ncardwell): implement a rate-based approach.
- */
-static bool bbr_set_cwnd_to_recover_or_restore(
-	struct sock *sk, const struct rate_sample *rs, u32 acked, u32 *new_cwnd)
+/* Returns the cwnd for PROBE_RTT mode. */
+static u32 bbr_probe_rtt_cwnd(struct sock *sk)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u8 prev_state = bbr->prev_ca_state, state = inet_csk(sk)->icsk_ca_state;
-	u32 cwnd = tp->snd_cwnd;
-
-	/* An ACK for P pkts should release at most 2*P packets. We do this
-	 * in two steps. First, here we deduct the number of lost packets.
-	 * Then, in bbr_set_cwnd() we slow start up toward the target cwnd.
-	 */
-	if (rs->losses > 0)
-		cwnd = max_t(s32, cwnd - rs->losses, 1);
-
-	if (state == TCP_CA_Recovery && prev_state != TCP_CA_Recovery) {
-		/* Starting 1st round of Recovery, so do packet conservation. */
-		bbr->packet_conservation = 1;
-		bbr->next_rtt_delivered = tp->delivered;  /* start round now */
-		/* Cut unused cwnd from app behavior, TSQ, or TSO deferral: */
-		cwnd = tcp_packets_in_flight(tp) + acked;
-	} else if (prev_state >= TCP_CA_Recovery && state < TCP_CA_Recovery) {
-		/* Exiting loss recovery; restore cwnd saved before recovery. */
-		cwnd = max(cwnd, bbr->prior_cwnd);
-		bbr->packet_conservation = 0;
-	}
-	bbr->prev_ca_state = state;
-
-	if (bbr->packet_conservation) {
-		*new_cwnd = max(cwnd, tcp_packets_in_flight(tp) + acked);
-		return true;	/* yes, using packet conservation */
-	}
-	*new_cwnd = cwnd;
-	return false;
+	return max_t(u32, bbr_param(sk, cwnd_min_target),
+		     bbr_bdp(sk, bbr_bw(sk), bbr_param(sk, probe_rtt_cwnd_gain)));
 }
 
 /* Slow-start up toward target cwnd (if bw estimate is growing, or packet loss
  * has drawn us down below target), or snap down to target if we're above it.
  */
 static void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,
-			 u32 acked, u32 bw, int gain)
+			 u32 acked, u32 bw, int gain, u32 cwnd,
+			 struct bbr_context *ctx)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
-	u32 cwnd = tp->snd_cwnd, target_cwnd = 0;
+	u32 target_cwnd = 0;
 
 	if (!acked)
 		goto done;  /* no packet fully ACKed; just apply caps */
 
-	if (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &cwnd))
-		goto done;
-
 	target_cwnd = bbr_bdp(sk, bw, gain);
 
 	/* Increment the cwnd to account for excess ACKed data that seems
@@ -548,74 +707,25 @@ static void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,
 	target_cwnd += bbr_ack_aggregation_cwnd(sk);
 	target_cwnd = bbr_quantization_budget(sk, target_cwnd);
 
-	/* If we're below target cwnd, slow start cwnd toward target cwnd. */
-	if (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */
-		cwnd = min(cwnd + acked, target_cwnd);
-	else if (cwnd < target_cwnd || tp->delivered < TCP_INIT_CWND)
-		cwnd = cwnd + acked;
-	cwnd = max(cwnd, bbr_cwnd_min_target);
+	/* Update cwnd and enable fast path if cwnd reaches target_cwnd. */
+	bbr->try_fast_path = 0;
+	if (bbr_full_bw_reached(sk)) { /* only cut cwnd if we filled the pipe */
+		cwnd += acked;
+		if (cwnd >= target_cwnd) {
+			cwnd = target_cwnd;
+			bbr->try_fast_path = 1;
+		}
+	} else if (cwnd < target_cwnd || cwnd  < 2 * bbr->init_cwnd) {
+		cwnd += acked;
+	} else {
+		bbr->try_fast_path = 1;
+	}
 
+	cwnd = max_t(u32, cwnd, bbr_param(sk, cwnd_min_target));
 done:
-	tp->snd_cwnd = min(cwnd, tp->snd_cwnd_clamp);	/* apply global cap */
+	tcp_snd_cwnd_set(tp, min(cwnd, tp->snd_cwnd_clamp));  /* global cap */
 	if (bbr->mode == BBR_PROBE_RTT)  /* drain queue, refresh min_rtt */
-		tp->snd_cwnd = min(tp->snd_cwnd, bbr_cwnd_min_target);
-}
-
-/* End cycle phase if it's time and/or we hit the phase's in-flight target. */
-static bool bbr_is_next_cycle_phase(struct sock *sk,
-				    const struct rate_sample *rs)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	bool is_full_length =
-		tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp) >
-		bbr->min_rtt_us;
-	u32 inflight, bw;
-
-	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
-	 * use the pipe without increasing the queue.
-	 */
-	if (bbr->pacing_gain == BBR_UNIT)
-		return is_full_length;		/* just use wall clock time */
-
-	inflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);
-	bw = bbr_max_bw(sk);
-
-	/* A pacing_gain > 1.0 probes for bw by trying to raise inflight to at
-	 * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is
-	 * small (e.g. on a LAN). We do not persist if packets are lost, since
-	 * a path with small buffers may not hold that much.
-	 */
-	if (bbr->pacing_gain > BBR_UNIT)
-		return is_full_length &&
-			(rs->losses ||  /* perhaps pacing_gain*BDP won't fit */
-			 inflight >= bbr_inflight(sk, bw, bbr->pacing_gain));
-
-	/* A pacing_gain < 1.0 tries to drain extra queue we added if bw
-	 * probing didn't find more bw. If inflight falls to match BDP then we
-	 * estimate queue is drained; persisting would underutilize the pipe.
-	 */
-	return is_full_length ||
-		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
-}
-
-static void bbr_advance_cycle_phase(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->cycle_idx = (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);
-	bbr->cycle_mstamp = tp->delivered_mstamp;
-}
-
-/* Gain cycling: cycle pacing gain to converge to fair share of available bw. */
-static void bbr_update_cycle_phase(struct sock *sk,
-				   const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
-		bbr_advance_cycle_phase(sk);
+		tcp_snd_cwnd_set(tp, min(tcp_snd_cwnd(tp), bbr_cwnd_min_target));
 }
 
 static void bbr_reset_startup_mode(struct sock *sk)
@@ -625,191 +735,49 @@ static void bbr_reset_startup_mode(struct sock *sk)
 	bbr->mode = BBR_STARTUP;
 }
 
-static void bbr_reset_probe_bw_mode(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->mode = BBR_PROBE_BW;
-	bbr->cycle_idx = CYCLE_LEN - 1 - prandom_u32_max(bbr_cycle_rand);
-	bbr_advance_cycle_phase(sk);	/* flip to next phase of gain cycle */
-}
-
-static void bbr_reset_mode(struct sock *sk)
-{
-	if (!bbr_full_bw_reached(sk))
-		bbr_reset_startup_mode(sk);
-	else
-		bbr_reset_probe_bw_mode(sk);
-}
-
-/* Start a new long-term sampling interval. */
-static void bbr_reset_lt_bw_sampling_interval(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->lt_last_stamp = div_u64(tp->delivered_mstamp, USEC_PER_MSEC);
-	bbr->lt_last_delivered = tp->delivered;
-	bbr->lt_last_lost = tp->lost;
-	bbr->lt_rtt_cnt = 0;
-}
-
-/* Completely reset long-term bandwidth sampling. */
-static void bbr_reset_lt_bw_sampling(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->lt_bw = 0;
-	bbr->lt_use_bw = 0;
-	bbr->lt_is_sampling = false;
-	bbr_reset_lt_bw_sampling_interval(sk);
-}
-
-/* Long-term bw sampling interval is done. Estimate whether we're policed. */
-static void bbr_lt_bw_interval_done(struct sock *sk, u32 bw)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 diff;
-
-	if (bbr->lt_bw) {  /* do we have bw from a previous interval? */
-		/* Is new bw close to the lt_bw from the previous interval? */
-		diff = abs(bw - bbr->lt_bw);
-		if ((diff * BBR_UNIT <= bbr_lt_bw_ratio * bbr->lt_bw) ||
-		    (bbr_rate_bytes_per_sec(sk, diff, BBR_UNIT) <=
-		     bbr_lt_bw_diff)) {
-			/* All criteria are met; estimate we're policed. */
-			bbr->lt_bw = (bw + bbr->lt_bw) >> 1;  /* avg 2 intvls */
-			bbr->lt_use_bw = 1;
-			bbr->pacing_gain = BBR_UNIT;  /* try to avoid drops */
-			bbr->lt_rtt_cnt = 0;
-			return;
-		}
-	}
-	bbr->lt_bw = bw;
-	bbr_reset_lt_bw_sampling_interval(sk);
-}
-
-/* Token-bucket traffic policers are common (see "An Internet-Wide Analysis of
- * Traffic Policing", SIGCOMM 2016). BBR detects token-bucket policers and
- * explicitly models their policed rate, to reduce unnecessary losses. We
- * estimate that we're policed if we see 2 consecutive sampling intervals with
- * consistent throughput and high packet loss. If we think we're being policed,
- * set lt_bw to the "long-term" average delivery rate from those 2 intervals.
+/* See if we have reached next round trip. Upon start of the new round,
+ * returns packets delivered since previous round start plus this ACK.
  */
-static void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample *rs)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 lost, delivered;
-	u64 bw;
-	u32 t;
-
-	if (bbr->lt_use_bw) {	/* already using long-term rate, lt_bw? */
-		if (bbr->mode == BBR_PROBE_BW && bbr->round_start &&
-		    ++bbr->lt_rtt_cnt >= bbr_lt_bw_max_rtts) {
-			bbr_reset_lt_bw_sampling(sk);    /* stop using lt_bw */
-			bbr_reset_probe_bw_mode(sk);  /* restart gain cycling */
-		}
-		return;
-	}
-
-	/* Wait for the first loss before sampling, to let the policer exhaust
-	 * its tokens and estimate the steady-state rate allowed by the policer.
-	 * Starting samples earlier includes bursts that over-estimate the bw.
-	 */
-	if (!bbr->lt_is_sampling) {
-		if (!rs->losses)
-			return;
-		bbr_reset_lt_bw_sampling_interval(sk);
-		bbr->lt_is_sampling = true;
-	}
-
-	/* To avoid underestimates, reset sampling if we run out of data. */
-	if (rs->is_app_limited) {
-		bbr_reset_lt_bw_sampling(sk);
-		return;
-	}
-
-	if (bbr->round_start)
-		bbr->lt_rtt_cnt++;	/* count round trips in this interval */
-	if (bbr->lt_rtt_cnt < bbr_lt_intvl_min_rtts)
-		return;		/* sampling interval needs to be longer */
-	if (bbr->lt_rtt_cnt > 4 * bbr_lt_intvl_min_rtts) {
-		bbr_reset_lt_bw_sampling(sk);  /* interval is too long */
-		return;
-	}
-
-	/* End sampling interval when a packet is lost, so we estimate the
-	 * policer tokens were exhausted. Stopping the sampling before the
-	 * tokens are exhausted under-estimates the policed rate.
-	 */
-	if (!rs->losses)
-		return;
-
-	/* Calculate packets lost and delivered in sampling interval. */
-	lost = tp->lost - bbr->lt_last_lost;
-	delivered = tp->delivered - bbr->lt_last_delivered;
-	/* Is loss rate (lost/delivered) >= lt_loss_thresh? If not, wait. */
-	if (!delivered || (lost << BBR_SCALE) < bbr_lt_loss_thresh * delivered)
-		return;
-
-	/* Find average delivery rate in this sampling interval. */
-	t = div_u64(tp->delivered_mstamp, USEC_PER_MSEC) - bbr->lt_last_stamp;
-	if ((s32)t < 1)
-		return;		/* interval is less than one ms, so wait */
-	/* Check if can multiply without overflow */
-	if (t >= ~0U / USEC_PER_MSEC) {
-		bbr_reset_lt_bw_sampling(sk);  /* interval too long; reset */
-		return;
-	}
-	t *= USEC_PER_MSEC;
-	bw = (u64)delivered * BW_UNIT;
-	do_div(bw, t);
-	bbr_lt_bw_interval_done(sk, bw);
-}
-
-/* Estimate the bandwidth based on how fast packets are delivered */
-static void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)
+static u32 bbr_update_round_start(struct sock *sk,
+		const struct rate_sample *rs, struct bbr_context *ctx)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
-	u64 bw;
+	u32 round_delivered = 0;
 
 	bbr->round_start = 0;
-	if (rs->delivered < 0 || rs->interval_us <= 0)
-		return; /* Not a valid observation */
 
 	/* See if we've reached the next RTT */
-	if (!before(rs->prior_delivered, bbr->next_rtt_delivered)) {
+	if (rs->interval_us > 0 &&
+	    !before(rs->prior_delivered, bbr->next_rtt_delivered)) {
+		round_delivered = tp->delivered - bbr->next_rtt_delivered;
 		bbr->next_rtt_delivered = tp->delivered;
-		bbr->rtt_cnt++;
 		bbr->round_start = 1;
-		bbr->packet_conservation = 0;
 	}
+	return round_delivered;
+}
 
-	bbr_lt_bw_sampling(sk, rs);
+/* Calculate the bandwidth based on how fast packets are delivered */
+static void bbr_calculate_bw_sample(struct sock *sk,
+			const struct rate_sample *rs, struct bbr_context *ctx)
+{
+	u64 bw = 0;
 
 	/* Divide delivered by the interval to find a (lower bound) bottleneck
 	 * bandwidth sample. Delivered is in packets and interval_us in uS and
 	 * ratio will be <<1 for most connections. So delivered is first scaled.
+	 * Round up to allow growth at low rates, even with integer division.
 	 */
-	bw = div64_long((u64)rs->delivered * BW_UNIT, rs->interval_us);
-
-	/* If this sample is application-limited, it is likely to have a very
-	 * low delivered count that represents application behavior rather than
-	 * the available network rate. Such a sample could drag down estimated
-	 * bw, causing needless slow-down. Thus, to continue to send at the
-	 * last measured network rate, we filter out app-limited samples unless
-	 * they describe the path bw at least as well as our bw model.
-	 *
-	 * So the goal during app-limited phase is to proceed with the best
-	 * network rate no matter how long. We automatically leave this
-	 * phase when app writes faster than the network can deliver :)
-	 */
-	if (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {
-		/* Incorporate new sample into our max bw filter. */
-		minmax_running_max(&bbr->bw, bbr_bw_rtts, bbr->rtt_cnt, bw);
+	if (rs->interval_us > 0) {
+		if (WARN_ONCE(rs->delivered < 0,
+			      "negative delivered: %d interval_us: %ld\n",
+			      rs->delivered, rs->interval_us))
+			return;
+
+		bw = DIV_ROUND_UP_ULL((u64)rs->delivered * BW_UNIT, rs->interval_us);
 	}
+
+	ctx->sample_bw = bw;
 }
 
 /* Estimates the windowed max degree of ack aggregation.
@@ -823,7 +791,7 @@ static void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)
  *
  * Max extra_acked is clamped by cwnd and bw * bbr_extra_acked_max_us (100 ms).
  * Max filter is an approximate sliding window of 5-10 (packet timed) round
- * trips.
+ * trips for non-startup phase, and 1-2 round trips for startup.
  */
 static void bbr_update_ack_aggregation(struct sock *sk,
 				       const struct rate_sample *rs)
@@ -831,15 +799,19 @@ static void bbr_update_ack_aggregation(struct sock *sk,
 	u32 epoch_us, expected_acked, extra_acked;
 	struct bbr *bbr = inet_csk_ca(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
+	u32 extra_acked_win_rtts_thresh = bbr_param(sk, extra_acked_win_rtts);
 
-	if (!bbr_extra_acked_gain || rs->acked_sacked <= 0 ||
+	if (!bbr_param(sk, extra_acked_gain) || rs->acked_sacked <= 0 ||
 	    rs->delivered < 0 || rs->interval_us <= 0)
 		return;
 
 	if (bbr->round_start) {
 		bbr->extra_acked_win_rtts = min(0x1F,
 						bbr->extra_acked_win_rtts + 1);
-		if (bbr->extra_acked_win_rtts >= bbr_extra_acked_win_rtts) {
+		if (!bbr_full_bw_reached(sk))
+			extra_acked_win_rtts_thresh = 1;
+		if (bbr->extra_acked_win_rtts >=
+		    extra_acked_win_rtts_thresh) {
 			bbr->extra_acked_win_rtts = 0;
 			bbr->extra_acked_win_idx = bbr->extra_acked_win_idx ?
 						   0 : 1;
@@ -868,54 +840,11 @@ static void bbr_update_ack_aggregation(struct sock *sk,
 	bbr->ack_epoch_acked = min_t(u32, 0xFFFFF,
 				     bbr->ack_epoch_acked + rs->acked_sacked);
 	extra_acked = bbr->ack_epoch_acked - expected_acked;
-	extra_acked = min(extra_acked, tp->snd_cwnd);
+	extra_acked = min(extra_acked, tcp_snd_cwnd(tp));
 	if (extra_acked > bbr->extra_acked[bbr->extra_acked_win_idx])
 		bbr->extra_acked[bbr->extra_acked_win_idx] = extra_acked;
 }
 
-/* Estimate when the pipe is full, using the change in delivery rate: BBR
- * estimates that STARTUP filled the pipe if the estimated bw hasn't changed by
- * at least bbr_full_bw_thresh (25%) after bbr_full_bw_cnt (3) non-app-limited
- * rounds. Why 3 rounds: 1: rwin autotuning grows the rwin, 2: we fill the
- * higher rwin, 3: we get higher delivery rate samples. Or transient
- * cross-traffic or radio noise can go away. CUBIC Hystart shares a similar
- * design goal, but uses delay and inter-ACK spacing instead of bandwidth.
- */
-static void bbr_check_full_bw_reached(struct sock *sk,
-				      const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 bw_thresh;
-
-	if (bbr_full_bw_reached(sk) || !bbr->round_start || rs->is_app_limited)
-		return;
-
-	bw_thresh = (u64)bbr->full_bw * bbr_full_bw_thresh >> BBR_SCALE;
-	if (bbr_max_bw(sk) >= bw_thresh) {
-		bbr->full_bw = bbr_max_bw(sk);
-		bbr->full_bw_cnt = 0;
-		return;
-	}
-	++bbr->full_bw_cnt;
-	bbr->full_bw_reached = bbr->full_bw_cnt >= bbr_full_bw_cnt;
-}
-
-/* If pipe is probably full, drain the queue and then enter steady-state. */
-static void bbr_check_drain(struct sock *sk, const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {
-		bbr->mode = BBR_DRAIN;	/* drain queue we created */
-		tcp_sk(sk)->snd_ssthresh =
-				bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
-	}	/* fall through to check if in-flight is already small: */
-	if (bbr->mode == BBR_DRAIN &&
-	    bbr_packets_in_net_at_edt(sk, tcp_packets_in_flight(tcp_sk(sk))) <=
-	    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT))
-		bbr_reset_probe_bw_mode(sk);  /* we estimate queue is drained */
-}
-
 static void bbr_check_probe_rtt_done(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -925,9 +854,9 @@ static void bbr_check_probe_rtt_done(struct sock *sk)
 	      after(tcp_jiffies32, bbr->probe_rtt_done_stamp)))
 		return;
 
-	bbr->min_rtt_stamp = tcp_jiffies32;  /* wait a while until PROBE_RTT */
+	bbr->probe_rtt_min_stamp = tcp_jiffies32; /* schedule next PROBE_RTT */
 	tp->snd_cwnd = max(tp->snd_cwnd, bbr->prior_cwnd);
-	bbr_reset_mode(sk);
+	bbr_exit_probe_rtt(sk);
 }
 
 /* The goal of PROBE_RTT mode is to have BBR flows cooperatively and
@@ -953,23 +882,35 @@ static void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
-	bool filter_expired;
+	bool probe_rtt_expired, min_rtt_expired;
+	u32 expire;
 
-	/* Track min RTT seen in the min_rtt_win_sec filter window: */
-	filter_expired = after(tcp_jiffies32,
-			       bbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);
+	/* Track min RTT in probe_rtt_win_ms to time next PROBE_RTT state. */
+	expire = bbr->probe_rtt_min_stamp +
+		 msecs_to_jiffies(bbr_param(sk, probe_rtt_win_ms));
+	probe_rtt_expired = after(tcp_jiffies32, expire);
 	if (rs->rtt_us >= 0 &&
-	    (rs->rtt_us < bbr->min_rtt_us ||
-	     (filter_expired && !rs->is_ack_delayed))) {
-		bbr->min_rtt_us = rs->rtt_us;
-		bbr->min_rtt_stamp = tcp_jiffies32;
+	    (rs->rtt_us < bbr->probe_rtt_min_us ||
+	     (probe_rtt_expired && !rs->is_ack_delayed))) {
+		bbr->probe_rtt_min_us = rs->rtt_us;
+		bbr->probe_rtt_min_stamp = tcp_jiffies32;
+	}
+	/* Track min RTT seen in the min_rtt_win_sec filter window: */
+	expire = bbr->min_rtt_stamp + bbr_param(sk, min_rtt_win_sec) * HZ;
+	min_rtt_expired = after(tcp_jiffies32, expire);
+	if (bbr->probe_rtt_min_us <= bbr->min_rtt_us ||
+	    min_rtt_expired) {
+		bbr->min_rtt_us = bbr->probe_rtt_min_us;
+		bbr->min_rtt_stamp = bbr->probe_rtt_min_stamp;
 	}
 
-	if (bbr_probe_rtt_mode_ms > 0 && filter_expired &&
+	if (bbr_param(sk, probe_rtt_mode_ms) > 0 && probe_rtt_expired &&
 	    !bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {
 		bbr->mode = BBR_PROBE_RTT;  /* dip, drain queue */
 		bbr_save_cwnd(sk);  /* note cwnd so we can restore it */
 		bbr->probe_rtt_done_stamp = 0;
+		bbr->ack_phase = BBR_ACKS_PROBE_STOPPING;
+		bbr->next_rtt_delivered = tp->delivered;
 	}
 
 	if (bbr->mode == BBR_PROBE_RTT) {
@@ -978,9 +919,9 @@ static void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)
 			(tp->delivered + tcp_packets_in_flight(tp)) ? : 1;
 		/* Maintain min packets in flight for max(200 ms, 1 round). */
 		if (!bbr->probe_rtt_done_stamp &&
-		    tcp_packets_in_flight(tp) <= bbr_cwnd_min_target) {
+		    tcp_packets_in_flight(tp) <= bbr_probe_rtt_cwnd(sk)) {
 			bbr->probe_rtt_done_stamp = tcp_jiffies32 +
-				msecs_to_jiffies(bbr_probe_rtt_mode_ms);
+				msecs_to_jiffies(bbr_param(sk, probe_rtt_mode_ms));
 			bbr->probe_rtt_round_done = 0;
 			bbr->next_rtt_delivered = tp->delivered;
 		} else if (bbr->probe_rtt_done_stamp) {
@@ -1001,18 +942,20 @@ static void bbr_update_gains(struct sock *sk)
 
 	switch (bbr->mode) {
 	case BBR_STARTUP:
-		bbr->pacing_gain = bbr_high_gain;
-		bbr->cwnd_gain	 = bbr_high_gain;
+		bbr->pacing_gain = bbr_param(sk, startup_pacing_gain);
+		bbr->cwnd_gain	 = bbr_param(sk, startup_cwnd_gain);
 		break;
 	case BBR_DRAIN:
-		bbr->pacing_gain = bbr_drain_gain;	/* slow, to drain */
-		bbr->cwnd_gain	 = bbr_high_gain;	/* keep cwnd */
+		bbr->pacing_gain = bbr_param(sk, drain_gain);  /* slow, to drain */
+		bbr->cwnd_gain	 = bbr_param(sk, startup_cwnd_gain);  /* keep cwnd */
 		break;
 	case BBR_PROBE_BW:
-		bbr->pacing_gain = (bbr->lt_use_bw ?
-				    BBR_UNIT :
-				    bbr_pacing_gain[bbr->cycle_idx]);
-		bbr->cwnd_gain	 = bbr_cwnd_gain;
+		bbr->pacing_gain = bbr_pacing_gain[bbr->cycle_idx];
+		bbr->cwnd_gain	 = bbr_param(sk, cwnd_gain);
+		if (bbr_param(sk, bw_probe_cwnd_gain) &&
+		    bbr->cycle_idx == BBR_BW_PROBE_UP)
+			bbr->cwnd_gain +=
+				BBR_UNIT * bbr_param(sk, bw_probe_cwnd_gain) / 4;
 		break;
 	case BBR_PROBE_RTT:
 		bbr->pacing_gain = BBR_UNIT;
@@ -1024,140 +967,1392 @@ static void bbr_update_gains(struct sock *sk)
 	}
 }
 
-static void bbr_update_model(struct sock *sk, const struct rate_sample *rs)
+static u32 bbr_sndbuf_expand(struct sock *sk)
 {
-	bbr_update_bw(sk, rs);
-	bbr_update_ack_aggregation(sk, rs);
-	bbr_update_cycle_phase(sk, rs);
-	bbr_check_full_bw_reached(sk, rs);
-	bbr_check_drain(sk, rs);
-	bbr_update_min_rtt(sk, rs);
-	bbr_update_gains(sk);
+	/* Provision 3 * cwnd since BBR may slow-start even during recovery. */
+	return 3;
 }
 
-static void bbr_main(struct sock *sk, const struct rate_sample *rs)
+/* Incorporate a new bw sample into the current window of our max filter. */
+static void bbr_take_max_bw_sample(struct sock *sk, u32 bw)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
-	u32 bw;
-
-	bbr_update_model(sk, rs);
 
-	bw = bbr_bw(sk);
-	bbr_set_pacing_rate(sk, bw, bbr->pacing_gain);
-	bbr_set_cwnd(sk, rs, rs->acked_sacked, bw, bbr->cwnd_gain);
+	bbr->bw_hi[1] = max(bw, bbr->bw_hi[1]);
 }
 
-static void bbr_init(struct sock *sk)
+/* Keep max of last 1-2 cycles. Each PROBE_BW cycle, flip filter window. */
+static void bbr_advance_max_bw_filter(struct sock *sk)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 
-	bbr->prior_cwnd = 0;
-	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
-	bbr->rtt_cnt = 0;
-	bbr->next_rtt_delivered = tp->delivered;
-	bbr->prev_ca_state = TCP_CA_Open;
-	bbr->packet_conservation = 0;
-
-	bbr->probe_rtt_done_stamp = 0;
-	bbr->probe_rtt_round_done = 0;
-	bbr->min_rtt_us = tcp_min_rtt(tp);
-	bbr->min_rtt_stamp = tcp_jiffies32;
-
-	minmax_reset(&bbr->bw, bbr->rtt_cnt, 0);  /* init max bw to 0 */
+	if (!bbr->bw_hi[1])
+		return;  /* no samples in this window; remember old window */
+	bbr->bw_hi[0] = bbr->bw_hi[1];
+	bbr->bw_hi[1] = 0;
+}
 
-	bbr->has_seen_rtt = 0;
-	bbr_init_pacing_rate_from_rtt(sk);
+/* Reset the estimator for reaching full bandwidth based on bw plateau. */
+static void bbr_reset_full_bw(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
 
-	bbr->round_start = 0;
-	bbr->idle_restart = 0;
-	bbr->full_bw_reached = 0;
 	bbr->full_bw = 0;
 	bbr->full_bw_cnt = 0;
-	bbr->cycle_mstamp = 0;
-	bbr->cycle_idx = 0;
-	bbr_reset_lt_bw_sampling(sk);
-	bbr_reset_startup_mode(sk);
-
-	bbr->ack_epoch_mstamp = tp->tcp_mstamp;
-	bbr->ack_epoch_acked = 0;
-	bbr->extra_acked_win_rtts = 0;
-	bbr->extra_acked_win_idx = 0;
-	bbr->extra_acked[0] = 0;
-	bbr->extra_acked[1] = 0;
-
-	cmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);
+	bbr->full_bw_now = 0;
 }
 
-static u32 bbr_sndbuf_expand(struct sock *sk)
+/* How much do we want in flight? Our BDP, unless congestion cut cwnd. */
+static u32 bbr_target_inflight(struct sock *sk)
 {
-	/* Provision 3 * cwnd since BBR may slow-start even during recovery. */
-	return 3;
+	u32 bdp = bbr_inflight(sk, bbr_bw(sk), BBR_UNIT);
+
+	return min(bdp, tcp_sk(sk)->snd_cwnd);
 }
 
-/* In theory BBR does not need to undo the cwnd since it does not
- * always reduce cwnd on losses (see bbr_main()). Keep it for now.
- */
-static u32 bbr_undo_cwnd(struct sock *sk)
+static bool bbr_is_probing_bandwidth(struct sock *sk)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
 
-	bbr->full_bw = 0;   /* spurious slow-down; reset full pipe detection */
-	bbr->full_bw_cnt = 0;
-	bbr_reset_lt_bw_sampling(sk);
-	return tcp_sk(sk)->snd_cwnd;
+	return (bbr->mode == BBR_STARTUP) ||
+		(bbr->mode == BBR_PROBE_BW &&
+		 (bbr->cycle_idx == BBR_BW_PROBE_REFILL ||
+		  bbr->cycle_idx == BBR_BW_PROBE_UP));
 }
 
-/* Entering loss recovery, so save cwnd for when we exit or undo recovery. */
+/* Has the given amount of time elapsed since we marked the phase start? */
+static bool bbr_has_elapsed_in_phase(const struct sock *sk, u32 interval_us)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	const struct bbr *bbr = inet_csk_ca(sk);
+
+	return tcp_stamp_us_delta(tp->tcp_mstamp,
+				  bbr->cycle_mstamp + interval_us) > 0;
+}
+
+static void bbr_handle_queue_too_high_in_startup(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 bdp;  /* estimated BDP in packets, with quantization budget */
+
+	bbr->full_bw_reached = 1;
+
+	bdp = bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
+	bbr->inflight_hi = max(bdp, bbr->inflight_latest);
+}
+
+/* Exit STARTUP upon N consecutive rounds with ECN mark rate > ecn_thresh. */
+static void bbr_check_ecn_too_high_in_startup(struct sock *sk, u32 ce_ratio)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (bbr_full_bw_reached(sk) || !bbr->ecn_eligible ||
+	    !bbr_param(sk, full_ecn_cnt) || !bbr_param(sk, ecn_thresh))
+		return;
+
+	if (ce_ratio >= bbr_param(sk, ecn_thresh))
+		bbr->startup_ecn_rounds++;
+	else
+		bbr->startup_ecn_rounds = 0;
+
+	if (bbr->startup_ecn_rounds >= bbr_param(sk, full_ecn_cnt)) {
+		bbr_handle_queue_too_high_in_startup(sk);
+		return;
+	}
+}
+
+/* Updates ecn_alpha and returns ce_ratio. -1 if not available. *//* Updates ecn_alpha and returns ce_ratio. -1 if not available. */
+static int bbr_update_ecn_alpha(struct sock *sk)
+{
+    struct tcp_sock *tp = tcp_sk(sk);
+    struct net *net = sock_net(sk);
+    struct bbr *bbr = inet_csk_ca(sk);
+    s32 delivered, delivered_ce;
+    u64 alpha, ce_ratio;
+    u32 gain;
+    bool want_ecn_alpha;
+
+    /* See if we should use ECN sender logic for this connection. */
+    if (!bbr->ecn_eligible) {
+        if (bbr_can_use_ecn(sk)) {
+            if (bbr_param(sk, ecn_factor)) {
+                if (bbr->min_rtt_us <= bbr_ecn_max_rtt_us || !bbr_ecn_max_rtt_us) {
+                    bbr->ecn_eligible = 1;
+                }
+            }
+        }
+    }
+
+    /* Skip updating alpha only if not ECN-eligible and PLB is disabled. */
+    want_ecn_alpha = bbr->ecn_eligible;
+    if (!want_ecn_alpha) {
+        want_ecn_alpha = bbr_can_use_ecn(sk) & READ_ONCE(net->ipv4.sysctl_tcp_plb_enabled);
+    }
+    if (!want_ecn_alpha)
+        return -1;
+
+    delivered = tp->delivered - bbr->alpha_last_delivered;
+    delivered_ce = tp->delivered_ce - bbr->alpha_last_delivered_ce;
+    if (delivered == 0 || /* avoid divide by zero */
+        WARN_ON_ONCE(delivered < 0 || delivered_ce < 0)) /* backwards? */
+        return -1;
+
+    BUILD_BUG_ON(BBR_SCALE != TCP_PLB_SCALE);
+    ce_ratio = (u64)delivered_ce << BBR_SCALE;
+    do_div(ce_ratio, delivered);
+    gain = bbr_param(sk, ecn_alpha_gain);
+    alpha = ((BBR_UNIT - gain) * bbr->ecn_alpha) >> BBR_SCALE;
+    alpha += (gain * ce_ratio) >> BBR_SCALE;
+    bbr->ecn_alpha = min_t(u32, alpha, BBR_UNIT);
+    bbr->alpha_last_delivered = tp->delivered;
+    bbr->alpha_last_delivered_ce = tp->delivered_ce;
+
+    bbr_check_ecn_too_high_in_startup(sk, ce_ratio);
+    return (int)ce_ratio;
+}
+
+/* Protective Load Balancing (PLB). PLB rehashes outgoing data (to a new IPv6
+ * flow label) if it encounters sustained congestion in the form of ECN marks.
+ */
+static void bbr_plb(struct sock *sk, const struct rate_sample *rs, int ce_ratio)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (bbr->round_start && ce_ratio >= 0)
+		tcp_plb_update_state(sk, &bbr->plb, ce_ratio);
+
+	tcp_plb_check_rehash(sk, &bbr->plb);
+}
+
+/* Each round trip of BBR_BW_PROBE_UP, double volume of probing data. */
+static void bbr_raise_inflight_hi_slope(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 growth_this_round, cnt;
+
+	/* Calculate "slope": packets S/Acked per inflight_hi increment. */
+	growth_this_round = 1 << bbr->bw_probe_up_rounds;
+	bbr->bw_probe_up_rounds = min(bbr->bw_probe_up_rounds + 1, 30);
+	cnt = tcp_snd_cwnd(tp) / growth_this_round;
+	cnt = max(cnt, 1U);
+	bbr->bw_probe_up_cnt = cnt;
+}
+
+/* In BBR_BW_PROBE_UP, not seeing high loss/ECN/queue, so raise inflight_hi. */
+static void bbr_probe_inflight_hi_upward(struct sock *sk,
+					  const struct rate_sample *rs)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 delta;
+
+	if (!tp->is_cwnd_limited || tcp_snd_cwnd(tp) < bbr->inflight_hi)
+		return;  /* not fully using inflight_hi, so don't grow it */
+
+	/* For each bw_probe_up_cnt packets ACKed, increase inflight_hi by 1. */
+	bbr->bw_probe_up_acks += rs->acked_sacked;
+	if (bbr->bw_probe_up_acks >=  bbr->bw_probe_up_cnt) {
+		delta = bbr->bw_probe_up_acks / bbr->bw_probe_up_cnt;
+		bbr->bw_probe_up_acks -= delta * bbr->bw_probe_up_cnt;
+		bbr->inflight_hi += delta;
+		bbr->try_fast_path = 0;  /* Need to update cwnd */
+	}
+
+	if (bbr->round_start)
+		bbr_raise_inflight_hi_slope(sk);
+}
+
+/* Does loss/ECN rate for this sample say inflight is "too high"?
+ * This is used by both the bbr_check_loss_too_high_in_startup() function,
+ * which can be used in either v1 or v2, and the PROBE_UP phase of v2, which
+ * uses it to notice when loss/ECN rates suggest inflight is too high.
+ */
+static bool bbr_is_inflight_too_high(const struct sock *sk,
+				      const struct rate_sample *rs)
+{
+	const struct bbr *bbr = inet_csk_ca(sk);
+	u32 loss_thresh, ecn_thresh;
+
+	if (rs->lost > 0 && rs->tx_in_flight) {
+		loss_thresh = (u64)rs->tx_in_flight * bbr_param(sk, loss_thresh) >>
+				BBR_SCALE;
+		if (rs->lost > loss_thresh) {
+			return true;
+		}
+	}
+
+    if (rs->delivered_ce > 0) {
+        if (rs->delivered > 0) {
+            if (bbr->ecn_eligible) {
+                u32 thresh_param = bbr_param(sk, ecn_thresh);
+                if (thresh_param) {
+                    ecn_thresh = (u64)rs->delivered * thresh_param >> BBR_SCALE;
+                    if (rs->delivered_ce > ecn_thresh) {
+                        return true;
+                    }
+                }
+            }
+        }
+    }
+
+	return false;
+}
+
+/* Calculate the tx_in_flight level that corresponded to excessive loss.
+ * We find "lost_prefix" segs of the skb where loss rate went too high,
+ * by solving for "lost_prefix" in the following equation:
+ *   lost                     /  inflight                     >= loss_thresh
+ *  (lost_prev + lost_prefix) / (inflight_prev + lost_prefix) >= loss_thresh
+ * Then we take that equation, convert it to fixed point, and
+ * round up to the nearest packet.
+ */
+static u32 bbr_inflight_hi_from_lost_skb(const struct sock *sk,
+					  const struct rate_sample *rs,
+					  const struct sk_buff *skb)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	u32 loss_thresh  = bbr_param(sk, loss_thresh);
+	u32 pcount, divisor, inflight_hi;
+	s32 inflight_prev, lost_prev;
+	u64 loss_budget, lost_prefix;
+
+	pcount = tcp_skb_pcount(skb);
+
+	/* How much data was in flight before this skb? */
+	inflight_prev = rs->tx_in_flight - pcount;
+	if (inflight_prev < 0) {
+		WARN_ONCE(tcp_skb_tx_in_flight_is_suspicious(
+				  pcount,
+				  TCP_SKB_CB(skb)->sacked,
+				  rs->tx_in_flight),
+			  "tx_in_flight: %u pcount: %u reneg: %u",
+			  rs->tx_in_flight, pcount, tcp_sk(sk)->is_sack_reneg);
+		return ~0U;
+	}
+
+	/* How much inflight data was marked lost before this skb? */
+	lost_prev = rs->lost - pcount;
+	if (WARN_ONCE(lost_prev < 0,
+		      "cwnd: %u ca: %d out: %u lost: %u pif: %u "
+		      "tx_in_flight: %u tx.lost: %u tp->lost: %u rs->lost: %d "
+		      "lost_prev: %d pcount: %d seq: %u end_seq: %u reneg: %u",
+		      tcp_snd_cwnd(tp), inet_csk(sk)->icsk_ca_state,
+		      tp->packets_out, tp->lost_out, tcp_packets_in_flight(tp),
+		      rs->tx_in_flight, TCP_SKB_CB(skb)->tx.lost, tp->lost,
+		      rs->lost, lost_prev, pcount,
+		      TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq,
+		      tp->is_sack_reneg))
+		return ~0U;
+
+	/* At what prefix of this lost skb did losss rate exceed loss_thresh? */
+	loss_budget = (u64)inflight_prev * loss_thresh + BBR_UNIT - 1;
+	loss_budget >>= BBR_SCALE;
+	if (lost_prev >= loss_budget) {
+		lost_prefix = 0;   /* previous losses crossed loss_thresh */
+	} else {
+		lost_prefix = loss_budget - lost_prev;
+		lost_prefix <<= BBR_SCALE;
+		divisor = BBR_UNIT - loss_thresh;
+		if (WARN_ON_ONCE(!divisor))  /* loss_thresh is 8 bits */
+			return ~0U;
+		do_div(lost_prefix, divisor);
+	}
+
+	inflight_hi = inflight_prev + lost_prefix;
+	return inflight_hi;
+}
+
+/* If loss/ECN rates during probing indicated we may have overfilled a
+ * buffer, return an operating point that tries to leave unutilized headroom in
+ * the path for other flows, for fairness convergence and lower RTTs and loss.
+ */
+static u32 bbr_inflight_with_headroom(const struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 headroom, headroom_fraction;
+
+	if (bbr->inflight_hi == ~0U)
+		return ~0U;
+
+	headroom_fraction = bbr_param(sk, inflight_headroom);
+	headroom = ((u64)bbr->inflight_hi * headroom_fraction) >> BBR_SCALE;
+	headroom = max(headroom, 1U);
+	return max_t(s32, bbr->inflight_hi - headroom,
+		     bbr_param(sk, cwnd_min_target));
+}
+
+/* Bound cwnd to a sensible level, based on our current probing state
+ * machine phase and model of a good inflight level (inflight_lo, inflight_hi).
+ */
+static void bbr_bound_cwnd_for_inflight_model(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 cap;
+
+	/* tcp_rcv_synsent_state_process() currently calls tcp_ack()
+	 * and thus cong_control() without first initializing us(!).
+	 */
+	if (!bbr->initialized)
+		return;
+
+	cap = ~0U;
+	if (bbr->mode == BBR_PROBE_BW &&
+	    bbr->cycle_idx != BBR_BW_PROBE_CRUISE) {
+		/* Probe to see if more packets fit in the path. */
+		cap = bbr->inflight_hi;
+	} else {
+		if (bbr->mode == BBR_PROBE_RTT ||
+		    (bbr->mode == BBR_PROBE_BW &&
+		     bbr->cycle_idx == BBR_BW_PROBE_CRUISE))
+			cap = bbr_inflight_with_headroom(sk);
+	}
+	/* Adapt to any loss/ECN since our last bw probe. */
+	cap = min(cap, bbr->inflight_lo);
+
+	cap = max_t(u32, cap, bbr_param(sk, cwnd_min_target));
+	tcp_snd_cwnd_set(tp, min(cap, tcp_snd_cwnd(tp)));
+}
+
+/* How should we multiplicatively cut bw or inflight limits based on ECN? */
+u32 bbr_ecn_cut(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	return BBR_UNIT -
+		((bbr->ecn_alpha * bbr_param(sk, ecn_factor)) >> BBR_SCALE);
+}
+
+/* Init lower bounds if have not inited yet. */
+static void bbr_init_lower_bounds(struct sock *sk, bool init_bw)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (init_bw && bbr->bw_lo == ~0U)
+		bbr->bw_lo = bbr_max_bw(sk);
+	if (bbr->inflight_lo == ~0U)
+		bbr->inflight_lo = tcp_snd_cwnd(tp);
+}
+
+/* Reduce bw and inflight to (1 - beta). */
+static void bbr_loss_lower_bounds(struct sock *sk, u32 *bw, u32 *inflight)
+{
+	struct bbr* bbr = inet_csk_ca(sk);
+	u32 loss_cut = BBR_UNIT - bbr_param(sk, beta);
+
+	*bw = max_t(u32, bbr->bw_latest,
+		    (u64)bbr->bw_lo * loss_cut >> BBR_SCALE);
+	*inflight = max_t(u32, bbr->inflight_latest,
+			  (u64)bbr->inflight_lo * loss_cut >> BBR_SCALE);
+}
+
+/* Reduce inflight to (1 - alpha*ecn_factor). */
+static void bbr_ecn_lower_bounds(struct sock *sk, u32 *inflight)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 ecn_cut = bbr_ecn_cut(sk);
+
+	*inflight = (u64)bbr->inflight_lo * ecn_cut >> BBR_SCALE;
+}
+
+/* Estimate a short-term lower bound on the capacity available now, based
+ * on measurements of the current delivery process and recent history. When we
+ * are seeing loss/ECN at times when we are not probing bw, then conservatively
+ * move toward flow balance by multiplicatively cutting our short-term
+ * estimated safe rate and volume of data (bw_lo and inflight_lo). We use a
+ * multiplicative decrease in order to converge to a lower capacity in time
+ * logarithmic in the magnitude of the decrease.
+ *
+ * However, we do not cut our short-term estimates lower than the current rate
+ * and volume of delivered data from this round trip, since from the current
+ * delivery process we can estimate the measured capacity available now.
+ *
+ * Anything faster than that approach would knowingly risk high loss, which can
+ * cause low bw for Reno/CUBIC and high loss recovery latency for
+ * request/response flows using any congestion control.
+ */
+static void bbr_adapt_lower_bounds(struct sock *sk,
+				    const struct rate_sample *rs)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 ecn_inflight_lo = ~0U;
+
+	/* We only use lower-bound estimates when not probing bw.
+	 * When probing we need to push inflight higher to probe bw.
+	 */
+	if (bbr_is_probing_bandwidth(sk))
+		return;
+
+	/* ECN response. */
+    if (bbr->ecn_in_round) {
+        if (bbr_param(sk, ecn_factor)) {
+            bbr_init_lower_bounds(sk, false);
+            bbr_ecn_lower_bounds(sk, &ecn_inflight_lo);
+        }
+    }
+
+	/* Loss response. */
+	if (bbr->loss_in_round) {
+		bbr_init_lower_bounds(sk, true);
+		bbr_loss_lower_bounds(sk, &bbr->bw_lo, &bbr->inflight_lo);
+	}
+
+	/* Adjust to the lower of the levels implied by loss/ECN. */
+	bbr->inflight_lo = min(bbr->inflight_lo, ecn_inflight_lo);
+	bbr->bw_lo = max(1U, bbr->bw_lo);
+}
+
+/* Reset any short-term lower-bound adaptation to congestion, so that we can
+ * push our inflight up.
+ */
+static void bbr_reset_lower_bounds(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->bw_lo = ~0U;
+	bbr->inflight_lo = ~0U;
+}
+
+/* After bw probing (STARTUP/PROBE_UP), reset signals before entering a state
+ * machine phase where we adapt our lower bound based on congestion signals.
+ */
+static void bbr_reset_congestion_signals(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->loss_in_round = 0;
+	bbr->ecn_in_round = 0;
+	bbr->loss_in_cycle = 0;
+	bbr->ecn_in_cycle = 0;
+	bbr->bw_latest = 0;
+	bbr->inflight_latest = 0;
+}
+
+static void bbr_exit_loss_recovery(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	tcp_snd_cwnd_set(tp, max(tcp_snd_cwnd(tp), bbr->prior_cwnd));
+	bbr->try_fast_path = 0; /* bound cwnd using latest model */
+}
+
+/* Update rate and volume of delivered data from latest round trip. */
+static void bbr_update_latest_delivery_signals(
+	struct sock *sk, const struct rate_sample *rs, struct bbr_context *ctx)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->loss_round_start = 0;
+	if (rs->interval_us <= 0 || !rs->acked_sacked)
+		return; /* Not a valid observation */
+
+	bbr->bw_latest       = max_t(u32, bbr->bw_latest,       ctx->sample_bw);
+	bbr->inflight_latest = max_t(u32, bbr->inflight_latest, rs->delivered);
+
+	if (!before(rs->prior_delivered, bbr->loss_round_delivered)) {
+		bbr->loss_round_delivered = tp->delivered;
+		bbr->loss_round_start = 1;  /* mark start of new round trip */
+	}
+}
+
+/* Once per round, reset filter for latest rate and volume of delivered data. */
+static void bbr_advance_latest_delivery_signals(
+	struct sock *sk, const struct rate_sample *rs, struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	/* If ACK matches a TLP retransmit, persist the filter. If we detect
+	 * that a TLP retransmit plugged a tail loss, we'll want to remember
+	 * how much data the path delivered before the tail loss.
+	 */
+	if (bbr->loss_round_start && !rs->is_acking_tlp_retrans_seq) {
+		bbr->bw_latest = ctx->sample_bw;
+		bbr->inflight_latest = rs->delivered;
+	}
+}
+
+/* Update (most of) our congestion signals: track the recent rate and volume of
+ * delivered data, presence of loss, and EWMA degree of ECN marking.
+ */
+static void bbr_update_congestion_signals(
+	struct sock *sk, const struct rate_sample *rs, struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u64 bw;
+
+	if (rs->interval_us <= 0 || !rs->acked_sacked)
+		return; /* Not a valid observation */
+	bw = ctx->sample_bw;
+
+	if (!rs->is_app_limited || bw >= bbr_max_bw(sk))
+		bbr_take_max_bw_sample(sk, bw);
+
+	bbr->loss_in_round |= (rs->losses > 0);
+
+	if (!bbr->loss_round_start)
+		return;		/* skip the per-round-trip updates */
+	/* Now do per-round-trip updates. */
+	bbr_adapt_lower_bounds(sk, rs);
+
+	bbr->loss_in_round = 0;
+	bbr->ecn_in_round  = 0;
+}
+
+/* Bandwidth probing can cause loss. To help coexistence with loss-based
+ * congestion control we spread out our probing in a Reno-conscious way. Due to
+ * the shape of the Reno sawtooth, the time required between loss epochs for an
+ * idealized Reno flow is a number of round trips that is the BDP of that
+ * flow. We count packet-timed round trips directly, since measured RTT can
+ * vary widely, and Reno is driven by packet-timed round trips.
+ */
+static bool bbr_is_reno_coexistence_probe_time(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 rounds;
+
+	/* Random loss can shave some small percentage off of our inflight
+	 * in each round. To survive this, flows need robust periodic probes.
+	 */
+	rounds = min_t(u32, bbr_param(sk, bw_probe_max_rounds), bbr_target_inflight(sk));
+	return bbr->rounds_since_probe >= rounds;
+}
+
+/* How long do we want to wait before probing for bandwidth (and risking
+ * loss)? We randomize the wait, for better mixing and fairness convergence.
+ *
+ * We bound the Reno-coexistence inter-bw-probe time to be 62-63 round trips.
+ * This is calculated to allow fairness with a 25Mbps, 30ms Reno flow,
+ * (eg 4K video to a broadband user):
+ *   BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
+ *
+ * We bound the BBR-native inter-bw-probe wall clock time to be:
+ *  (a) higher than 2 sec: to try to avoid causing loss for a long enough time
+ *      to allow Reno at 30ms to get 4K video bw, the inter-bw-probe time must
+ *      be at least: 25Mbps * .030sec / (1514bytes) * 0.030sec = 1.9secs
+ *  (b) lower than 3 sec: to ensure flows can start probing in a reasonable
+ *      amount of time to discover unutilized bw on human-scale interactive
+ *      time-scales (e.g. perhaps traffic from a web page download that we
+ *      were competing with is now complete).
+ */
+static void bbr_pick_probe_wait(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	/* Decide the random round-trip bound for wait until probe: */
+	bbr->rounds_since_probe =
+		get_random_u32_below(bbr_param(sk, bw_probe_rand_rounds));
+	/* Decide the random wall clock bound for wait until probe: */
+	bbr->probe_wait_us = bbr_param(sk, bw_probe_base_us) +
+			     get_random_u32_below(bbr_param(sk, bw_probe_rand_us));
+}
+
+static void bbr_set_cycle_idx(struct sock *sk, int cycle_idx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->cycle_idx = cycle_idx;
+	/* New phase, so need to update cwnd and pacing rate. */
+	bbr->try_fast_path = 0;
+}
+
+/* Send at estimated bw to fill the pipe, but not queue. We need this phase
+ * before PROBE_UP, because as soon as we send faster than the available bw
+ * we will start building a queue, and if the buffer is shallow we can cause
+ * loss. If we do not fill the pipe before we cause this loss, our bw_hi and
+ * inflight_hi estimates will underestimate.
+ */
+static void bbr_start_bw_probe_refill(struct sock *sk, u32 bw_probe_up_rounds)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr_reset_lower_bounds(sk);
+	bbr->bw_probe_up_rounds = bw_probe_up_rounds;
+	bbr->bw_probe_up_acks = 0;
+	bbr->stopped_risky_probe = 0;
+	bbr->ack_phase = BBR_ACKS_REFILLING;
+	bbr->next_rtt_delivered = tp->delivered;
+	bbr_set_cycle_idx(sk, BBR_BW_PROBE_REFILL);
+}
+
+/* Now probe max deliverable data rate and volume. */
+static void bbr_start_bw_probe_up(struct sock *sk, struct bbr_context *ctx)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->ack_phase = BBR_ACKS_PROBE_STARTING;
+	bbr->next_rtt_delivered = tp->delivered;
+	bbr->cycle_mstamp = tp->tcp_mstamp;
+	bbr_reset_full_bw(sk);
+	bbr->full_bw = ctx->sample_bw;
+	bbr_set_cycle_idx(sk, BBR_BW_PROBE_UP);
+	bbr_raise_inflight_hi_slope(sk);
+}
+
+/* Start a new PROBE_BW probing cycle of some wall clock length. Pick a wall
+ * clock time at which to probe beyond an inflight that we think to be
+ * safe. This will knowingly risk packet loss, so we want to do this rarely, to
+ * keep packet loss rates low. Also start a round-trip counter, to probe faster
+ * if we estimate a Reno flow at our BDP would probe faster.
+ */
+static void bbr_start_bw_probe_down(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr_reset_congestion_signals(sk);
+	bbr->bw_probe_up_cnt = ~0U;     /* not growing inflight_hi any more */
+	bbr_pick_probe_wait(sk);
+	bbr->cycle_mstamp = tp->tcp_mstamp;		/* start wall clock */
+	bbr->ack_phase = BBR_ACKS_PROBE_STOPPING;
+	bbr->next_rtt_delivered = tp->delivered;
+	bbr_set_cycle_idx(sk, BBR_BW_PROBE_DOWN);
+}
+
+/* Cruise: maintain what we estimate to be a neutral, conservative
+ * operating point, without attempting to probe up for bandwidth or down for
+ * RTT, and only reducing inflight in response to loss/ECN signals.
+ */
+static void bbr_start_bw_probe_cruise(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (bbr->inflight_lo != ~0U)
+		bbr->inflight_lo = min(bbr->inflight_lo, bbr->inflight_hi);
+
+	bbr_set_cycle_idx(sk, BBR_BW_PROBE_CRUISE);
+}
+
+/* Loss and/or ECN rate is too high while probing.
+ * Adapt (once per bw probe) by cutting inflight_hi and then restarting cycle.
+ */
+static void bbr_handle_inflight_too_high(struct sock *sk,
+					  const struct rate_sample *rs)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	const u32 beta = bbr_param(sk, beta);
+
+	bbr->prev_probe_too_high = 1;
+	bbr->bw_probe_samples = 0;  /* only react once per probe */
+	/* If we are app-limited then we are not robustly
+	 * probing the max volume of inflight data we think
+	 * might be safe (analogous to how app-limited bw
+	 * samples are not known to be robustly probing bw).
+	 */
+	if (!rs->is_app_limited) {
+		bbr->inflight_hi = max_t(u32, rs->tx_in_flight,
+					 (u64)bbr_target_inflight(sk) *
+					 (BBR_UNIT - beta) >> BBR_SCALE);
+	}
+	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == BBR_BW_PROBE_UP)
+		bbr_start_bw_probe_down(sk);
+}
+
+/* If we're seeing bw and loss samples reflecting our bw probing, adapt
+ * using the signals we see. If loss or ECN mark rate gets too high, then adapt
+ * inflight_hi downward. If we're able to push inflight higher without such
+ * signals, push higher: adapt inflight_hi upward.
+ */
+static bool bbr_adapt_upper_bounds(struct sock *sk,
+				    const struct rate_sample *rs,
+				    struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	/* Track when we'll see bw/loss samples resulting from our bw probes. */
+	if (bbr->ack_phase == BBR_ACKS_PROBE_STARTING && bbr->round_start)
+		bbr->ack_phase = BBR_ACKS_PROBE_FEEDBACK;
+	if (bbr->ack_phase == BBR_ACKS_PROBE_STOPPING && bbr->round_start) {
+		/* End of samples from bw probing phase. */
+		bbr->bw_probe_samples = 0;
+		bbr->ack_phase = BBR_ACKS_INIT;
+		/* At this point in the cycle, our current bw sample is also
+		 * our best recent chance at finding the highest available bw
+		 * for this flow. So now is the best time to forget the bw
+		 * samples from the previous cycle, by advancing the window.
+		 */
+		if (bbr->mode == BBR_PROBE_BW && !rs->is_app_limited)
+			bbr_advance_max_bw_filter(sk);
+		/* If we had an inflight_hi, then probed and pushed inflight all
+		 * the way up to hit that inflight_hi without seeing any
+		 * high loss/ECN in all the resulting ACKs from that probing,
+		 * then probe up again, this time letting inflight persist at
+		 * inflight_hi for a round trip, then accelerating beyond.
+		 */
+		if (bbr->mode == BBR_PROBE_BW &&
+		    bbr->stopped_risky_probe && !bbr->prev_probe_too_high) {
+			bbr_start_bw_probe_refill(sk, 0);
+			return true;  /* yes, decided state transition */
+		}
+	}
+	if (bbr_is_inflight_too_high(sk, rs)) {
+		if (bbr->bw_probe_samples)  /*  sample is from bw probing? */
+			bbr_handle_inflight_too_high(sk, rs);
+	} else {
+		/* Loss/ECN rate is declared safe. Adjust upper bound upward. */
+
+		if (bbr->inflight_hi == ~0U)
+			return false;   /* no excess queue signals yet */
+
+		/* To be resilient to random loss, we must raise bw/inflight_hi
+		 * if we observe in any phase that a higher level is safe.
+		 */
+		if (rs->tx_in_flight > bbr->inflight_hi) {
+			bbr->inflight_hi = rs->tx_in_flight;
+		}
+
+		if (bbr->mode == BBR_PROBE_BW &&
+		    bbr->cycle_idx == BBR_BW_PROBE_UP)
+			bbr_probe_inflight_hi_upward(sk, rs);
+	}
+
+	return false;
+}
+
+/* Check if it's time to probe for bandwidth now, and if so, kick it off. */
+static bool bbr_check_time_to_probe_bw(struct sock *sk,
+					const struct rate_sample *rs)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 n;
+
+	/* If we seem to be at an operating point where we are not seeing loss
+	 * but we are seeing ECN marks, then when the ECN marks cease we reprobe
+	 * quickly (in case cross-traffic has ceased and freed up bw).
+	 */
+	if (bbr_param(sk, ecn_reprobe_gain) && bbr->ecn_eligible &&
+	    bbr->ecn_in_cycle && !bbr->loss_in_cycle &&
+	    inet_csk(sk)->icsk_ca_state == TCP_CA_Open) {
+		/* Calculate n so that when bbr_raise_inflight_hi_slope()
+		 * computes growth_this_round as 2^n it will be roughly the
+		 * desired volume of data (inflight_hi*ecn_reprobe_gain).
+		 */
+		n = ilog2((((u64)bbr->inflight_hi *
+			    bbr_param(sk, ecn_reprobe_gain)) >> BBR_SCALE));
+		bbr_start_bw_probe_refill(sk, n);
+		return true;
+	}
+
+	if (bbr_has_elapsed_in_phase(sk, bbr->probe_wait_us) ||
+	    bbr_is_reno_coexistence_probe_time(sk)) {
+		bbr_start_bw_probe_refill(sk, 0);
+		return true;
+	}
+	return false;
+}
+
+/* Is it time to transition from PROBE_DOWN to PROBE_CRUISE? */
+static bool bbr_check_time_to_cruise(struct sock *sk, u32 inflight, u32 bw)
+{
+	/* Always need to pull inflight down to leave headroom in queue. */
+	if (inflight > bbr_inflight_with_headroom(sk))
+		return false;
+
+	return inflight <= bbr_inflight(sk, bw, BBR_UNIT);
+}
+
+/* PROBE_BW state machine: cruise, refill, probe for bw, or drain? */
+static void bbr_update_cycle_phase(struct sock *sk,
+				    const struct rate_sample *rs,
+				    struct bbr_context *ctx)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	bool is_bw_probe_done = false;
+	u32 inflight, bw;
+
+	if (!bbr_full_bw_reached(sk))
+		return;
+
+	/* In DRAIN, PROBE_BW, or PROBE_RTT, adjust upper bounds. */
+	if (bbr_adapt_upper_bounds(sk, rs, ctx))
+		return;		/* already decided state transition */
+
+	if (bbr->mode != BBR_PROBE_BW)
+		return;
+
+	inflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);
+	bw = bbr_max_bw(sk);
+
+	switch (bbr->cycle_idx) {
+	/* First we spend most of our time cruising with a pacing_gain of 1.0,
+	 * which paces at the estimated bw, to try to fully use the pipe
+	 * without building queue. If we encounter loss/ECN marks, we adapt
+	 * by slowing down.
+	 */
+	case BBR_BW_PROBE_CRUISE:
+		if (bbr_check_time_to_probe_bw(sk, rs))
+			return;		/* already decided state transition */
+		break;
+
+	/* After cruising, when it's time to probe, we first "refill": we send
+	 * at the estimated bw to fill the pipe, before probing higher and
+	 * knowingly risking overflowing the bottleneck buffer (causing loss).
+	 */
+	case BBR_BW_PROBE_REFILL:
+		if (bbr->round_start) {
+			/* After one full round trip of sending in REFILL, we
+			 * start to see bw samples reflecting our REFILL, which
+			 * may be putting too much data in flight.
+			 */
+			bbr->bw_probe_samples = 1;
+			bbr_start_bw_probe_up(sk, ctx);
+		}
+		break;
+
+	/* After we refill the pipe, we probe by using a pacing_gain > 1.0, to
+	 * probe for bw. If we have not seen loss/ECN, we try to raise inflight
+	 * to at least pacing_gain*BDP; note that this may take more than
+	 * min_rtt if min_rtt is small (e.g. on a LAN).
+	 *
+	 * We terminate PROBE_UP bandwidth probing upon any of the following:
+	 *
+	 * (1) We've pushed inflight up to hit the inflight_hi target set in the
+	 *     most recent previous bw probe phase. Thus we want to start
+	 *     draining the queue immediately because it's very likely the most
+	 *     recently sent packets will fill the queue and cause drops.
+	 * (2) If inflight_hi has not limited bandwidth growth recently, and
+	 *     yet delivered bandwidth has not increased much recently
+	 *     (bbr->full_bw_now).
+	 * (3) Loss filter says loss rate is "too high".
+	 * (4) ECN filter says ECN mark rate is "too high".
+	 *
+	 * (1) (2) checked here, (3) (4) checked in bbr_is_inflight_too_high()
+	 */
+	case BBR_BW_PROBE_UP:
+		if (bbr->prev_probe_too_high &&
+		    inflight >= bbr->inflight_hi) {
+			bbr->stopped_risky_probe = 1;
+			is_bw_probe_done = true;
+		} else {
+			if (tp->is_cwnd_limited &&
+			    tcp_snd_cwnd(tp) >= bbr->inflight_hi) {
+				/* inflight_hi is limiting bw growth */
+				bbr_reset_full_bw(sk);
+				bbr->full_bw = ctx->sample_bw;
+			} else if (bbr->full_bw_now) {
+				/* Plateau in estimated bw. Pipe looks full. */
+				is_bw_probe_done = true;
+			}
+		}
+		if (is_bw_probe_done) {
+			bbr->prev_probe_too_high = 0;  /* no loss/ECN (yet) */
+			bbr_start_bw_probe_down(sk);  /* restart w/ down */
+		}
+		break;
+
+	/* After probing in PROBE_UP, we have usually accumulated some data in
+	 * the bottleneck buffer (if bw probing didn't find more bw). We next
+	 * enter PROBE_DOWN to try to drain any excess data from the queue. To
+	 * do this, we use a pacing_gain < 1.0. We hold this pacing gain until
+	 * our inflight is less then that target cruising point, which is the
+	 * minimum of (a) the amount needed to leave headroom, and (b) the
+	 * estimated BDP. Once inflight falls to match the target, we estimate
+	 * the queue is drained; persisting would underutilize the pipe.
+	 */
+	case BBR_BW_PROBE_DOWN:
+		if (bbr_check_time_to_probe_bw(sk, rs))
+			return;		/* already decided state transition */
+		if (bbr_check_time_to_cruise(sk, inflight, bw))
+			bbr_start_bw_probe_cruise(sk);
+		break;
+
+	default:
+		WARN_ONCE(1, "BBR invalid cycle index %u\n", bbr->cycle_idx);
+	}
+}
+
+/* Exiting PROBE_RTT, so return to bandwidth probing in STARTUP or PROBE_BW. */
+static void bbr_exit_probe_rtt(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr_reset_lower_bounds(sk);
+	if (bbr_full_bw_reached(sk)) {
+		bbr->mode = BBR_PROBE_BW;
+		/* Raising inflight after PROBE_RTT may cause loss, so reset
+		 * the PROBE_BW clock and schedule the next bandwidth probe for
+		 * a friendly and randomized future point in time.
+		 */
+		bbr_start_bw_probe_down(sk);
+		/* Since we are exiting PROBE_RTT, we know inflight is
+		 * below our estimated BDP, so it is reasonable to cruise.
+		 */
+		bbr_start_bw_probe_cruise(sk);
+	} else {
+		bbr->mode = BBR_STARTUP;
+	}
+}
+
+/* Exit STARTUP based on loss rate > 1% and loss gaps in round >= N. Wait until
+ * the end of the round in recovery to get a good estimate of how many packets
+ * have been lost, and how many we need to drain with a low pacing rate.
+ */
+static void bbr_check_loss_too_high_in_startup(struct sock *sk,
+						const struct rate_sample *rs)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (bbr_full_bw_reached(sk))
+		return;
+
+	/* For STARTUP exit, check the loss rate at the end of each round trip
+	 * of Recovery episodes in STARTUP. We check the loss rate at the end
+	 * of the round trip to filter out noisy/low loss and have a better
+	 * sense of inflight (extent of loss), so we can drain more accurately.
+	 */
+	if (rs->losses && bbr->loss_events_in_round < 0xf)
+		bbr->loss_events_in_round++;  /* update saturating counter */
+	if (bbr_param(sk, full_loss_cnt) && bbr->loss_round_start &&
+	    inet_csk(sk)->icsk_ca_state == TCP_CA_Recovery &&
+	    bbr->loss_events_in_round >= bbr_param(sk, full_loss_cnt) &&
+	    bbr_is_inflight_too_high(sk, rs)) {
+		bbr_handle_queue_too_high_in_startup(sk);
+		return;
+	}
+	if (bbr->loss_round_start)
+		bbr->loss_events_in_round = 0;
+}
+
+/* Estimate when the pipe is full, using the change in delivery rate: BBR
+ * estimates bw probing filled the pipe if the estimated bw hasn't changed by
+ * at least bbr_full_bw_thresh (25%) after bbr_full_bw_cnt (3) non-app-limited
+ * rounds. Why 3 rounds: 1: rwin autotuning grows the rwin, 2: we fill the
+ * higher rwin, 3: we get higher delivery rate samples. Or transient
+ * cross-traffic or radio noise can go away. CUBIC Hystart shares a similar
+ * design goal, but uses delay and inter-ACK spacing instead of bandwidth.
+ */
+static void bbr_check_full_bw_reached(struct sock *sk,
+				       const struct rate_sample *rs,
+				       struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 bw_thresh, full_cnt, thresh;
+
+	if (bbr->full_bw_now || rs->is_app_limited)
+		return;
+
+	thresh = bbr_param(sk, full_bw_thresh);
+	full_cnt = bbr_param(sk, full_bw_cnt);
+	bw_thresh = (u64)bbr->full_bw * thresh >> BBR_SCALE;
+	if (ctx->sample_bw >= bw_thresh) {
+		bbr_reset_full_bw(sk);
+		bbr->full_bw = ctx->sample_bw;
+		return;
+	}
+	if (!bbr->round_start)
+		return;
+	++bbr->full_bw_cnt;
+	bbr->full_bw_now = bbr->full_bw_cnt >= full_cnt;
+	bbr->full_bw_reached |= bbr->full_bw_now;
+}
+
+/* If pipe is probably full, drain the queue and then enter steady-state. */
+static void bbr_check_drain(struct sock *sk, const struct rate_sample *rs,
+			    struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	if (bbr->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {
+		bbr->mode = BBR_DRAIN;	/* drain queue we created */
+		/* Set ssthresh to export purely for monitoring, to signal
+		 * completion of initial STARTUP by setting to a non-
+		 * TCP_INFINITE_SSTHRESH value (ssthresh is not used by BBR).
+		 */
+		tcp_sk(sk)->snd_ssthresh =
+				bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
+		bbr_reset_congestion_signals(sk);
+	}	/* fall through to check if in-flight is already small: */
+	if (bbr->mode == BBR_DRAIN &&
+	    bbr_packets_in_net_at_edt(sk, tcp_packets_in_flight(tcp_sk(sk))) <=
+	    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT)) {
+		bbr->mode = BBR_PROBE_BW;
+		bbr_start_bw_probe_down(sk);
+	}
+}
+
+static void bbr_update_model(struct sock *sk, const struct rate_sample *rs,
+			      struct bbr_context *ctx)
+{
+	bbr_update_congestion_signals(sk, rs, ctx);
+	bbr_update_ack_aggregation(sk, rs);
+	bbr_check_loss_too_high_in_startup(sk, rs);
+	bbr_check_full_bw_reached(sk, rs, ctx);
+	bbr_check_drain(sk, rs, ctx);
+	bbr_update_cycle_phase(sk, rs, ctx);
+	bbr_update_min_rtt(sk, rs);
+}
+
+/* Fast path for app-limited case.
+ *
+ * On each ack, we execute bbr state machine, which primarily consists of:
+ * 1) update model based on new rate sample, and
+ * 2) update control based on updated model or state change.
+ *
+ * There are certain workload/scenarios, e.g. app-limited case, where
+ * either we can skip updating model or we can skip update of both model
+ * as well as control. This provides signifcant softirq cpu savings for
+ * processing incoming acks.
+ *
+ * In case of app-limited, if there is no congestion (loss/ecn) and
+ * if observed bw sample is less than current estimated bw, then we can
+ * skip some of the computation in bbr state processing:
+ *
+ * - if there is no rtt/mode/phase change: In this case, since all the
+ *   parameters of the network model are constant, we can skip model
+ *   as well control update.
+ *
+ * - else we can skip rest of the model update. But we still need to
+ *   update the control to account for the new rtt/mode/phase.
+ *
+ * Returns whether we can take fast path or not.
+ */
+static bool bbr_run_fast_path(struct sock *sk, bool *update_model,
+		const struct rate_sample *rs, struct bbr_context *ctx)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 prev_min_rtt_us, prev_mode;
+
+	if (bbr_param(sk, fast_path) && bbr->try_fast_path &&
+	    rs->is_app_limited && ctx->sample_bw < bbr_max_bw(sk) &&
+	    !bbr->loss_in_round && !bbr->ecn_in_round ) {
+		prev_mode = bbr->mode;
+		prev_min_rtt_us = bbr->min_rtt_us;
+		bbr_check_drain(sk, rs, ctx);
+		bbr_update_cycle_phase(sk, rs, ctx);
+		bbr_update_min_rtt(sk, rs);
+
+		if (bbr->mode == prev_mode &&
+		    bbr->min_rtt_us == prev_min_rtt_us &&
+		    bbr->try_fast_path) {
+			return true;
+		}
+
+		/* Skip model update, but control still needs to be updated */
+		*update_model = false;
+	}
+	return false;
+}
+
+void bbr_main(struct sock *sk, const struct rate_sample *rs)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	struct bbr_context ctx = { 0 };
+	bool update_model = true;
+	u32 bw, round_delivered;
+	int ce_ratio = -1;
+
+	round_delivered = bbr_update_round_start(sk, rs, &ctx);
+	if (bbr->round_start) {
+		bbr->rounds_since_probe =
+			min_t(s32, bbr->rounds_since_probe + 1, 0xFF);
+		ce_ratio = bbr_update_ecn_alpha(sk);
+	}
+	bbr_plb(sk, rs, ce_ratio);
+
+	bbr->ecn_in_round  |= (bbr->ecn_eligible && rs->is_ece);
+	bbr_calculate_bw_sample(sk, rs, &ctx);
+	bbr_update_latest_delivery_signals(sk, rs, &ctx);
+
+	if (bbr_run_fast_path(sk, &update_model, rs, &ctx))
+		goto out;
+
+	if (update_model)
+		bbr_update_model(sk, rs, &ctx);
+
+	bbr_update_gains(sk);
+	bw = bbr_bw(sk);
+	bbr_set_pacing_rate(sk, bw, bbr->pacing_gain);
+	bbr_set_cwnd(sk, rs, rs->acked_sacked, bw, bbr->cwnd_gain,
+		     tcp_snd_cwnd(tp), &ctx);
+	bbr_bound_cwnd_for_inflight_model(sk);
+
+out:
+	bbr_advance_latest_delivery_signals(sk, rs, &ctx);
+	bbr->prev_ca_state = inet_csk(sk)->icsk_ca_state;
+	bbr->loss_in_cycle |= rs->lost > 0;
+	bbr->ecn_in_cycle  |= rs->delivered_ce > 0;
+}
+
+static void bbr_init(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr->initialized = 1;
+
+	bbr->init_cwnd = min(0x7FU, tcp_snd_cwnd(tp));
+	bbr->prior_cwnd = tp->prior_cwnd;
+	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
+	bbr->next_rtt_delivered = tp->delivered;
+	bbr->prev_ca_state = TCP_CA_Open;
+
+	bbr->probe_rtt_done_stamp = 0;
+	bbr->probe_rtt_round_done = 0;
+	bbr->probe_rtt_min_us = tcp_min_rtt(tp);
+	bbr->probe_rtt_min_stamp = tcp_jiffies32;
+	bbr->min_rtt_us = tcp_min_rtt(tp);
+	bbr->min_rtt_stamp = tcp_jiffies32;
+
+	bbr->has_seen_rtt = 0;
+	bbr_init_pacing_rate_from_rtt(sk);
+
+	bbr->round_start = 0;
+	bbr->idle_restart = 0;
+	bbr->full_bw_reached = 0;
+	bbr->full_bw = 0;
+	bbr->full_bw_cnt = 0;
+	bbr->cycle_mstamp = 0;
+	bbr->cycle_idx = 0;
+
+	bbr_reset_startup_mode(sk);
+
+	bbr->ack_epoch_mstamp = tp->tcp_mstamp;
+	bbr->ack_epoch_acked = 0;
+	bbr->extra_acked_win_rtts = 0;
+	bbr->extra_acked_win_idx = 0;
+	bbr->extra_acked[0] = 0;
+	bbr->extra_acked[1] = 0;
+
+	bbr->ce_state = 0;
+	bbr->prior_rcv_nxt = tp->rcv_nxt;
+	bbr->try_fast_path = 0;
+
+	cmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);
+
+	/* Start sampling ECN mark rate after first full flight is ACKed: */
+	bbr->loss_round_delivered = tp->delivered + 1;
+	bbr->loss_round_start = 0;
+	bbr->undo_bw_lo = 0;
+	bbr->undo_inflight_lo = 0;
+	bbr->undo_inflight_hi = 0;
+	bbr->loss_events_in_round = 0;
+	bbr->startup_ecn_rounds = 0;
+	bbr_reset_congestion_signals(sk);
+	bbr->bw_lo = ~0U;
+	bbr->bw_hi[0] = 0;
+	bbr->bw_hi[1] = 0;
+	bbr->inflight_lo = ~0U;
+	bbr->inflight_hi = ~0U;
+	bbr_reset_full_bw(sk);
+	bbr->bw_probe_up_cnt = ~0U;
+	bbr->bw_probe_up_acks = 0;
+	bbr->bw_probe_up_rounds = 0;
+	bbr->probe_wait_us = 0;
+	bbr->stopped_risky_probe = 0;
+	bbr->ack_phase = BBR_ACKS_INIT;
+	bbr->rounds_since_probe = 0;
+	bbr->bw_probe_samples = 0;
+	bbr->prev_probe_too_high = 0;
+	bbr->ecn_eligible = 0;
+	bbr->ecn_alpha = bbr_param(sk, ecn_alpha_init);
+	bbr->alpha_last_delivered = 0;
+	bbr->alpha_last_delivered_ce = 0;
+	bbr->plb.pause_until = 0;
+
+	tp->fast_ack_mode = bbr_fast_ack_mode ? 1 : 0;
+
+	if (bbr_can_use_ecn(sk))
+		tp->ecn_flags |= TCP_ECN_ECT_PERMANENT;
+}
+
+/* BBR marks the current round trip as a loss round. */
+static void bbr_note_loss(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	/* Capture "current" data over the full round trip of loss, to
+	 * have a better chance of observing the full capacity of the path.
+	 */
+	if (!bbr->loss_in_round)  /* first loss in this round trip? */
+		bbr->loss_round_delivered = tp->delivered;  /* set round trip */
+	bbr->loss_in_round = 1;
+	bbr->loss_in_cycle = 1;
+}
+
+/* Core TCP stack informs us that the given skb was just marked lost. */
+static void bbr_skb_marked_lost(struct sock *sk,
+					    const struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	struct tcp_skb_cb *scb = TCP_SKB_CB(skb);
+	struct rate_sample rs = {};
+
+	bbr_note_loss(sk);
+
+	if (!bbr->bw_probe_samples)
+		return;  /* not an skb sent while probing for bandwidth */
+	if (unlikely(!scb->tx.delivered_mstamp))
+		return;  /* skb was SACKed, reneged, marked lost; ignore it */
+	/* We are probing for bandwidth. Construct a rate sample that
+	 * estimates what happened in the flight leading up to this lost skb,
+	 * then see if the loss rate went too high, and if so at which packet.
+	 */
+	rs.tx_in_flight = scb->tx.in_flight;
+	rs.lost = tp->lost - scb->tx.lost;
+	rs.is_app_limited = scb->tx.is_app_limited;
+	if (bbr_is_inflight_too_high(sk, &rs)) {
+		rs.tx_in_flight = bbr_inflight_hi_from_lost_skb(sk, &rs, skb);
+		bbr_handle_inflight_too_high(sk, &rs);
+	}
+}
+
+static void bbr_run_loss_probe_recovery(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct bbr *bbr = inet_csk_ca(sk);
+	struct rate_sample rs = {0};
+
+	bbr_note_loss(sk);
+
+	if (!bbr->bw_probe_samples)
+		return;  /* not sent while probing for bandwidth */
+	/* We are probing for bandwidth. Construct a rate sample that
+	 * estimates what happened in the flight leading up to this
+	 * loss, then see if the loss rate went too high.
+	 */
+	rs.lost = 1;	/* TLP probe repaired loss of a single segment */
+	rs.tx_in_flight = bbr->inflight_latest + rs.lost;
+	rs.is_app_limited = tp->tlp_orig_data_app_limited;
+	if (bbr_is_inflight_too_high(sk, &rs))
+		bbr_handle_inflight_too_high(sk, &rs);
+}
+
+/* Revert short-term model if current loss recovery event was spurious. */
+static u32 bbr_undo_cwnd(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+
+	bbr_reset_full_bw(sk); /* spurious slow-down; reset full bw detector */
+	bbr->loss_in_round = 0;
+
+	/* Revert to cwnd and other state saved before loss episode. */
+	bbr->bw_lo = max(bbr->bw_lo, bbr->undo_bw_lo);
+	bbr->inflight_lo = max(bbr->inflight_lo, bbr->undo_inflight_lo);
+	bbr->inflight_hi = max(bbr->inflight_hi, bbr->undo_inflight_hi);
+	bbr->try_fast_path = 0;  /* take slow path to set proper cwnd, pacing */
+	return bbr->prior_cwnd;
+}
+
+/* Entering loss recovery, so save state for when we undo recovery. */
 static u32 bbr_ssthresh(struct sock *sk)
 {
+	struct bbr *bbr = inet_csk_ca(sk);
+
 	bbr_save_cwnd(sk);
+	/* For undo, save state that adapts based on loss signal. */
+	bbr->undo_bw_lo		= bbr->bw_lo;
+	bbr->undo_inflight_lo	= bbr->inflight_lo;
+	bbr->undo_inflight_hi	= bbr->inflight_hi;
 	return tcp_sk(sk)->snd_ssthresh;
 }
 
+static enum tcp_bbr_phase bbr_get_phase(struct bbr *bbr)
+{
+	switch (bbr->mode) {
+	case BBR_STARTUP:
+		return BBR_PHASE_STARTUP;
+	case BBR_DRAIN:
+		return BBR_PHASE_DRAIN;
+	case BBR_PROBE_BW:
+		break;
+	case BBR_PROBE_RTT:
+		return BBR_PHASE_PROBE_RTT;
+	default:
+		return BBR_PHASE_INVALID;
+	}
+	switch (bbr->cycle_idx) {
+	case BBR_BW_PROBE_UP:
+		return BBR_PHASE_PROBE_BW_UP;
+	case BBR_BW_PROBE_DOWN:
+		return BBR_PHASE_PROBE_BW_DOWN;
+	case BBR_BW_PROBE_CRUISE:
+		return BBR_PHASE_PROBE_BW_CRUISE;
+	case BBR_BW_PROBE_REFILL:
+		return BBR_PHASE_PROBE_BW_REFILL;
+	default:
+		return BBR_PHASE_INVALID;
+	}
+}
+
 static size_t bbr_get_info(struct sock *sk, u32 ext, int *attr,
 			   union tcp_cc_info *info)
 {
 	if (ext & (1 << (INET_DIAG_BBRINFO - 1)) ||
 	    ext & (1 << (INET_DIAG_VEGASINFO - 1))) {
-		struct tcp_sock *tp = tcp_sk(sk);
 		struct bbr *bbr = inet_csk_ca(sk);
-		u64 bw = bbr_bw(sk);
-
-		bw = bw * tp->mss_cache * USEC_PER_SEC >> BW_SCALE;
-		memset(&info->bbr, 0, sizeof(info->bbr));
-		info->bbr.bbr_bw_lo		= (u32)bw;
-		info->bbr.bbr_bw_hi		= (u32)(bw >> 32);
-		info->bbr.bbr_min_rtt		= bbr->min_rtt_us;
-		info->bbr.bbr_pacing_gain	= bbr->pacing_gain;
-		info->bbr.bbr_cwnd_gain		= bbr->cwnd_gain;
+		u64 bw = bbr_bw_bytes_per_sec(sk, bbr_bw(sk));
+		u64 bw_hi = bbr_bw_bytes_per_sec(sk, bbr_max_bw(sk));
+		u64 bw_lo = bbr->bw_lo == ~0U ?
+			~0ULL : bbr_bw_bytes_per_sec(sk, bbr->bw_lo);
+		struct tcp_bbr_info *bbr_info = &info->bbr;
+
+		memset(bbr_info, 0, sizeof(*bbr_info));
+		bbr_info->bbr_bw_lo		= (u32)bw;
+		bbr_info->bbr_bw_hi		= (u32)(bw >> 32);
+		bbr_info->bbr_min_rtt		= bbr->min_rtt_us;
+		bbr_info->bbr_pacing_gain	= bbr->pacing_gain;
+		bbr_info->bbr_cwnd_gain		= bbr->cwnd_gain;
+		bbr_info->bbr_bw_hi_lsb		= (u32)bw_hi;
+		bbr_info->bbr_bw_hi_msb		= (u32)(bw_hi >> 32);
+		bbr_info->bbr_bw_lo_lsb		= (u32)bw_lo;
+		bbr_info->bbr_bw_lo_msb		= (u32)(bw_lo >> 32);
+		bbr_info->bbr_mode		= bbr->mode;
+		bbr_info->bbr_phase		= (__u8)bbr_get_phase(bbr);
+		bbr_info->bbr_version		= (__u8)BBR_VERSION;
+		bbr_info->bbr_inflight_lo	= bbr->inflight_lo;
+		bbr_info->bbr_inflight_hi	= bbr->inflight_hi;
+		bbr_info->bbr_extra_acked	= bbr_extra_acked(sk);
 		*attr = INET_DIAG_BBRINFO;
-		return sizeof(info->bbr);
+		return sizeof(*bbr_info);
 	}
 	return 0;
 }
 
 static void bbr_set_state(struct sock *sk, u8 new_state)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 
 	if (new_state == TCP_CA_Loss) {
-		struct rate_sample rs = { .losses = 1 };
 
 		bbr->prev_ca_state = TCP_CA_Loss;
-		bbr->full_bw = 0;
-		bbr->round_start = 1;	/* treat RTO like end of a round */
-		bbr_lt_bw_sampling(sk, &rs);
+		tcp_plb_update_state_upon_rto(sk, &bbr->plb);
+		/* The tcp_write_timeout() call to sk_rethink_txhash() likely
+		 * repathed this flow, so re-learn the min network RTT on the
+		 * new path:
+		 */
+		bbr_reset_full_bw(sk);
+		if (!bbr_is_probing_bandwidth(sk) && bbr->inflight_lo == ~0U) {
+			/* bbr_adapt_lower_bounds() needs cwnd before
+			 * we suffered an RTO, to update inflight_lo:
+			 */
+			bbr->inflight_lo =
+				max(tcp_snd_cwnd(tp), bbr->prior_cwnd);
+		}
+	} else if (bbr->prev_ca_state == TCP_CA_Loss &&
+		   new_state != TCP_CA_Loss) {
+		bbr_exit_loss_recovery(sk);
 	}
 }
 
+
 static struct tcp_congestion_ops tcp_bbr_cong_ops __read_mostly = {
-	.flags		= TCP_CONG_NON_RESTRICTED,
+	.flags		= TCP_CONG_NON_RESTRICTED | TCP_CONG_WANTS_CE_EVENTS,
 	.name		= "bbr",
 	.owner		= THIS_MODULE,
 	.init		= bbr_init,
 	.cong_control	= bbr_main,
 	.sndbuf_expand	= bbr_sndbuf_expand,
+	.skb_marked_lost = bbr_skb_marked_lost,
 	.undo_cwnd	= bbr_undo_cwnd,
 	.cwnd_event	= bbr_cwnd_event,
 	.ssthresh	= bbr_ssthresh,
@@ -1184,5 +2379,12 @@ MODULE_AUTHOR("Van Jacobson <vanj@google.com>");
 MODULE_AUTHOR("Neal Cardwell <ncardwell@google.com>");
 MODULE_AUTHOR("Yuchung Cheng <ycheng@google.com>");
 MODULE_AUTHOR("Soheil Hassas Yeganeh <soheil@google.com>");
+MODULE_AUTHOR("Priyaranjan Jha <priyarjha@google.com>");
+MODULE_AUTHOR("Yousuk Seung <ysseung@google.com>");
+MODULE_AUTHOR("Kevin Yang <yyd@google.com>");
+MODULE_AUTHOR("Arjun Roy <arjunroy@google.com>");
+MODULE_AUTHOR("David Morley <morleyd@google.com>");
+
 MODULE_LICENSE("Dual BSD/GPL");
 MODULE_DESCRIPTION("TCP BBR (Bottleneck Bandwidth and RTT)");
+MODULE_VERSION(__stringify(BBR_VERSION));
diff --git a/net/ipv4/tcp_bbr2.c b/net/ipv4/tcp_bbr2.c
deleted file mode 100644
index 09319695da82..000000000000
--- a/net/ipv4/tcp_bbr2.c
+++ /dev/null
@@ -1,2692 +0,0 @@
-/* BBR (Bottleneck Bandwidth and RTT) congestion control, v2
- *
- * BBRv2 is a model-based congestion control algorithm that aims for low
- * queues, low loss, and (bounded) Reno/CUBIC coexistence. To maintain a model
- * of the network path, it uses measurements of bandwidth and RTT, as well as
- * (if they occur) packet loss and/or DCTCP/L4S-style ECN signals.  Note that
- * although it can use ECN or loss signals explicitly, it does not require
- * either; it can bound its in-flight data based on its estimate of the BDP.
- *
- * The model has both higher and lower bounds for the operating range:
- *   lo: bw_lo, inflight_lo: conservative short-term lower bound
- *   hi: bw_hi, inflight_hi: robust long-term upper bound
- * The bandwidth-probing time scale is (a) extended dynamically based on
- * estimated BDP to improve coexistence with Reno/CUBIC; (b) bounded by
- * an interactive wall-clock time-scale to be more scalable and responsive
- * than Reno and CUBIC.
- *
- * Here is a state transition diagram for BBR:
- *
- *             |
- *             V
- *    +---> STARTUP  ----+
- *    |        |         |
- *    |        V         |
- *    |      DRAIN   ----+
- *    |        |         |
- *    |        V         |
- *    +---> PROBE_BW ----+
- *    |      ^    |      |
- *    |      |    |      |
- *    |      +----+      |
- *    |                  |
- *    +---- PROBE_RTT <--+
- *
- * A BBR flow starts in STARTUP, and ramps up its sending rate quickly.
- * When it estimates the pipe is full, it enters DRAIN to drain the queue.
- * In steady state a BBR flow only uses PROBE_BW and PROBE_RTT.
- * A long-lived BBR flow spends the vast majority of its time remaining
- * (repeatedly) in PROBE_BW, fully probing and utilizing the pipe's bandwidth
- * in a fair manner, with a small, bounded queue. *If* a flow has been
- * continuously sending for the entire min_rtt window, and hasn't seen an RTT
- * sample that matches or decreases its min_rtt estimate for 10 seconds, then
- * it briefly enters PROBE_RTT to cut inflight to a minimum value to re-probe
- * the path's two-way propagation delay (min_rtt). When exiting PROBE_RTT, if
- * we estimated that we reached the full bw of the pipe then we enter PROBE_BW;
- * otherwise we enter STARTUP to try to fill the pipe.
- *
- * BBR is described in detail in:
- *   "BBR: Congestion-Based Congestion Control",
- *   Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh,
- *   Van Jacobson. ACM Queue, Vol. 14 No. 5, September-October 2016.
- *
- * There is a public e-mail list for discussing BBR development and testing:
- *   https://groups.google.com/forum/#!forum/bbr-dev
- *
- * NOTE: BBR might be used with the fq qdisc ("man tc-fq") with pacing enabled,
- * otherwise TCP stack falls back to an internal pacing using one high
- * resolution timer per TCP socket and may use more resources.
- */
-#include <linux/module.h>
-#include <net/tcp.h>
-#include <linux/inet_diag.h>
-#include <linux/inet.h>
-#include <linux/random.h>
-
-#include "tcp_dctcp.h"
-
-/* Scale factor for rate in pkt/uSec unit to avoid truncation in bandwidth
- * estimation. The rate unit ~= (1500 bytes / 1 usec / 2^24) ~= 715 bps.
- * This handles bandwidths from 0.06pps (715bps) to 256Mpps (3Tbps) in a u32.
- * Since the minimum window is >=4 packets, the lower bound isn't
- * an issue. The upper bound isn't an issue with existing technologies.
- */
-#define BW_SCALE 24
-#define BW_UNIT (1 << BW_SCALE)
-
-#define BBR_SCALE 8	/* scaling factor for fractions in BBR (e.g. gains) */
-#define BBR_UNIT (1 << BBR_SCALE)
-
-#define FLAG_DEBUG_VERBOSE	0x1	/* Verbose debugging messages */
-#define FLAG_DEBUG_LOOPBACK	0x2	/* Do NOT skip loopback addr */
-
-#define CYCLE_LEN		8	/* number of phases in a pacing gain cycle */
-
-/* BBR has the following modes for deciding how fast to send: */
-enum bbr_mode {
-	BBR_STARTUP,	/* ramp up sending rate rapidly to fill pipe */
-	BBR_DRAIN,	/* drain any queue created during startup */
-	BBR_PROBE_BW,	/* discover, share bw: pace around estimated bw */
-	BBR_PROBE_RTT,	/* cut inflight to min to probe min_rtt */
-};
-
-/* How does the incoming ACK stream relate to our bandwidth probing? */
-enum bbr_ack_phase {
-	BBR_ACKS_INIT,		  /* not probing; not getting probe feedback */
-	BBR_ACKS_REFILLING,	  /* sending at est. bw to fill pipe */
-	BBR_ACKS_PROBE_STARTING,  /* inflight rising to probe bw */
-	BBR_ACKS_PROBE_FEEDBACK,  /* getting feedback from bw probing */
-	BBR_ACKS_PROBE_STOPPING,  /* stopped probing; still getting feedback */
-};
-
-/* BBR congestion control block */
-struct bbr {
-	u32	min_rtt_us;	        /* min RTT in min_rtt_win_sec window */
-	u32	min_rtt_stamp;	        /* timestamp of min_rtt_us */
-	u32	probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
-	u32	probe_rtt_min_us;	/* min RTT in bbr_probe_rtt_win_ms window */
-	u32	probe_rtt_min_stamp;	/* timestamp of probe_rtt_min_us*/
-	u32     next_rtt_delivered; /* scb->tx.delivered at end of round */
-	u32	prior_rcv_nxt;	/* tp->rcv_nxt when CE state last changed */
-	u64	cycle_mstamp;	     /* time of this cycle phase start */
-	u32     mode:3,		     /* current bbr_mode in state machine */
-		prev_ca_state:3,     /* CA state on previous ACK */
-		packet_conservation:1,  /* use packet conservation? */
-		round_start:1,	     /* start of packet-timed tx->ack round? */
-		ce_state:1,          /* If most recent data has CE bit set */
-		bw_probe_up_rounds:5,   /* cwnd-limited rounds in PROBE_UP */
-		try_fast_path:1, 	/* can we take fast path? */
-		unused2:11,
-		idle_restart:1,	     /* restarting after idle? */
-		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
-		cycle_idx:3,	/* current index in pacing_gain cycle array */
-		has_seen_rtt:1;	     /* have we seen an RTT sample yet? */
-	u32	pacing_gain:11,	/* current gain for setting pacing rate */
-		cwnd_gain:11,	/* current gain for setting cwnd */
-		full_bw_reached:1,   /* reached full bw in Startup? */
-		full_bw_cnt:2,	/* number of rounds without large bw gains */
-		init_cwnd:7;	/* initial cwnd */
-	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */
-	u32	full_bw;	/* recent bw, to estimate if pipe is full */
-
-	/* For tracking ACK aggregation: */
-	u64	ack_epoch_mstamp;	/* start of ACK sampling epoch */
-	u16	extra_acked[2];		/* max excess data ACKed in epoch */
-	u32	ack_epoch_acked:20,	/* packets (S)ACKed in sampling epoch */
-		extra_acked_win_rtts:5,	/* age of extra_acked, in round trips */
-		extra_acked_win_idx:1,	/* current index in extra_acked array */
-	/* BBR v2 state: */
-		unused1:2,
-		startup_ecn_rounds:2,	/* consecutive hi ECN STARTUP rounds */
-		loss_in_cycle:1,	/* packet loss in this cycle? */
-		ecn_in_cycle:1;		/* ECN in this cycle? */
-	u32	loss_round_delivered; /* scb->tx.delivered ending loss round */
-	u32	undo_bw_lo;	     /* bw_lo before latest losses */
-	u32	undo_inflight_lo;    /* inflight_lo before latest losses */
-	u32	undo_inflight_hi;    /* inflight_hi before latest losses */
-	u32	bw_latest;	 /* max delivered bw in last round trip */
-	u32	bw_lo;		 /* lower bound on sending bandwidth */
-	u32	bw_hi[2];	 /* upper bound of sending bandwidth range*/
-	u32	inflight_latest; /* max delivered data in last round trip */
-	u32	inflight_lo;	 /* lower bound of inflight data range */
-	u32	inflight_hi;	 /* upper bound of inflight data range */
-	u32	bw_probe_up_cnt; /* packets delivered per inflight_hi incr */
-	u32	bw_probe_up_acks;  /* packets (S)ACKed since inflight_hi incr */
-	u32	probe_wait_us;	 /* PROBE_DOWN until next clock-driven probe */
-	u32	ecn_eligible:1,	/* sender can use ECN (RTT, handshake)? */
-		ecn_alpha:9,	/* EWMA delivered_ce/delivered; 0..256 */
-		bw_probe_samples:1,    /* rate samples reflect bw probing? */
-		prev_probe_too_high:1, /* did last PROBE_UP go too high? */
-		stopped_risky_probe:1, /* last PROBE_UP stopped due to risk? */
-		rounds_since_probe:8,  /* packet-timed rounds since probed bw */
-		loss_round_start:1,    /* loss_round_delivered round trip? */
-		loss_in_round:1,       /* loss marked in this round trip? */
-		ecn_in_round:1,	       /* ECN marked in this round trip? */
-		ack_phase:3,	       /* bbr_ack_phase: meaning of ACKs */
-		loss_events_in_round:4,/* losses in STARTUP round */
-		initialized:1;	       /* has bbr_init() been called? */
-	u32	alpha_last_delivered;	 /* tp->delivered    at alpha update */
-	u32	alpha_last_delivered_ce; /* tp->delivered_ce at alpha update */
-	struct	tcp_plb_state plb;
-
-	/* Params configurable using setsockopt. Refer to correspoding
-	 * module param for detailed description of params.
-	 */
-	struct bbr_params {
-		u32	high_gain:11,		/* max allowed value: 2047 */
-			drain_gain:10,		/* max allowed value: 1023 */
-			cwnd_gain:11;		/* max allowed value: 2047 */
-		u32	cwnd_min_target:4,	/* max allowed value: 15 */
-			min_rtt_win_sec:5,	/* max allowed value: 31 */
-			probe_rtt_mode_ms:9,	/* max allowed value: 511 */
-			full_bw_cnt:3,		/* max allowed value: 7 */
-			cwnd_tso_budget:1,	/* allowed values: {0, 1} */
-			unused3:6,
-			drain_to_target:1,	/* boolean */
-			precise_ece_ack:1,	/* boolean */
-			extra_acked_in_startup:1, /* allowed values: {0, 1} */
-			fast_path:1;		/* boolean */
-		u32	full_bw_thresh:10,	/* max allowed value: 1023 */
-			startup_cwnd_gain:11,	/* max allowed value: 2047 */
-			bw_probe_pif_gain:9,	/* max allowed value: 511 */
-			usage_based_cwnd:1, 	/* boolean */
-			unused2:1;
-		u16	probe_rtt_win_ms:14,	/* max allowed value: 16383 */
-			refill_add_inc:2;	/* max allowed value: 3 */
-		u16	extra_acked_gain:11,	/* max allowed value: 2047 */
-			extra_acked_win_rtts:5; /* max allowed value: 31*/
-		u16	pacing_gain[CYCLE_LEN]; /* max allowed value: 1023 */
-		/* Mostly BBR v2 parameters below here: */
-		u32	ecn_alpha_gain:8,	/* max allowed value: 255 */
-			ecn_factor:8,		/* max allowed value: 255 */
-			ecn_thresh:8,		/* max allowed value: 255 */
-			beta:8;			/* max allowed value: 255 */
-		u32	ecn_max_rtt_us:19,	/* max allowed value: 524287 */
-			bw_probe_reno_gain:9,	/* max allowed value: 511 */
-			full_loss_cnt:4;	/* max allowed value: 15 */
-		u32	probe_rtt_cwnd_gain:8,	/* max allowed value: 255 */
-			inflight_headroom:8,	/* max allowed value: 255 */
-			loss_thresh:8,		/* max allowed value: 255 */
-			bw_probe_max_rounds:8;	/* max allowed value: 255 */
-		u32	bw_probe_rand_rounds:4, /* max allowed value: 15 */
-			bw_probe_base_us:26,	/* usecs: 0..2^26-1 (67 secs) */
-			full_ecn_cnt:2;		/* max allowed value: 3 */
-		u32	bw_probe_rand_us:26,	/* usecs: 0..2^26-1 (67 secs) */
-			undo:1,			/* boolean */
-			tso_rtt_shift:4,	/* max allowed value: 15 */
-			unused5:1;
-		u32	ecn_reprobe_gain:9,	/* max allowed value: 511 */
-			unused1:14,
-			ecn_alpha_init:9;	/* max allowed value: 256 */
-	} params;
-
-	struct {
-		u32	snd_isn; /* Initial sequence number */
-		u32	rs_bw; 	 /* last valid rate sample bw */
-		u32	target_cwnd; /* target cwnd, based on BDP */
-		u8	undo:1,  /* Undo even happened but not yet logged */
-			unused:7;
-		char	event;	 /* single-letter event debug codes */
-		u16	unused2;
-	} debug;
-};
-
-struct bbr_context {
-	u32 sample_bw;
-	u32 target_cwnd;
-	u32 log:1;
-};
-
-/* Window length of min_rtt filter (in sec). Max allowed value is 31 (0x1F) */
-static u32 bbr_min_rtt_win_sec = 10;
-/* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode.
- * Max allowed value is 511 (0x1FF).
- */
-static u32 bbr_probe_rtt_mode_ms = 200;
-/* Window length of probe_rtt_min_us filter (in ms), and consequently the
- * typical interval between PROBE_RTT mode entries.
- * Note that bbr_probe_rtt_win_ms must be <= bbr_min_rtt_win_sec * MSEC_PER_SEC
- */
-static u32 bbr_probe_rtt_win_ms = 5000;
-/* Skip TSO below the following bandwidth (bits/sec): */
-static int bbr_min_tso_rate = 1200000;
-
-/* Use min_rtt to help adapt TSO burst size, with smaller min_rtt resulting
- * in bigger TSO bursts. By default we cut the RTT-based allowance in half
- * for every 2^9 usec (aka 512 us) of RTT, so that the RTT-based allowance
- * is below 1500 bytes after 6 * ~500 usec = 3ms.
- */
-static u32 bbr_tso_rtt_shift = 9;  /* halve allowance per 2^9 usecs, 512us */
-
-/* Select cwnd TSO budget approach:
- *  0: padding
- *  1: flooring
- */
-static uint bbr_cwnd_tso_budget = 1;
-
-/* Pace at ~1% below estimated bw, on average, to reduce queue at bottleneck.
- * In order to help drive the network toward lower queues and low latency while
- * maintaining high utilization, the average pacing rate aims to be slightly
- * lower than the estimated bandwidth. This is an important aspect of the
- * design.
- */
-static const int bbr_pacing_margin_percent = 1;
-
-/* We use a high_gain value of 2/ln(2) because it's the smallest pacing gain
- * that will allow a smoothly increasing pacing rate that will double each RTT
- * and send the same number of packets per RTT that an un-paced, slow-starting
- * Reno or CUBIC flow would. Max allowed value is 2047 (0x7FF).
- */
-static int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1;
-/* The gain for deriving startup cwnd. Max allowed value is 2047 (0x7FF). */
-static int bbr_startup_cwnd_gain  = BBR_UNIT * 2885 / 1000 + 1;
-/* The pacing gain of 1/high_gain in BBR_DRAIN is calculated to typically drain
- * the queue created in BBR_STARTUP in a single round. Max allowed value
- * is 1023 (0x3FF).
- */
-static int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
-/* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs.
- * Max allowed value is 2047 (0x7FF).
- */
-static int bbr_cwnd_gain  = BBR_UNIT * 2;
-/* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw.
- * Max allowed value for each element is 1023 (0x3FF).
- */
-enum bbr_pacing_gain_phase {
-	BBR_BW_PROBE_UP		= 0,  /* push up inflight to probe for bw/vol */
-	BBR_BW_PROBE_DOWN	= 1,  /* drain excess inflight from the queue */
-	BBR_BW_PROBE_CRUISE	= 2,  /* use pipe, w/ headroom in queue/pipe */
-	BBR_BW_PROBE_REFILL	= 3,  /* v2: refill the pipe again to 100% */
-};
-static int bbr_pacing_gain[] = {
-	BBR_UNIT * 5 / 4,	/* probe for more available bw */
-	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
-};
-
-/* Try to keep at least this many packets in flight, if things go smoothly. For
- * smooth functioning, a sliding window protocol ACKing every other packet
- * needs at least 4 packets in flight. Max allowed value is 15 (0xF).
- */
-static u32 bbr_cwnd_min_target = 4;
-
-/* Cwnd to BDP proportion in PROBE_RTT mode scaled by BBR_UNIT. Default: 50%.
- * Use 0 to disable. Max allowed value is 255.
- */
-static u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2;
-
-/* To estimate if BBR_STARTUP mode (i.e. high_gain) has filled pipe... */
-/* If bw has increased significantly (1.25x), there may be more bw available.
- * Max allowed value is 1023 (0x3FF).
- */
-static u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
-/* But after 3 rounds w/o significant bw growth, estimate pipe is full.
- * Max allowed value is 7 (0x7).
- */
-static u32 bbr_full_bw_cnt = 3;
-
-static u32 bbr_flags;		/* Debugging related stuff */
-
-/* Whether to debug using printk.
- */
-static bool bbr_debug_with_printk;
-
-/* Whether to debug using ftrace event tcp:tcp_bbr_event.
- * Ignored when bbr_debug_with_printk is set.
- */
-static bool bbr_debug_ftrace;
-
-/* Experiment: each cycle, try to hold sub-unity gain until inflight <= BDP. */
-static bool bbr_drain_to_target = true;		/* default: enabled */
-
-/* Experiment: Flags to control BBR with ECN behavior.
- */
-static bool bbr_precise_ece_ack = true;		/* default: enabled */
-
-/* The max rwin scaling shift factor is 14 (RFC 1323), so the max sane rwin is
- * (2^(16+14) B)/(1024 B/packet) = 1M packets.
- */
-static u32 bbr_cwnd_warn_val	= 1U << 20;
-
-static u16 bbr_debug_port_mask;
-
-/* BBR module parameters. These are module parameters only in Google prod.
- * Upstream these are intentionally not module parameters.
- */
-static int bbr_pacing_gain_size = CYCLE_LEN;
-
-/* Gain factor for adding extra_acked to target cwnd: */
-static int bbr_extra_acked_gain = 256;
-
-/* Window length of extra_acked window. Max allowed val is 31. */
-static u32 bbr_extra_acked_win_rtts = 5;
-
-/* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
-static u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
-
-/* Time period for clamping cwnd increment due to ack aggregation */
-static u32 bbr_extra_acked_max_us = 100 * 1000;
-
-/* Use extra acked in startup ?
- * 0: disabled
- * 1: use latest extra_acked value from 1-2 rtt in startup
- */
-static int bbr_extra_acked_in_startup = 1;		/* default: enabled */
-
-/* Experiment: don't grow cwnd beyond twice of what we just probed. */
-static bool bbr_usage_based_cwnd;		/* default: disabled */
-
-/* For lab testing, researchers can enable BBRv2 ECN support with this flag,
- * when they know that any ECN marks that the connections experience will be
- * DCTCP/L4S-style ECN marks, rather than RFC3168 ECN marks.
- * TODO(ncardwell): Production use of the BBRv2 ECN functionality depends on
- * negotiation or configuration that is outside the scope of the BBRv2
- * alpha release.
- */
-static bool bbr_ecn_enable = false;
-
-module_param_named(min_tso_rate,      bbr_min_tso_rate,      int,    0644);
-module_param_named(tso_rtt_shift,     bbr_tso_rtt_shift,     int,    0644);
-module_param_named(high_gain,         bbr_high_gain,         int,    0644);
-module_param_named(drain_gain,        bbr_drain_gain,        int,    0644);
-module_param_named(startup_cwnd_gain, bbr_startup_cwnd_gain, int,    0644);
-module_param_named(cwnd_gain,         bbr_cwnd_gain,         int,    0644);
-module_param_array_named(pacing_gain, bbr_pacing_gain,       int,
-			 &bbr_pacing_gain_size, 0644);
-module_param_named(cwnd_min_target,   bbr_cwnd_min_target,   uint,   0644);
-module_param_named(probe_rtt_cwnd_gain,
-		   bbr_probe_rtt_cwnd_gain,		     uint,   0664);
-module_param_named(cwnd_warn_val,     bbr_cwnd_warn_val,     uint,   0664);
-module_param_named(debug_port_mask,   bbr_debug_port_mask,   ushort, 0644);
-module_param_named(flags,             bbr_flags,             uint,   0644);
-module_param_named(debug_ftrace,      bbr_debug_ftrace, bool,   0644);
-module_param_named(debug_with_printk, bbr_debug_with_printk, bool,   0644);
-module_param_named(min_rtt_win_sec,   bbr_min_rtt_win_sec,   uint,   0644);
-module_param_named(probe_rtt_mode_ms, bbr_probe_rtt_mode_ms, uint,   0644);
-module_param_named(probe_rtt_win_ms,  bbr_probe_rtt_win_ms,  uint,   0644);
-module_param_named(full_bw_thresh,    bbr_full_bw_thresh,    uint,   0644);
-module_param_named(full_bw_cnt,       bbr_full_bw_cnt,       uint,   0644);
-module_param_named(cwnd_tso_bduget,   bbr_cwnd_tso_budget,   uint,   0664);
-module_param_named(extra_acked_gain,  bbr_extra_acked_gain,  int,    0664);
-module_param_named(extra_acked_win_rtts,
-		   bbr_extra_acked_win_rtts, uint,   0664);
-module_param_named(extra_acked_max_us,
-		   bbr_extra_acked_max_us, uint,   0664);
-module_param_named(ack_epoch_acked_reset_thresh,
-		   bbr_ack_epoch_acked_reset_thresh, uint,   0664);
-module_param_named(drain_to_target,   bbr_drain_to_target,   bool,   0664);
-module_param_named(precise_ece_ack,   bbr_precise_ece_ack,   bool,   0664);
-module_param_named(extra_acked_in_startup,
-		   bbr_extra_acked_in_startup, int, 0664);
-module_param_named(usage_based_cwnd, bbr_usage_based_cwnd, bool,   0664);
-module_param_named(ecn_enable,       bbr_ecn_enable,         bool,   0664);
-
-static void bbr2_exit_probe_rtt(struct sock *sk);
-static void bbr2_reset_congestion_signals(struct sock *sk);
-
-static void bbr_check_probe_rtt_done(struct sock *sk);
-
-/* Do we estimate that STARTUP filled the pipe? */
-static bool bbr_full_bw_reached(const struct sock *sk)
-{
-	const struct bbr *bbr = inet_csk_ca(sk);
-
-	return bbr->full_bw_reached;
-}
-
-/* Return the windowed max recent bandwidth sample, in pkts/uS << BW_SCALE. */
-static u32 bbr_max_bw(const struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	return max(bbr->bw_hi[0], bbr->bw_hi[1]);
-}
-
-/* Return the estimated bandwidth of the path, in pkts/uS << BW_SCALE. */
-static u32 bbr_bw(const struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	return min(bbr_max_bw(sk), bbr->bw_lo);
-}
-
-/* Return maximum extra acked in past k-2k round trips,
- * where k = bbr_extra_acked_win_rtts.
- */
-static u16 bbr_extra_acked(const struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	return max(bbr->extra_acked[0], bbr->extra_acked[1]);
-}
-
-/* Return rate in bytes per second, optionally with a gain.
- * The order here is chosen carefully to avoid overflow of u64. This should
- * work for input rates of up to 2.9Tbit/sec and gain of 2.89x.
- */
-static u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain,
-				  int margin)
-{
-	unsigned int mss = tcp_sk(sk)->mss_cache;
-
-	rate *= mss;
-	rate *= gain;
-	rate >>= BBR_SCALE;
-	rate *= USEC_PER_SEC / 100 * (100 - margin);
-	rate >>= BW_SCALE;
-	rate = max(rate, 1ULL);
-	return rate;
-}
-
-static u64 bbr_bw_bytes_per_sec(struct sock *sk, u64 rate)
-{
-	return bbr_rate_bytes_per_sec(sk, rate, BBR_UNIT, 0);
-}
-
-static u64 bbr_rate_kbps(struct sock *sk, u64 rate)
-{
-	rate = bbr_bw_bytes_per_sec(sk, rate);
-	rate *= 8;
-	do_div(rate, 1000);
-	return rate;
-}
-
-static u32 bbr_tso_segs_goal(struct sock *sk);
-static void bbr_debug(struct sock *sk, u32 acked,
-		      const struct rate_sample *rs, struct bbr_context *ctx)
-{
-	static const char ca_states[] = {
-		[TCP_CA_Open]		= 'O',
-		[TCP_CA_Disorder]	= 'D',
-		[TCP_CA_CWR]		= 'C',
-		[TCP_CA_Recovery]	= 'R',
-		[TCP_CA_Loss]		= 'L',
-	};
-	static const char mode[] = {
-		'G',  /* Growing   - BBR_STARTUP */
-		'D',  /* Drain     - BBR_DRAIN */
-		'W',  /* Window    - BBR_PROBE_BW */
-		'M',  /* Min RTT   - BBR_PROBE_RTT */
-	};
-	static const char ack_phase[] = { /* bbr_ack_phase strings */
-		'I',	/* BBR_ACKS_INIT	   - 'Init' */
-		'R',	/* BBR_ACKS_REFILLING	   - 'Refilling' */
-		'B',	/* BBR_ACKS_PROBE_STARTING - 'Before' */
-		'F',	/* BBR_ACKS_PROBE_FEEDBACK - 'Feedback' */
-		'A',	/* BBR_ACKS_PROBE_STOPPING - 'After' */
-	};
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	const u32 una = tp->snd_una - bbr->debug.snd_isn;
-	const u32 fack = tcp_highest_sack_seq(tp);
-	const u16 dport = ntohs(inet_sk(sk)->inet_dport);
-	bool is_port_match = (bbr_debug_port_mask &&
-			      ((dport & bbr_debug_port_mask) == 0));
-	char debugmsg[320];
-
-	if (sk->sk_state == TCP_SYN_SENT)
-		return;  /* no bbr_init() yet if SYN retransmit -> CA_Loss */
-
-	if (!tp->snd_cwnd || tp->snd_cwnd > bbr_cwnd_warn_val) {
-		char addr[INET6_ADDRSTRLEN + 10] = { 0 };
-
-		if (sk->sk_family == AF_INET)
-			snprintf(addr, sizeof(addr), "%pI4:%u",
-				 &inet_sk(sk)->inet_daddr, dport);
-		else if (sk->sk_family == AF_INET6)
-			snprintf(addr, sizeof(addr), "%pI6:%u",
-				 &sk->sk_v6_daddr, dport);
-
-		WARN_ONCE(1,
-			"BBR %s cwnd alert: %u "
-			"snd_una: %u ca: %d pacing_gain: %u cwnd_gain: %u "
-			"bw: %u rtt: %u min_rtt: %u "
-			"acked: %u tso_segs: %u "
-			"bw: %d %ld %d pif: %u\n",
-			addr, tp->snd_cwnd,
-			una, inet_csk(sk)->icsk_ca_state,
-			bbr->pacing_gain, bbr->cwnd_gain,
-			bbr_max_bw(sk), (tp->srtt_us >> 3), bbr->min_rtt_us,
-			acked, bbr_tso_segs_goal(sk),
-			rs->delivered, rs->interval_us, rs->is_retrans,
-			tcp_packets_in_flight(tp));
-	}
-
-	if (likely(!bbr_debug_with_printk && !bbr_debug_ftrace))
-		return;
-
-	if (!sock_flag(sk, SOCK_DBG) && !is_port_match)
-		return;
-
-	if (!ctx->log && !tp->app_limited && !(bbr_flags & FLAG_DEBUG_VERBOSE))
-		return;
-
-	if (ipv4_is_loopback(inet_sk(sk)->inet_daddr) &&
-	    !(bbr_flags & FLAG_DEBUG_LOOPBACK))
-		return;
-
-	snprintf(debugmsg, sizeof(debugmsg) - 1,
-		 "BBR %pI4:%-5u %5u,%03u:%-7u %c "
-		 "%c %2u br %2u cr %2d rtt %5ld d %2d i %5ld mrtt %d %cbw %llu "
-		 "bw %llu lb %llu ib %llu qb %llu "
-		 "a %u if %2u %c %c dl %u l %u al %u # %u t %u %c %c "
-		 "lr %d er %d ea %d bwl %lld il %d ih %d c %d "
-		 "v %d %c %u %c %s\n",
-		 &inet_sk(sk)->inet_daddr, dport,
-		 una / 1000, una % 1000, fack - tp->snd_una,
-		 ca_states[inet_csk(sk)->icsk_ca_state],
-		 bbr->debug.undo ? '@' : mode[bbr->mode],
-		 tp->snd_cwnd,
-		 bbr_extra_acked(sk),	/* br (legacy): extra_acked */
-		 rs->tx_in_flight,	/* cr (legacy): tx_inflight */
-		 rs->rtt_us,
-		 rs->delivered,
-		 rs->interval_us,
-		 bbr->min_rtt_us,
-		 rs->is_app_limited ? '_' : 'l',
-		 bbr_rate_kbps(sk, ctx->sample_bw), /* lbw: latest sample bw */
-		 bbr_rate_kbps(sk, bbr_max_bw(sk)), /* bw: max bw */
-		 0ULL,				    /* lb: [obsolete] */
-		 0ULL,				    /* ib: [obsolete] */
-		 (u64)sk->sk_pacing_rate * 8 / 1000,
-		 acked,
-		 tcp_packets_in_flight(tp),
-		 rs->is_ack_delayed ? 'd' : '.',
-		 bbr->round_start ? '*' : '.',
-		 tp->delivered, tp->lost,
-		 tp->app_limited,
-		 0,			    	    /* #: [obsolete] */
-		 ctx->target_cwnd,
-		 tp->reord_seen ? 'r' : '.',  /* r: reordering seen? */
-		 ca_states[bbr->prev_ca_state],
-		 (rs->lost + rs->delivered) > 0 ?
-		 (1000 * rs->lost /
-		  (rs->lost + rs->delivered)) : 0,    /* lr: loss rate x1000 */
-		 (rs->delivered) > 0 ?
-		 (1000 * rs->delivered_ce /
-		  (rs->delivered)) : 0,		      /* er: ECN rate x1000 */
-		 1000 * bbr->ecn_alpha >> BBR_SCALE,  /* ea: ECN alpha x1000 */
-		 bbr->bw_lo == ~0U ?
-		   -1 : (s64)bbr_rate_kbps(sk, bbr->bw_lo), /* bwl */
-		 bbr->inflight_lo,	/* il */
-		 bbr->inflight_hi,	/* ih */
-		 bbr->bw_probe_up_cnt,	/* c */
-		 2,			/* v: version */
-		 bbr->debug.event,
-		 bbr->cycle_idx,
-		 ack_phase[bbr->ack_phase],
-		 bbr->bw_probe_samples ? "Y" : "N");
-	debugmsg[sizeof(debugmsg) - 1] = 0;
-
-	/* printk takes a higher precedence. */
-	if (bbr_debug_with_printk)
-		printk(KERN_DEBUG "%s", debugmsg);
-
-	if (unlikely(bbr->debug.undo))
-		bbr->debug.undo = 0;
-}
-
-/* Convert a BBR bw and gain factor to a pacing rate in bytes per second. */
-static unsigned long bbr_bw_to_pacing_rate(struct sock *sk, u32 bw, int gain)
-{
-	u64 rate = bw;
-
-	rate = bbr_rate_bytes_per_sec(sk, rate, gain,
-				      bbr_pacing_margin_percent);
-	rate = min_t(u64, rate, sk->sk_max_pacing_rate);
-	return rate;
-}
-
-/* Initialize pacing rate to: high_gain * init_cwnd / RTT. */
-static void bbr_init_pacing_rate_from_rtt(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u64 bw;
-	u32 rtt_us;
-
-	if (tp->srtt_us) {		/* any RTT sample yet? */
-		rtt_us = max(tp->srtt_us >> 3, 1U);
-		bbr->has_seen_rtt = 1;
-	} else {			 /* no RTT sample yet */
-		rtt_us = USEC_PER_MSEC;	 /* use nominal default RTT */
-	}
-	bw = (u64)tp->snd_cwnd * BW_UNIT;
-	do_div(bw, rtt_us);
-	sk->sk_pacing_rate = bbr_bw_to_pacing_rate(sk, bw, bbr->params.high_gain);
-}
-
-/* Pace using current bw estimate and a gain factor. */
-static void bbr_set_pacing_rate(struct sock *sk, u32 bw, int gain)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	unsigned long rate = bbr_bw_to_pacing_rate(sk, bw, gain);
-
-	if (unlikely(!bbr->has_seen_rtt && tp->srtt_us))
-		bbr_init_pacing_rate_from_rtt(sk);
-	if (bbr_full_bw_reached(sk) || rate > sk->sk_pacing_rate)
-		sk->sk_pacing_rate = rate;
-}
-
-static u32 bbr_min_tso_segs(struct sock *sk)
-{
-	return sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
-}
-
-/* Return the number of segments BBR would like in a TSO/GSO skb, given
- * a particular max gso size as a constraint.
- */
-static u32 bbr_tso_segs_generic(struct sock *sk, unsigned int mss_now,
-				u32 gso_max_size)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 segs, r;
-	u64 bytes;
-
-	/* Budget a TSO/GSO burst size allowance based on bw (pacing_rate). */
-	bytes = sk->sk_pacing_rate >> sk->sk_pacing_shift;
-
-	/* Budget a TSO/GSO burst size allowance based on min_rtt. For every
-	 * K = 2^tso_rtt_shift microseconds of min_rtt, halve the burst.
-	 * The min_rtt-based burst allowance is: 64 KBytes / 2^(min_rtt/K)
-	 */
-	if (bbr->params.tso_rtt_shift) {
-		r = bbr->min_rtt_us >> bbr->params.tso_rtt_shift;
-		if (r < BITS_PER_TYPE(u32))   /* prevent undefined behavior */
-			bytes += GSO_MAX_SIZE >> r;
-	}
-
-	bytes = min_t(u32, bytes, gso_max_size - 1 - MAX_TCP_HEADER);
-	segs = max_t(u32, bytes / mss_now, bbr_min_tso_segs(sk));
-	return segs;
-}
-
-/* Custom tcp_tso_autosize() for BBR, used at transmit time to cap skb size. */
-static u32  bbr_tso_segs(struct sock *sk, unsigned int mss_now)
-{
-	return bbr_tso_segs_generic(sk, mss_now, sk->sk_gso_max_size);
-}
-
-/* Like bbr_tso_segs(), using mss_cache, ignoring driver's sk_gso_max_size. */
-static u32 bbr_tso_segs_goal(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	return  bbr_tso_segs_generic(sk, tp->mss_cache, GSO_MAX_SIZE);
-}
-
-/* Save "last known good" cwnd so we can restore it after losses or PROBE_RTT */
-static void bbr_save_cwnd(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->prev_ca_state < TCP_CA_Recovery && bbr->mode != BBR_PROBE_RTT)
-		bbr->prior_cwnd = tp->snd_cwnd;  /* this cwnd is good enough */
-	else  /* loss recovery or BBR_PROBE_RTT have temporarily cut cwnd */
-		bbr->prior_cwnd = max(bbr->prior_cwnd, tp->snd_cwnd);
-}
-
-static void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (event == CA_EVENT_TX_START) {
-		tcp_plb_check_rehash(sk, &bbr->plb);
-
-		if (!tp->app_limited)
-			return;
-		bbr->idle_restart = 1;
-		bbr->ack_epoch_mstamp = tp->tcp_mstamp;
-		bbr->ack_epoch_acked = 0;
-		/* Avoid pointless buffer overflows: pace at est. bw if we don't
-		 * need more speed (we're restarting from idle and app-limited).
-		 */
-		if (bbr->mode == BBR_PROBE_BW)
-			bbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);
-		else if (bbr->mode == BBR_PROBE_RTT)
-			bbr_check_probe_rtt_done(sk);
-	} else if ((event == CA_EVENT_ECN_IS_CE ||
-		    event == CA_EVENT_ECN_NO_CE) &&
-		    bbr_ecn_enable &&
-		    bbr->params.precise_ece_ack) {
-		u32 state = bbr->ce_state;
-		dctcp_ece_ack_update(sk, event, &bbr->prior_rcv_nxt, &state);
-		bbr->ce_state = state;
-		if (tp->fast_ack_mode == 2 && event == CA_EVENT_ECN_IS_CE)
-			tcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);
-	}
-}
-
-/* Calculate bdp based on min RTT and the estimated bottleneck bandwidth:
- *
- * bdp = ceil(bw * min_rtt * gain)
- *
- * The key factor, gain, controls the amount of queue. While a small gain
- * builds a smaller queue, it becomes more vulnerable to noise in RTT
- * measurements (e.g., delayed ACKs or other ACK compression effects). This
- * noise may cause BBR to under-estimate the rate.
- */
-static u32 bbr_bdp(struct sock *sk, u32 bw, int gain)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 bdp;
-	u64 w;
-
-	/* If we've never had a valid RTT sample, cap cwnd at the initial
-	 * default. This should only happen when the connection is not using TCP
-	 * timestamps and has retransmitted all of the SYN/SYNACK/data packets
-	 * ACKed so far. In this case, an RTO can cut cwnd to 1, in which
-	 * case we need to slow-start up toward something safe: initial cwnd.
-	 */
-	if (unlikely(bbr->min_rtt_us == ~0U))	 /* no valid RTT samples yet? */
-		return bbr->init_cwnd;  /* be safe: cap at initial cwnd */
-
-	w = (u64)bw * bbr->min_rtt_us;
-
-	/* Apply a gain to the given value, remove the BW_SCALE shift, and
-	 * round the value up to avoid a negative feedback loop.
-	 */
-	bdp = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;
-
-	return bdp;
-}
-
-/* To achieve full performance in high-speed paths, we budget enough cwnd to
- * fit full-sized skbs in-flight on both end hosts to fully utilize the path:
- *   - one skb in sending host Qdisc,
- *   - one skb in sending host TSO/GSO engine
- *   - one skb being received by receiver host LRO/GRO/delayed-ACK engine
- * Don't worry, at low rates (bbr_min_tso_rate) this won't bloat cwnd because
- * in such cases tso_segs_goal is 1. The minimum cwnd is 4 packets,
- * which allows 2 outstanding 2-packet sequences, to try to keep pipe
- * full even with ACK-every-other-packet delayed ACKs.
- */
-static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 tso_segs_goal;
-
-	tso_segs_goal = 3 * bbr_tso_segs_goal(sk);
-
-	/* Allow enough full-sized skbs in flight to utilize end systems. */
-	if (bbr->params.cwnd_tso_budget == 1) {
-		cwnd = max_t(u32, cwnd, tso_segs_goal);
-		cwnd = max_t(u32, cwnd, bbr->params.cwnd_min_target);
-	} else {
-		cwnd += tso_segs_goal;
-		cwnd = (cwnd + 1) & ~1U;
-	}
-	/* Ensure gain cycling gets inflight above BDP even for small BDPs. */
-	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == BBR_BW_PROBE_UP)
-		cwnd += 2;
-
-	return cwnd;
-}
-
-/* Find inflight based on min RTT and the estimated bottleneck bandwidth. */
-static u32 bbr_inflight(struct sock *sk, u32 bw, int gain)
-{
-	u32 inflight;
-
-	inflight = bbr_bdp(sk, bw, gain);
-	inflight = bbr_quantization_budget(sk, inflight);
-
-	return inflight;
-}
-
-/* With pacing at lower layers, there's often less data "in the network" than
- * "in flight". With TSQ and departure time pacing at lower layers (e.g. fq),
- * we often have several skbs queued in the pacing layer with a pre-scheduled
- * earliest departure time (EDT). BBR adapts its pacing rate based on the
- * inflight level that it estimates has already been "baked in" by previous
- * departure time decisions. We calculate a rough estimate of the number of our
- * packets that might be in the network at the earliest departure time for the
- * next skb scheduled:
- *   in_network_at_edt = inflight_at_edt - (EDT - now) * bw
- * If we're increasing inflight, then we want to know if the transmit of the
- * EDT skb will push inflight above the target, so inflight_at_edt includes
- * bbr_tso_segs_goal() from the skb departing at EDT. If decreasing inflight,
- * then estimate if inflight will sink too low just before the EDT transmit.
- */
-static u32 bbr_packets_in_net_at_edt(struct sock *sk, u32 inflight_now)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u64 now_ns, edt_ns, interval_us;
-	u32 interval_delivered, inflight_at_edt;
-
-	now_ns = tp->tcp_clock_cache;
-	edt_ns = max(tp->tcp_wstamp_ns, now_ns);
-	interval_us = div_u64(edt_ns - now_ns, NSEC_PER_USEC);
-	interval_delivered = (u64)bbr_bw(sk) * interval_us >> BW_SCALE;
-	inflight_at_edt = inflight_now;
-	if (bbr->pacing_gain > BBR_UNIT)              /* increasing inflight */
-		inflight_at_edt += bbr_tso_segs_goal(sk);  /* include EDT skb */
-	if (interval_delivered >= inflight_at_edt)
-		return 0;
-	return inflight_at_edt - interval_delivered;
-}
-
-/* Find the cwnd increment based on estimate of ack aggregation */
-static u32 bbr_ack_aggregation_cwnd(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 max_aggr_cwnd, aggr_cwnd = 0;
-
-	if (bbr->params.extra_acked_gain &&
-	    (bbr_full_bw_reached(sk) || bbr->params.extra_acked_in_startup)) {
-		max_aggr_cwnd = ((u64)bbr_bw(sk) * bbr_extra_acked_max_us)
-				/ BW_UNIT;
-		aggr_cwnd = (bbr->params.extra_acked_gain * bbr_extra_acked(sk))
-			     >> BBR_SCALE;
-		aggr_cwnd = min(aggr_cwnd, max_aggr_cwnd);
-	}
-
-	return aggr_cwnd;
-}
-
-/* Returns the cwnd for PROBE_RTT mode. */
-static u32 bbr_probe_rtt_cwnd(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->params.probe_rtt_cwnd_gain == 0)
-		return bbr->params.cwnd_min_target;
-	return max_t(u32, bbr->params.cwnd_min_target,
-		     bbr_bdp(sk, bbr_bw(sk), bbr->params.probe_rtt_cwnd_gain));
-}
-
-/* Slow-start up toward target cwnd (if bw estimate is growing, or packet loss
- * has drawn us down below target), or snap down to target if we're above it.
- */
-static void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,
-			 u32 acked, u32 bw, int gain, u32 cwnd,
-			 struct bbr_context *ctx)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 target_cwnd = 0, prev_cwnd = tp->snd_cwnd, max_probe;
-
-	if (!acked)
-		goto done;  /* no packet fully ACKed; just apply caps */
-
-	target_cwnd = bbr_bdp(sk, bw, gain);
-
-	/* Increment the cwnd to account for excess ACKed data that seems
-	 * due to aggregation (of data and/or ACKs) visible in the ACK stream.
-	 */
-	target_cwnd += bbr_ack_aggregation_cwnd(sk);
-	target_cwnd = bbr_quantization_budget(sk, target_cwnd);
-
-	/* If we're below target cwnd, slow start cwnd toward target cwnd. */
-	bbr->debug.target_cwnd = target_cwnd;
-
-	/* Update cwnd and enable fast path if cwnd reaches target_cwnd. */
-	bbr->try_fast_path = 0;
-	if (bbr_full_bw_reached(sk)) { /* only cut cwnd if we filled the pipe */
-		cwnd += acked;
-		if (cwnd >= target_cwnd) {
-			cwnd = target_cwnd;
-			bbr->try_fast_path = 1;
-		}
-	} else if (cwnd < target_cwnd || cwnd  < 2 * bbr->init_cwnd) {
-		cwnd += acked;
-	} else {
-		bbr->try_fast_path = 1;
-	}
-
-	/* When growing cwnd, don't grow beyond twice what we just probed. */
-	if (bbr->params.usage_based_cwnd) {
-		max_probe = max(2 * tp->max_packets_out, tp->snd_cwnd);
-		cwnd = min(cwnd, max_probe);
-	}
-
-	cwnd = max_t(u32, cwnd, bbr->params.cwnd_min_target);
-done:
-	tp->snd_cwnd = min(cwnd, tp->snd_cwnd_clamp);	/* apply global cap */
-	if (bbr->mode == BBR_PROBE_RTT)  /* drain queue, refresh min_rtt */
-		tp->snd_cwnd = min_t(u32, tp->snd_cwnd, bbr_probe_rtt_cwnd(sk));
-
-	ctx->target_cwnd = target_cwnd;
-	ctx->log = (tp->snd_cwnd != prev_cwnd);
-}
-
-/* See if we have reached next round trip */
-static void bbr_update_round_start(struct sock *sk,
-		const struct rate_sample *rs, struct bbr_context *ctx)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->round_start = 0;
-
-	/* See if we've reached the next RTT */
-	if (rs->interval_us > 0 &&
-	    !before(rs->prior_delivered, bbr->next_rtt_delivered)) {
-		bbr->next_rtt_delivered = tp->delivered;
-		bbr->round_start = 1;
-	}
-}
-
-/* Calculate the bandwidth based on how fast packets are delivered */
-static void bbr_calculate_bw_sample(struct sock *sk,
-			const struct rate_sample *rs, struct bbr_context *ctx)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u64 bw = 0;
-
-	/* Divide delivered by the interval to find a (lower bound) bottleneck
-	 * bandwidth sample. Delivered is in packets and interval_us in uS and
-	 * ratio will be <<1 for most connections. So delivered is first scaled.
-	 * Round up to allow growth at low rates, even with integer division.
-	 */
-	if (rs->interval_us > 0) {
-		if (WARN_ONCE(rs->delivered < 0,
-			      "negative delivered: %d interval_us: %ld\n",
-			      rs->delivered, rs->interval_us))
-			return;
-
-		bw = DIV_ROUND_UP_ULL((u64)rs->delivered * BW_UNIT, rs->interval_us);
-	}
-
-	ctx->sample_bw = bw;
-	bbr->debug.rs_bw = bw;
-}
-
-/* Estimates the windowed max degree of ack aggregation.
- * This is used to provision extra in-flight data to keep sending during
- * inter-ACK silences.
- *
- * Degree of ack aggregation is estimated as extra data acked beyond expected.
- *
- * max_extra_acked = "maximum recent excess data ACKed beyond max_bw * interval"
- * cwnd += max_extra_acked
- *
- * Max extra_acked is clamped by cwnd and bw * bbr_extra_acked_max_us (100 ms).
- * Max filter is an approximate sliding window of 5-10 (packet timed) round
- * trips for non-startup phase, and 1-2 round trips for startup.
- */
-static void bbr_update_ack_aggregation(struct sock *sk,
-				       const struct rate_sample *rs)
-{
-	u32 epoch_us, expected_acked, extra_acked;
-	struct bbr *bbr = inet_csk_ca(sk);
-	struct tcp_sock *tp = tcp_sk(sk);
-	u32 extra_acked_win_rtts_thresh = bbr->params.extra_acked_win_rtts;
-
-	if (!bbr->params.extra_acked_gain || rs->acked_sacked <= 0 ||
-	    rs->delivered < 0 || rs->interval_us <= 0)
-		return;
-
-	if (bbr->round_start) {
-		bbr->extra_acked_win_rtts = min(0x1F,
-						bbr->extra_acked_win_rtts + 1);
-		if (bbr->params.extra_acked_in_startup &&
-		    !bbr_full_bw_reached(sk))
-			extra_acked_win_rtts_thresh = 1;
-		if (bbr->extra_acked_win_rtts >=
-		    extra_acked_win_rtts_thresh) {
-			bbr->extra_acked_win_rtts = 0;
-			bbr->extra_acked_win_idx = bbr->extra_acked_win_idx ?
-						   0 : 1;
-			bbr->extra_acked[bbr->extra_acked_win_idx] = 0;
-		}
-	}
-
-	/* Compute how many packets we expected to be delivered over epoch. */
-	epoch_us = tcp_stamp_us_delta(tp->delivered_mstamp,
-				      bbr->ack_epoch_mstamp);
-	expected_acked = ((u64)bbr_bw(sk) * epoch_us) / BW_UNIT;
-
-	/* Reset the aggregation epoch if ACK rate is below expected rate or
-	 * significantly large no. of ack received since epoch (potentially
-	 * quite old epoch).
-	 */
-	if (bbr->ack_epoch_acked <= expected_acked ||
-	    (bbr->ack_epoch_acked + rs->acked_sacked >=
-	     bbr_ack_epoch_acked_reset_thresh)) {
-		bbr->ack_epoch_acked = 0;
-		bbr->ack_epoch_mstamp = tp->delivered_mstamp;
-		expected_acked = 0;
-	}
-
-	/* Compute excess data delivered, beyond what was expected. */
-	bbr->ack_epoch_acked = min_t(u32, 0xFFFFF,
-				   bbr->ack_epoch_acked + rs->acked_sacked);
-	extra_acked = bbr->ack_epoch_acked - expected_acked;
-	extra_acked = min(extra_acked, tp->snd_cwnd);
-	if (extra_acked > bbr->extra_acked[bbr->extra_acked_win_idx])
-		bbr->extra_acked[bbr->extra_acked_win_idx] = extra_acked;
-}
-
-/* Estimate when the pipe is full, using the change in delivery rate: BBR
- * estimates that STARTUP filled the pipe if the estimated bw hasn't changed by
- * at least bbr_full_bw_thresh (25%) after bbr_full_bw_cnt (3) non-app-limited
- * rounds. Why 3 rounds: 1: rwin autotuning grows the rwin, 2: we fill the
- * higher rwin, 3: we get higher delivery rate samples. Or transient
- * cross-traffic or radio noise can go away. CUBIC Hystart shares a similar
- * design goal, but uses delay and inter-ACK spacing instead of bandwidth.
- */
-static void bbr_check_full_bw_reached(struct sock *sk,
-				      const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 bw_thresh;
-
-	if (bbr_full_bw_reached(sk) || !bbr->round_start || rs->is_app_limited)
-		return;
-
-	bw_thresh = (u64)bbr->full_bw * bbr->params.full_bw_thresh >> BBR_SCALE;
-	if (bbr_max_bw(sk) >= bw_thresh) {
-		bbr->full_bw = bbr_max_bw(sk);
-		bbr->full_bw_cnt = 0;
-		return;
-	}
-	++bbr->full_bw_cnt;
-	bbr->full_bw_reached = bbr->full_bw_cnt >= bbr->params.full_bw_cnt;
-}
-
-/* If pipe is probably full, drain the queue and then enter steady-state. */
-static bool bbr_check_drain(struct sock *sk, const struct rate_sample *rs,
-			    struct bbr_context *ctx)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {
-		bbr->mode = BBR_DRAIN;	/* drain queue we created */
-		tcp_sk(sk)->snd_ssthresh =
-				bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
-		bbr2_reset_congestion_signals(sk);
-	}	/* fall through to check if in-flight is already small: */
-	if (bbr->mode == BBR_DRAIN &&
-	    bbr_packets_in_net_at_edt(sk, tcp_packets_in_flight(tcp_sk(sk))) <=
-	    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT))
-		return true;  /* exiting DRAIN now */
-	return false;
-}
-
-static void bbr_check_probe_rtt_done(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (!(bbr->probe_rtt_done_stamp &&
-	      after(tcp_jiffies32, bbr->probe_rtt_done_stamp)))
-		return;
-
-	bbr->probe_rtt_min_stamp = tcp_jiffies32; /* schedule next PROBE_RTT */
-	tp->snd_cwnd = max(tp->snd_cwnd, bbr->prior_cwnd);
-	bbr2_exit_probe_rtt(sk);
-}
-
-/* The goal of PROBE_RTT mode is to have BBR flows cooperatively and
- * periodically drain the bottleneck queue, to converge to measure the true
- * min_rtt (unloaded propagation delay). This allows the flows to keep queues
- * small (reducing queuing delay and packet loss) and achieve fairness among
- * BBR flows.
- *
- * The min_rtt filter window is 10 seconds. When the min_rtt estimate expires,
- * we enter PROBE_RTT mode and cap the cwnd at bbr_cwnd_min_target=4 packets.
- * After at least bbr_probe_rtt_mode_ms=200ms and at least one packet-timed
- * round trip elapsed with that flight size <= 4, we leave PROBE_RTT mode and
- * re-enter the previous mode. BBR uses 200ms to approximately bound the
- * performance penalty of PROBE_RTT's cwnd capping to roughly 2% (200ms/10s).
- *
- * Note that flows need only pay 2% if they are busy sending over the last 10
- * seconds. Interactive applications (e.g., Web, RPCs, video chunks) often have
- * natural silences or low-rate periods within 10 seconds where the rate is low
- * enough for long enough to drain its queue in the bottleneck. We pick up
- * these min RTT measurements opportunistically with our min_rtt filter. :-)
- */
-static void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	bool probe_rtt_expired, min_rtt_expired;
-	u32 expire;
-
-	/* Track min RTT in probe_rtt_win_ms to time next PROBE_RTT state. */
-	expire = bbr->probe_rtt_min_stamp +
-		 msecs_to_jiffies(bbr->params.probe_rtt_win_ms);
-	probe_rtt_expired = after(tcp_jiffies32, expire);
-	if (rs->rtt_us >= 0 &&
-	    (rs->rtt_us <= bbr->probe_rtt_min_us ||
-	     (probe_rtt_expired && !rs->is_ack_delayed))) {
-		bbr->probe_rtt_min_us = rs->rtt_us;
-		bbr->probe_rtt_min_stamp = tcp_jiffies32;
-	}
-	/* Track min RTT seen in the min_rtt_win_sec filter window: */
-	expire = bbr->min_rtt_stamp + bbr->params.min_rtt_win_sec * HZ;
-	min_rtt_expired = after(tcp_jiffies32, expire);
-	if (bbr->probe_rtt_min_us <= bbr->min_rtt_us ||
-	    min_rtt_expired) {
-		bbr->min_rtt_us = bbr->probe_rtt_min_us;
-		bbr->min_rtt_stamp = bbr->probe_rtt_min_stamp;
-	}
-
-	if (bbr->params.probe_rtt_mode_ms > 0 && probe_rtt_expired &&
-	    !bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {
-		bbr->mode = BBR_PROBE_RTT;  /* dip, drain queue */
-		bbr_save_cwnd(sk);  /* note cwnd so we can restore it */
-		bbr->probe_rtt_done_stamp = 0;
-		bbr->ack_phase = BBR_ACKS_PROBE_STOPPING;
-		bbr->next_rtt_delivered = tp->delivered;
-	}
-
-	if (bbr->mode == BBR_PROBE_RTT) {
-		/* Ignore low rate samples during this mode. */
-		tp->app_limited =
-			(tp->delivered + tcp_packets_in_flight(tp)) ? : 1;
-		/* Maintain min packets in flight for max(200 ms, 1 round). */
-		if (!bbr->probe_rtt_done_stamp &&
-		    tcp_packets_in_flight(tp) <= bbr_probe_rtt_cwnd(sk)) {
-			bbr->probe_rtt_done_stamp = tcp_jiffies32 +
-				msecs_to_jiffies(bbr->params.probe_rtt_mode_ms);
-			bbr->probe_rtt_round_done = 0;
-			bbr->next_rtt_delivered = tp->delivered;
-		} else if (bbr->probe_rtt_done_stamp) {
-			if (bbr->round_start)
-				bbr->probe_rtt_round_done = 1;
-			if (bbr->probe_rtt_round_done)
-				bbr_check_probe_rtt_done(sk);
-		}
-	}
-	/* Restart after idle ends only once we process a new S/ACK for data */
-	if (rs->delivered > 0)
-		bbr->idle_restart = 0;
-}
-
-static void bbr_update_gains(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	switch (bbr->mode) {
-	case BBR_STARTUP:
-		bbr->pacing_gain = bbr->params.high_gain;
-		bbr->cwnd_gain	 = bbr->params.startup_cwnd_gain;
-		break;
-	case BBR_DRAIN:
-		bbr->pacing_gain = bbr->params.drain_gain;  /* slow, to drain */
-		bbr->cwnd_gain = bbr->params.startup_cwnd_gain;  /* keep cwnd */
-		break;
-	case BBR_PROBE_BW:
-		bbr->pacing_gain = bbr->params.pacing_gain[bbr->cycle_idx];
-		bbr->cwnd_gain = bbr->params.cwnd_gain;
-		break;
-	case BBR_PROBE_RTT:
-		bbr->pacing_gain = BBR_UNIT;
-		bbr->cwnd_gain = BBR_UNIT;
-		break;
-	default:
-		WARN_ONCE(1, "BBR bad mode: %u\n", bbr->mode);
-		break;
-	}
-}
-
-static void bbr_init(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	int i;
-
-	WARN_ON_ONCE(tp->snd_cwnd >= bbr_cwnd_warn_val);
-
-	bbr->initialized = 1;
-	bbr->params.high_gain = min(0x7FF, bbr_high_gain);
-	bbr->params.drain_gain = min(0x3FF, bbr_drain_gain);
-	bbr->params.startup_cwnd_gain = min(0x7FF, bbr_startup_cwnd_gain);
-	bbr->params.cwnd_gain = min(0x7FF, bbr_cwnd_gain);
-	bbr->params.cwnd_tso_budget = min(0x1U, bbr_cwnd_tso_budget);
-	bbr->params.cwnd_min_target = min(0xFU, bbr_cwnd_min_target);
-	bbr->params.min_rtt_win_sec = min(0x1FU, bbr_min_rtt_win_sec);
-	bbr->params.probe_rtt_mode_ms = min(0x1FFU, bbr_probe_rtt_mode_ms);
-	bbr->params.full_bw_cnt = min(0x7U, bbr_full_bw_cnt);
-	bbr->params.full_bw_thresh = min(0x3FFU, bbr_full_bw_thresh);
-	bbr->params.extra_acked_gain = min(0x7FF, bbr_extra_acked_gain);
-	bbr->params.extra_acked_win_rtts = min(0x1FU, bbr_extra_acked_win_rtts);
-	bbr->params.drain_to_target = bbr_drain_to_target ? 1 : 0;
-	bbr->params.precise_ece_ack = bbr_precise_ece_ack ? 1 : 0;
-	bbr->params.extra_acked_in_startup = bbr_extra_acked_in_startup ? 1 : 0;
-	bbr->params.probe_rtt_cwnd_gain = min(0xFFU, bbr_probe_rtt_cwnd_gain);
-	bbr->params.probe_rtt_win_ms =
-		min(0x3FFFU,
-		    min_t(u32, bbr_probe_rtt_win_ms,
-			  bbr->params.min_rtt_win_sec * MSEC_PER_SEC));
-	for (i = 0; i < CYCLE_LEN; i++)
-		bbr->params.pacing_gain[i] = min(0x3FF, bbr_pacing_gain[i]);
-	bbr->params.usage_based_cwnd = bbr_usage_based_cwnd ? 1 : 0;
-	bbr->params.tso_rtt_shift =  min(0xFU, bbr_tso_rtt_shift);
-
-	bbr->debug.snd_isn = tp->snd_una;
-	bbr->debug.target_cwnd = 0;
-	bbr->debug.undo = 0;
-
-	bbr->init_cwnd = min(0x7FU, tp->snd_cwnd);
-	bbr->prior_cwnd = tp->prior_cwnd;
-	tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
-	bbr->next_rtt_delivered = 0;
-	bbr->prev_ca_state = TCP_CA_Open;
-	bbr->packet_conservation = 0;
-
-	bbr->probe_rtt_done_stamp = 0;
-	bbr->probe_rtt_round_done = 0;
-	bbr->probe_rtt_min_us = tcp_min_rtt(tp);
-	bbr->probe_rtt_min_stamp = tcp_jiffies32;
-	bbr->min_rtt_us = tcp_min_rtt(tp);
-	bbr->min_rtt_stamp = tcp_jiffies32;
-
-	bbr->has_seen_rtt = 0;
-	bbr_init_pacing_rate_from_rtt(sk);
-
-	bbr->round_start = 0;
-	bbr->idle_restart = 0;
-	bbr->full_bw_reached = 0;
-	bbr->full_bw = 0;
-	bbr->full_bw_cnt = 0;
-	bbr->cycle_mstamp = 0;
-	bbr->cycle_idx = 0;
-	bbr->mode = BBR_STARTUP;
-	bbr->debug.rs_bw = 0;
-
-	bbr->ack_epoch_mstamp = tp->tcp_mstamp;
-	bbr->ack_epoch_acked = 0;
-	bbr->extra_acked_win_rtts = 0;
-	bbr->extra_acked_win_idx = 0;
-	bbr->extra_acked[0] = 0;
-	bbr->extra_acked[1] = 0;
-
-	bbr->ce_state = 0;
-	bbr->prior_rcv_nxt = tp->rcv_nxt;
-	bbr->try_fast_path = 0;
-
-	cmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);
-}
-
-static u32 bbr_sndbuf_expand(struct sock *sk)
-{
-	/* Provision 3 * cwnd since BBR may slow-start even during recovery. */
-	return 3;
-}
-
-/* __________________________________________________________________________
- *
- * Functions new to BBR v2 ("bbr") congestion control are below here.
- * __________________________________________________________________________
- */
-
-/* Incorporate a new bw sample into the current window of our max filter. */
-static void bbr2_take_bw_hi_sample(struct sock *sk, u32 bw)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->bw_hi[1] = max(bw, bbr->bw_hi[1]);
-}
-
-/* Keep max of last 1-2 cycles. Each PROBE_BW cycle, flip filter window. */
-static void bbr2_advance_bw_hi_filter(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (!bbr->bw_hi[1])
-		return;  /* no samples in this window; remember old window */
-	bbr->bw_hi[0] = bbr->bw_hi[1];
-	bbr->bw_hi[1] = 0;
-}
-
-/* How much do we want in flight? Our BDP, unless congestion cut cwnd. */
-static u32 bbr2_target_inflight(struct sock *sk)
-{
-	u32 bdp = bbr_inflight(sk, bbr_bw(sk), BBR_UNIT);
-
-	return min(bdp, tcp_sk(sk)->snd_cwnd);
-}
-
-static bool bbr2_is_probing_bandwidth(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	return (bbr->mode == BBR_STARTUP) ||
-		(bbr->mode == BBR_PROBE_BW &&
-		 (bbr->cycle_idx == BBR_BW_PROBE_REFILL ||
-		  bbr->cycle_idx == BBR_BW_PROBE_UP));
-}
-
-/* Has the given amount of time elapsed since we marked the phase start? */
-static bool bbr2_has_elapsed_in_phase(const struct sock *sk, u32 interval_us)
-{
-	const struct tcp_sock *tp = tcp_sk(sk);
-	const struct bbr *bbr = inet_csk_ca(sk);
-
-	return tcp_stamp_us_delta(tp->tcp_mstamp,
-				  bbr->cycle_mstamp + interval_us) > 0;
-}
-
-static void bbr2_handle_queue_too_high_in_startup(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->full_bw_reached = 1;
-	bbr->inflight_hi = bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);
-}
-
-/* Exit STARTUP upon N consecutive rounds with ECN mark rate > ecn_thresh. */
-static void bbr2_check_ecn_too_high_in_startup(struct sock *sk, u32 ce_ratio)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr_full_bw_reached(sk) || !bbr->ecn_eligible ||
-	    !bbr->params.full_ecn_cnt || !bbr->params.ecn_thresh)
-		return;
-
-	if (ce_ratio >= bbr->params.ecn_thresh)
-		bbr->startup_ecn_rounds++;
-	else
-		bbr->startup_ecn_rounds = 0;
-
-	if (bbr->startup_ecn_rounds >= bbr->params.full_ecn_cnt) {
-		bbr->debug.event = 'E';  /* ECN caused STARTUP exit */
-		bbr2_handle_queue_too_high_in_startup(sk);
-		return;
-	}
-}
-
-static int bbr2_update_ecn_alpha(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	s32 delivered, delivered_ce;
-	u64 alpha, ce_ratio;
-	u32 gain;
-
-	if (bbr->params.ecn_factor == 0)
-		return -1;
-
-	delivered = tp->delivered - bbr->alpha_last_delivered;
-	delivered_ce = tp->delivered_ce - bbr->alpha_last_delivered_ce;
-
-	if (delivered == 0 ||		/* avoid divide by zero */
-	    WARN_ON_ONCE(delivered < 0 || delivered_ce < 0))  /* backwards? */
-		return -1;
-
-	/* See if we should use ECN sender logic for this connection. */
-	if (!bbr->ecn_eligible && bbr_ecn_enable &&
-	    (bbr->min_rtt_us <= bbr->params.ecn_max_rtt_us ||
-	     !bbr->params.ecn_max_rtt_us))
-		bbr->ecn_eligible = 1;
-
-	ce_ratio = (u64)delivered_ce << BBR_SCALE;
-	do_div(ce_ratio, delivered);
-	gain = bbr->params.ecn_alpha_gain;
-	alpha = ((BBR_UNIT - gain) * bbr->ecn_alpha) >> BBR_SCALE;
-	alpha += (gain * ce_ratio) >> BBR_SCALE;
-	bbr->ecn_alpha = min_t(u32, alpha, BBR_UNIT);
-
-	bbr->alpha_last_delivered = tp->delivered;
-	bbr->alpha_last_delivered_ce = tp->delivered_ce;
-
-	bbr2_check_ecn_too_high_in_startup(sk, ce_ratio);
-	return (int)ce_ratio;
-}
-
-/* Each round trip of BBR_BW_PROBE_UP, double volume of probing data. */
-static void bbr2_raise_inflight_hi_slope(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 growth_this_round, cnt;
-
-	/* Calculate "slope": packets S/Acked per inflight_hi increment. */
-	growth_this_round = 1 << bbr->bw_probe_up_rounds;
-	bbr->bw_probe_up_rounds = min(bbr->bw_probe_up_rounds + 1, 30);
-	cnt = tp->snd_cwnd / growth_this_round;
-	cnt = max(cnt, 1U);
-	bbr->bw_probe_up_cnt = cnt;
-	bbr->debug.event = 'G';  /* Grow inflight_hi slope */
-}
-
-/* In BBR_BW_PROBE_UP, not seeing high loss/ECN/queue, so raise inflight_hi. */
-static void bbr2_probe_inflight_hi_upward(struct sock *sk,
-					  const struct rate_sample *rs)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 delta;
-
-	if (!tp->is_cwnd_limited || tp->snd_cwnd < bbr->inflight_hi) {
-		bbr->bw_probe_up_acks = 0;  /* don't accmulate unused credits */
-		return;  /* not fully using inflight_hi, so don't grow it */
-	}
-
-	/* For each bw_probe_up_cnt packets ACKed, increase inflight_hi by 1. */
-	bbr->bw_probe_up_acks += rs->acked_sacked;
-	if (bbr->bw_probe_up_acks >=  bbr->bw_probe_up_cnt) {
-		delta = bbr->bw_probe_up_acks / bbr->bw_probe_up_cnt;
-		bbr->bw_probe_up_acks -= delta * bbr->bw_probe_up_cnt;
-		bbr->inflight_hi += delta;
-		bbr->debug.event = 'I';  /* Increment inflight_hi */
-	}
-
-	if (bbr->round_start)
-		bbr2_raise_inflight_hi_slope(sk);
-}
-
-/* Does loss/ECN rate for this sample say inflight is "too high"?
- * This is used by both the bbr_check_loss_too_high_in_startup() function,
- * which can be used in either v1 or v2, and the PROBE_UP phase of v2, which
- * uses it to notice when loss/ECN rates suggest inflight is too high.
- */
-static bool bbr2_is_inflight_too_high(const struct sock *sk,
-				     const struct rate_sample *rs)
-{
-	const struct bbr *bbr = inet_csk_ca(sk);
-	u32 loss_thresh, ecn_thresh;
-
-	if (rs->lost > 0 && rs->tx_in_flight) {
-		loss_thresh = (u64)rs->tx_in_flight * bbr->params.loss_thresh >>
-				BBR_SCALE;
-		if (rs->lost > loss_thresh)
-			return true;
-	}
-
-	if (rs->delivered_ce > 0 && rs->delivered > 0 &&
-	    bbr->ecn_eligible && bbr->params.ecn_thresh) {
-		ecn_thresh = (u64)rs->delivered * bbr->params.ecn_thresh >>
-				BBR_SCALE;
-		if (rs->delivered_ce >= ecn_thresh)
-			return true;
-	}
-
-	return false;
-}
-
-/* Calculate the tx_in_flight level that corresponded to excessive loss.
- * We find "lost_prefix" segs of the skb where loss rate went too high,
- * by solving for "lost_prefix" in the following equation:
- *   lost                     /  inflight                     >= loss_thresh
- *  (lost_prev + lost_prefix) / (inflight_prev + lost_prefix) >= loss_thresh
- * Then we take that equation, convert it to fixed point, and
- * round up to the nearest packet.
- */
-static u32 bbr2_inflight_hi_from_lost_skb(const struct sock *sk,
-					  const struct rate_sample *rs,
-					  const struct sk_buff *skb)
-{
-	const struct bbr *bbr = inet_csk_ca(sk);
-	u32 loss_thresh  = bbr->params.loss_thresh;
-	u32 pcount, divisor, inflight_hi;
-	s32 inflight_prev, lost_prev;
-	u64 loss_budget, lost_prefix;
-
-	pcount = tcp_skb_pcount(skb);
-
-	/* How much data was in flight before this skb? */
-	inflight_prev = rs->tx_in_flight - pcount;
-	if (WARN_ONCE(inflight_prev < 0,
-		      "tx_in_flight: %u pcount: %u reneg: %u",
-		      rs->tx_in_flight, pcount, tcp_sk(sk)->is_sack_reneg))
-		return ~0U;
-
-	/* How much inflight data was marked lost before this skb? */
-	lost_prev = rs->lost - pcount;
-	if (WARN_ON_ONCE(lost_prev < 0))
-		return ~0U;
-
-	/* At what prefix of this lost skb did losss rate exceed loss_thresh? */
-	loss_budget = (u64)inflight_prev * loss_thresh + BBR_UNIT - 1;
-	loss_budget >>= BBR_SCALE;
-	if (lost_prev >= loss_budget) {
-		lost_prefix = 0;   /* previous losses crossed loss_thresh */
-	} else {
-		lost_prefix = loss_budget - lost_prev;
-		lost_prefix <<= BBR_SCALE;
-		divisor = BBR_UNIT - loss_thresh;
-		if (WARN_ON_ONCE(!divisor))  /* loss_thresh is 8 bits */
-			return ~0U;
-		do_div(lost_prefix, divisor);
-	}
-
-	inflight_hi = inflight_prev + lost_prefix;
-	return inflight_hi;
-}
-
-/* If loss/ECN rates during probing indicated we may have overfilled a
- * buffer, return an operating point that tries to leave unutilized headroom in
- * the path for other flows, for fairness convergence and lower RTTs and loss.
- */
-static u32 bbr2_inflight_with_headroom(const struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 headroom, headroom_fraction;
-
-	if (bbr->inflight_hi == ~0U)
-		return ~0U;
-
-	headroom_fraction = bbr->params.inflight_headroom;
-	headroom = ((u64)bbr->inflight_hi * headroom_fraction) >> BBR_SCALE;
-	headroom = max(headroom, 1U);
-	return max_t(s32, bbr->inflight_hi - headroom,
-		     bbr->params.cwnd_min_target);
-}
-
-/* Bound cwnd to a sensible level, based on our current probing state
- * machine phase and model of a good inflight level (inflight_lo, inflight_hi).
- */
-static void bbr2_bound_cwnd_for_inflight_model(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 cap;
-
-	/* tcp_rcv_synsent_state_process() currently calls tcp_ack()
-	 * and thus cong_control() without first initializing us(!).
-	 */
-	if (!bbr->initialized)
-		return;
-
-	cap = ~0U;
-	if (bbr->mode == BBR_PROBE_BW &&
-	    bbr->cycle_idx != BBR_BW_PROBE_CRUISE) {
-		/* Probe to see if more packets fit in the path. */
-		cap = bbr->inflight_hi;
-	} else {
-		if (bbr->mode == BBR_PROBE_RTT ||
-		    (bbr->mode == BBR_PROBE_BW &&
-		     bbr->cycle_idx == BBR_BW_PROBE_CRUISE))
-			cap = bbr2_inflight_with_headroom(sk);
-	}
-	/* Adapt to any loss/ECN since our last bw probe. */
-	cap = min(cap, bbr->inflight_lo);
-
-	cap = max_t(u32, cap, bbr->params.cwnd_min_target);
-	tp->snd_cwnd = min(cap, tp->snd_cwnd);
-}
-
-/* Estimate a short-term lower bound on the capacity available now, based
- * on measurements of the current delivery process and recent history. When we
- * are seeing loss/ECN at times when we are not probing bw, then conservatively
- * move toward flow balance by multiplicatively cutting our short-term
- * estimated safe rate and volume of data (bw_lo and inflight_lo). We use a
- * multiplicative decrease in order to converge to a lower capacity in time
- * logarithmic in the magnitude of the decrease.
- *
- * However, we do not cut our short-term estimates lower than the current rate
- * and volume of delivered data from this round trip, since from the current
- * delivery process we can estimate the measured capacity available now.
- *
- * Anything faster than that approach would knowingly risk high loss, which can
- * cause low bw for Reno/CUBIC and high loss recovery latency for
- * request/response flows using any congestion control.
- */
-static void bbr2_adapt_lower_bounds(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 ecn_cut, ecn_inflight_lo, beta;
-
-	/* We only use lower-bound estimates when not probing bw.
-	 * When probing we need to push inflight higher to probe bw.
-	 */
-	if (bbr2_is_probing_bandwidth(sk))
-		return;
-
-	/* ECN response. */
-	if (bbr->ecn_in_round && bbr->ecn_eligible && bbr->params.ecn_factor) {
-		/* Reduce inflight to (1 - alpha*ecn_factor). */
-		ecn_cut = (BBR_UNIT -
-			   ((bbr->ecn_alpha * bbr->params.ecn_factor) >>
-			    BBR_SCALE));
-		if (bbr->inflight_lo == ~0U)
-			bbr->inflight_lo = tp->snd_cwnd;
-		ecn_inflight_lo = (u64)bbr->inflight_lo * ecn_cut >> BBR_SCALE;
-	} else {
-		ecn_inflight_lo = ~0U;
-	}
-
-	/* Loss response. */
-	if (bbr->loss_in_round) {
-		/* Reduce bw and inflight to (1 - beta). */
-		if (bbr->bw_lo == ~0U)
-			bbr->bw_lo = bbr_max_bw(sk);
-		if (bbr->inflight_lo == ~0U)
-			bbr->inflight_lo = tp->snd_cwnd;
-		beta = bbr->params.beta;
-		bbr->bw_lo =
-			max_t(u32, bbr->bw_latest,
-			      (u64)bbr->bw_lo *
-			      (BBR_UNIT - beta) >> BBR_SCALE);
-		bbr->inflight_lo =
-			max_t(u32, bbr->inflight_latest,
-			      (u64)bbr->inflight_lo *
-			      (BBR_UNIT - beta) >> BBR_SCALE);
-	}
-
-	/* Adjust to the lower of the levels implied by loss or ECN. */
-	bbr->inflight_lo = min(bbr->inflight_lo, ecn_inflight_lo);
-}
-
-/* Reset any short-term lower-bound adaptation to congestion, so that we can
- * push our inflight up.
- */
-static void bbr2_reset_lower_bounds(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->bw_lo = ~0U;
-	bbr->inflight_lo = ~0U;
-}
-
-/* After bw probing (STARTUP/PROBE_UP), reset signals before entering a state
- * machine phase where we adapt our lower bound based on congestion signals.
- */
-static void bbr2_reset_congestion_signals(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->loss_in_round = 0;
-	bbr->ecn_in_round = 0;
-	bbr->loss_in_cycle = 0;
-	bbr->ecn_in_cycle = 0;
-	bbr->bw_latest = 0;
-	bbr->inflight_latest = 0;
-}
-
-/* Update (most of) our congestion signals: track the recent rate and volume of
- * delivered data, presence of loss, and EWMA degree of ECN marking.
- */
-static void bbr2_update_congestion_signals(
-	struct sock *sk, const struct rate_sample *rs, struct bbr_context *ctx)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	u64 bw;
-
-	bbr->loss_round_start = 0;
-	if (rs->interval_us <= 0 || !rs->acked_sacked)
-		return; /* Not a valid observation */
-	bw = ctx->sample_bw;
-
-	if (!rs->is_app_limited || bw >= bbr_max_bw(sk))
-		bbr2_take_bw_hi_sample(sk, bw);
-
-	bbr->loss_in_round |= (rs->losses > 0);
-
-	/* Update rate and volume of delivered data from latest round trip: */
-	bbr->bw_latest       = max_t(u32, bbr->bw_latest,       ctx->sample_bw);
-	bbr->inflight_latest = max_t(u32, bbr->inflight_latest, rs->delivered);
-
-	if (before(rs->prior_delivered, bbr->loss_round_delivered))
-		return;		/* skip the per-round-trip updates */
-	/* Now do per-round-trip updates. */
-	bbr->loss_round_delivered = tp->delivered;  /* mark round trip */
-	bbr->loss_round_start = 1;
-	bbr2_adapt_lower_bounds(sk);
-
-	/* Update windowed "latest" (single-round-trip) filters. */
-	bbr->loss_in_round = 0;
-	bbr->ecn_in_round  = 0;
-	bbr->bw_latest = ctx->sample_bw;
-	bbr->inflight_latest = rs->delivered;
-}
-
-/* Bandwidth probing can cause loss. To help coexistence with loss-based
- * congestion control we spread out our probing in a Reno-conscious way. Due to
- * the shape of the Reno sawtooth, the time required between loss epochs for an
- * idealized Reno flow is a number of round trips that is the BDP of that
- * flow. We count packet-timed round trips directly, since measured RTT can
- * vary widely, and Reno is driven by packet-timed round trips.
- */
-static bool bbr2_is_reno_coexistence_probe_time(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 inflight, rounds, reno_gain, reno_rounds;
-
-	/* Random loss can shave some small percentage off of our inflight
-	 * in each round. To survive this, flows need robust periodic probes.
-	 */
-	rounds = bbr->params.bw_probe_max_rounds;
-
-	reno_gain = bbr->params.bw_probe_reno_gain;
-	if (reno_gain) {
-		inflight = bbr2_target_inflight(sk);
-		reno_rounds = ((u64)inflight * reno_gain) >> BBR_SCALE;
-		rounds = min(rounds, reno_rounds);
-	}
-	return bbr->rounds_since_probe >= rounds;
-}
-
-/* How long do we want to wait before probing for bandwidth (and risking
- * loss)? We randomize the wait, for better mixing and fairness convergence.
- *
- * We bound the Reno-coexistence inter-bw-probe time to be 62-63 round trips.
- * This is calculated to allow fairness with a 25Mbps, 30ms Reno flow,
- * (eg 4K video to a broadband user):
- *   BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
- *
- * We bound the BBR-native inter-bw-probe wall clock time to be:
- *  (a) higher than 2 sec: to try to avoid causing loss for a long enough time
- *      to allow Reno at 30ms to get 4K video bw, the inter-bw-probe time must
- *      be at least: 25Mbps * .030sec / (1514bytes) * 0.030sec = 1.9secs
- *  (b) lower than 3 sec: to ensure flows can start probing in a reasonable
- *      amount of time to discover unutilized bw on human-scale interactive
- *      time-scales (e.g. perhaps traffic from a web page download that we
- *      were competing with is now complete).
- */
-static void bbr2_pick_probe_wait(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	/* Decide the random round-trip bound for wait until probe: */
-	bbr->rounds_since_probe =
-		prandom_u32_max(bbr->params.bw_probe_rand_rounds);
-	/* Decide the random wall clock bound for wait until probe: */
-	bbr->probe_wait_us = bbr->params.bw_probe_base_us +
-			     prandom_u32_max(bbr->params.bw_probe_rand_us);
-}
-
-static void bbr2_set_cycle_idx(struct sock *sk, int cycle_idx)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->cycle_idx = cycle_idx;
-	/* New phase, so need to update cwnd and pacing rate. */
-	bbr->try_fast_path = 0;
-}
-
-/* Send at estimated bw to fill the pipe, but not queue. We need this phase
- * before PROBE_UP, because as soon as we send faster than the available bw
- * we will start building a queue, and if the buffer is shallow we can cause
- * loss. If we do not fill the pipe before we cause this loss, our bw_hi and
- * inflight_hi estimates will underestimate.
- */
-static void bbr2_start_bw_probe_refill(struct sock *sk, u32 bw_probe_up_rounds)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr2_reset_lower_bounds(sk);
-	if (bbr->inflight_hi != ~0U)
-		bbr->inflight_hi += bbr->params.refill_add_inc;
-	bbr->bw_probe_up_rounds = bw_probe_up_rounds;
-	bbr->bw_probe_up_acks = 0;
-	bbr->stopped_risky_probe = 0;
-	bbr->ack_phase = BBR_ACKS_REFILLING;
-	bbr->next_rtt_delivered = tp->delivered;
-	bbr2_set_cycle_idx(sk, BBR_BW_PROBE_REFILL);
-}
-
-/* Now probe max deliverable data rate and volume. */
-static void bbr2_start_bw_probe_up(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->ack_phase = BBR_ACKS_PROBE_STARTING;
-	bbr->next_rtt_delivered = tp->delivered;
-	bbr->cycle_mstamp = tp->tcp_mstamp;
-	bbr2_set_cycle_idx(sk, BBR_BW_PROBE_UP);
-	bbr2_raise_inflight_hi_slope(sk);
-}
-
-/* Start a new PROBE_BW probing cycle of some wall clock length. Pick a wall
- * clock time at which to probe beyond an inflight that we think to be
- * safe. This will knowingly risk packet loss, so we want to do this rarely, to
- * keep packet loss rates low. Also start a round-trip counter, to probe faster
- * if we estimate a Reno flow at our BDP would probe faster.
- */
-static void bbr2_start_bw_probe_down(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr2_reset_congestion_signals(sk);
-	bbr->bw_probe_up_cnt = ~0U;     /* not growing inflight_hi any more */
-	bbr2_pick_probe_wait(sk);
-	bbr->cycle_mstamp = tp->tcp_mstamp;		/* start wall clock */
-	bbr->ack_phase = BBR_ACKS_PROBE_STOPPING;
-	bbr->next_rtt_delivered = tp->delivered;
-	bbr2_set_cycle_idx(sk, BBR_BW_PROBE_DOWN);
-}
-
-/* Cruise: maintain what we estimate to be a neutral, conservative
- * operating point, without attempting to probe up for bandwidth or down for
- * RTT, and only reducing inflight in response to loss/ECN signals.
- */
-static void bbr2_start_bw_probe_cruise(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr->inflight_lo != ~0U)
-		bbr->inflight_lo = min(bbr->inflight_lo, bbr->inflight_hi);
-
-	bbr2_set_cycle_idx(sk, BBR_BW_PROBE_CRUISE);
-}
-
-/* Loss and/or ECN rate is too high while probing.
- * Adapt (once per bw probe) by cutting inflight_hi and then restarting cycle.
- */
-static void bbr2_handle_inflight_too_high(struct sock *sk,
-					  const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	const u32 beta = bbr->params.beta;
-
-	bbr->prev_probe_too_high = 1;
-	bbr->bw_probe_samples = 0;  /* only react once per probe */
-	bbr->debug.event = 'L';     /* Loss/ECN too high */
-	/* If we are app-limited then we are not robustly
-	 * probing the max volume of inflight data we think
-	 * might be safe (analogous to how app-limited bw
-	 * samples are not known to be robustly probing bw).
-	 */
-	if (!rs->is_app_limited)
-		bbr->inflight_hi = max_t(u32, rs->tx_in_flight,
-					 (u64)bbr2_target_inflight(sk) *
-					 (BBR_UNIT - beta) >> BBR_SCALE);
-	if (bbr->mode == BBR_PROBE_BW && bbr->cycle_idx == BBR_BW_PROBE_UP)
-		bbr2_start_bw_probe_down(sk);
-}
-
-/* If we're seeing bw and loss samples reflecting our bw probing, adapt
- * using the signals we see. If loss or ECN mark rate gets too high, then adapt
- * inflight_hi downward. If we're able to push inflight higher without such
- * signals, push higher: adapt inflight_hi upward.
- */
-static bool bbr2_adapt_upper_bounds(struct sock *sk,
-				   const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	/* Track when we'll see bw/loss samples resulting from our bw probes. */
-	if (bbr->ack_phase == BBR_ACKS_PROBE_STARTING && bbr->round_start)
-		bbr->ack_phase = BBR_ACKS_PROBE_FEEDBACK;
-	if (bbr->ack_phase == BBR_ACKS_PROBE_STOPPING && bbr->round_start) {
-		/* End of samples from bw probing phase. */
-		bbr->bw_probe_samples = 0;
-		bbr->ack_phase = BBR_ACKS_INIT;
-		/* At this point in the cycle, our current bw sample is also
-		 * our best recent chance at finding the highest available bw
-		 * for this flow. So now is the best time to forget the bw
-		 * samples from the previous cycle, by advancing the window.
-		 */
-		if (bbr->mode == BBR_PROBE_BW && !rs->is_app_limited)
-			bbr2_advance_bw_hi_filter(sk);
-		/* If we had an inflight_hi, then probed and pushed inflight all
-		 * the way up to hit that inflight_hi without seeing any
-		 * high loss/ECN in all the resulting ACKs from that probing,
-		 * then probe up again, this time letting inflight persist at
-		 * inflight_hi for a round trip, then accelerating beyond.
-		 */
-		if (bbr->mode == BBR_PROBE_BW &&
-		    bbr->stopped_risky_probe && !bbr->prev_probe_too_high) {
-			bbr->debug.event = 'R';  /* reprobe */
-			bbr2_start_bw_probe_refill(sk, 0);
-			return true;  /* yes, decided state transition */
-		}
-	}
-
-	if (bbr2_is_inflight_too_high(sk, rs)) {
-		if (bbr->bw_probe_samples)  /*  sample is from bw probing? */
-			bbr2_handle_inflight_too_high(sk, rs);
-	} else {
-		/* Loss/ECN rate is declared safe. Adjust upper bound upward. */
-		if (bbr->inflight_hi == ~0U)  /* no excess queue signals yet? */
-			return false;
-
-		/* To be resilient to random loss, we must raise inflight_hi
-		 * if we observe in any phase that a higher level is safe.
-		 */
-		if (rs->tx_in_flight > bbr->inflight_hi) {
-			bbr->inflight_hi = rs->tx_in_flight;
-			bbr->debug.event = 'U';  /* raise up inflight_hi */
-		}
-
-		if (bbr->mode == BBR_PROBE_BW &&
-		    bbr->cycle_idx == BBR_BW_PROBE_UP)
-			bbr2_probe_inflight_hi_upward(sk, rs);
-	}
-
-	return false;
-}
-
-/* Check if it's time to probe for bandwidth now, and if so, kick it off. */
-static bool bbr2_check_time_to_probe_bw(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 n;
-
-	/* If we seem to be at an operating point where we are not seeing loss
-	 * but we are seeing ECN marks, then when the ECN marks cease we reprobe
-	 * quickly (in case a burst of cross-traffic has ceased and freed up bw,
-	 * or in case we are sharing with multiplicatively probing traffic).
-	 */
-	if (bbr->params.ecn_reprobe_gain && bbr->ecn_eligible &&
-	    bbr->ecn_in_cycle && !bbr->loss_in_cycle &&
-	    inet_csk(sk)->icsk_ca_state == TCP_CA_Open) {
-		bbr->debug.event = 'A';  /* *A*ll clear to probe *A*gain */
-		/* Calculate n so that when bbr2_raise_inflight_hi_slope()
-		 * computes growth_this_round as 2^n it will be roughly the
-		 * desired volume of data (inflight_hi*ecn_reprobe_gain).
-		 */
-		n = ilog2((((u64)bbr->inflight_hi *
-			    bbr->params.ecn_reprobe_gain) >> BBR_SCALE));
-		bbr2_start_bw_probe_refill(sk, n);
-		return true;
-	}
-
-	if (bbr2_has_elapsed_in_phase(sk, bbr->probe_wait_us) ||
-	    bbr2_is_reno_coexistence_probe_time(sk)) {
-		bbr2_start_bw_probe_refill(sk, 0);
-		return true;
-	}
-	return false;
-}
-
-/* Is it time to transition from PROBE_DOWN to PROBE_CRUISE? */
-static bool bbr2_check_time_to_cruise(struct sock *sk, u32 inflight, u32 bw)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	bool is_under_bdp, is_long_enough;
-
-	/* Always need to pull inflight down to leave headroom in queue. */
-	if (inflight > bbr2_inflight_with_headroom(sk))
-		return false;
-
-	is_under_bdp = inflight <= bbr_inflight(sk, bw, BBR_UNIT);
-	if (bbr->params.drain_to_target)
-		return is_under_bdp;
-
-	is_long_enough = bbr2_has_elapsed_in_phase(sk, bbr->min_rtt_us);
-	return is_under_bdp || is_long_enough;
-}
-
-/* PROBE_BW state machine: cruise, refill, probe for bw, or drain? */
-static void bbr2_update_cycle_phase(struct sock *sk,
-				    const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	bool is_risky = false, is_queuing = false;
-	u32 inflight, bw;
-
-	if (!bbr_full_bw_reached(sk))
-		return;
-
-	/* In DRAIN, PROBE_BW, or PROBE_RTT, adjust upper bounds. */
-	if (bbr2_adapt_upper_bounds(sk, rs))
-		return;		/* already decided state transition */
-
-	if (bbr->mode != BBR_PROBE_BW)
-		return;
-
-	inflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);
-	bw = bbr_max_bw(sk);
-
-	switch (bbr->cycle_idx) {
-	/* First we spend most of our time cruising with a pacing_gain of 1.0,
-	 * which paces at the estimated bw, to try to fully use the pipe
-	 * without building queue. If we encounter loss/ECN marks, we adapt
-	 * by slowing down.
-	 */
-	case BBR_BW_PROBE_CRUISE:
-		if (bbr2_check_time_to_probe_bw(sk))
-			return;		/* already decided state transition */
-		break;
-
-	/* After cruising, when it's time to probe, we first "refill": we send
-	 * at the estimated bw to fill the pipe, before probing higher and
-	 * knowingly risking overflowing the bottleneck buffer (causing loss).
-	 */
-	case BBR_BW_PROBE_REFILL:
-		if (bbr->round_start) {
-			/* After one full round trip of sending in REFILL, we
-			 * start to see bw samples reflecting our REFILL, which
-			 * may be putting too much data in flight.
-			 */
-			bbr->bw_probe_samples = 1;
-			bbr2_start_bw_probe_up(sk);
-		}
-		break;
-
-	/* After we refill the pipe, we probe by using a pacing_gain > 1.0, to
-	 * probe for bw. If we have not seen loss/ECN, we try to raise inflight
-	 * to at least pacing_gain*BDP; note that this may take more than
-	 * min_rtt if min_rtt is small (e.g. on a LAN).
-	 *
-	 * We terminate PROBE_UP bandwidth probing upon any of the following:
-	 *
-	 * (1) We've pushed inflight up to hit the inflight_hi target set in the
-	 *     most recent previous bw probe phase. Thus we want to start
-	 *     draining the queue immediately because it's very likely the most
-	 *     recently sent packets will fill the queue and cause drops.
-	 *     (checked here)
-	 * (2) We have probed for at least 1*min_rtt_us, and the
-	 *     estimated queue is high enough (inflight > 1.25 * estimated_bdp).
-	 *     (checked here)
-	 * (3) Loss filter says loss rate is "too high".
-	 *     (checked in bbr_is_inflight_too_high())
-	 * (4) ECN filter says ECN mark rate is "too high".
-	 *     (checked in bbr_is_inflight_too_high())
-	 */
-	case BBR_BW_PROBE_UP:
-		if (bbr->prev_probe_too_high &&
-		    inflight >= bbr->inflight_hi) {
-			bbr->stopped_risky_probe = 1;
-			is_risky = true;
-			bbr->debug.event = 'D';   /* D for danger */
-		} else if (bbr2_has_elapsed_in_phase(sk, bbr->min_rtt_us) &&
-			   inflight >=
-			   bbr_inflight(sk, bw,
-					bbr->params.bw_probe_pif_gain)) {
-			is_queuing = true;
-			bbr->debug.event = 'Q'; /* building Queue */
-		}
-		if (is_risky || is_queuing) {
-			bbr->prev_probe_too_high = 0;  /* no loss/ECN (yet) */
-			bbr2_start_bw_probe_down(sk);  /* restart w/ down */
-		}
-		break;
-
-	/* After probing in PROBE_UP, we have usually accumulated some data in
-	 * the bottleneck buffer (if bw probing didn't find more bw). We next
-	 * enter PROBE_DOWN to try to drain any excess data from the queue. To
-	 * do this, we use a pacing_gain < 1.0. We hold this pacing gain until
-	 * our inflight is less then that target cruising point, which is the
-	 * minimum of (a) the amount needed to leave headroom, and (b) the
-	 * estimated BDP. Once inflight falls to match the target, we estimate
-	 * the queue is drained; persisting would underutilize the pipe.
-	 */
-	case BBR_BW_PROBE_DOWN:
-		if (bbr2_check_time_to_probe_bw(sk))
-			return;		/* already decided state transition */
-		if (bbr2_check_time_to_cruise(sk, inflight, bw))
-			bbr2_start_bw_probe_cruise(sk);
-		break;
-
-	default:
-		WARN_ONCE(1, "BBR invalid cycle index %u\n", bbr->cycle_idx);
-	}
-}
-
-/* Exiting PROBE_RTT, so return to bandwidth probing in STARTUP or PROBE_BW. */
-static void bbr2_exit_probe_rtt(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr2_reset_lower_bounds(sk);
-	if (bbr_full_bw_reached(sk)) {
-		bbr->mode = BBR_PROBE_BW;
-		/* Raising inflight after PROBE_RTT may cause loss, so reset
-		 * the PROBE_BW clock and schedule the next bandwidth probe for
-		 * a friendly and randomized future point in time.
-		 */
-		bbr2_start_bw_probe_down(sk);
-		/* Since we are exiting PROBE_RTT, we know inflight is
-		 * below our estimated BDP, so it is reasonable to cruise.
-		 */
-		bbr2_start_bw_probe_cruise(sk);
-	} else {
-		bbr->mode = BBR_STARTUP;
-	}
-}
-
-/* Exit STARTUP based on loss rate > 1% and loss gaps in round >= N. Wait until
- * the end of the round in recovery to get a good estimate of how many packets
- * have been lost, and how many we need to drain with a low pacing rate.
- */
-static void bbr2_check_loss_too_high_in_startup(struct sock *sk,
-					       const struct rate_sample *rs)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr_full_bw_reached(sk))
-		return;
-
-	/* For STARTUP exit, check the loss rate at the end of each round trip
-	 * of Recovery episodes in STARTUP. We check the loss rate at the end
-	 * of the round trip to filter out noisy/low loss and have a better
-	 * sense of inflight (extent of loss), so we can drain more accurately.
-	 */
-	if (rs->losses && bbr->loss_events_in_round < 0xf)
-		bbr->loss_events_in_round++;  /* update saturating counter */
-	if (bbr->params.full_loss_cnt && bbr->loss_round_start &&
-	    inet_csk(sk)->icsk_ca_state == TCP_CA_Recovery &&
-	    bbr->loss_events_in_round >= bbr->params.full_loss_cnt &&
-	    bbr2_is_inflight_too_high(sk, rs)) {
-		bbr->debug.event = 'P';  /* Packet loss caused STARTUP exit */
-		bbr2_handle_queue_too_high_in_startup(sk);
-		return;
-	}
-	if (bbr->loss_round_start)
-		bbr->loss_events_in_round = 0;
-}
-
-/* If we are done draining, advance into steady state operation in PROBE_BW. */
-static void bbr2_check_drain(struct sock *sk, const struct rate_sample *rs,
-			     struct bbr_context *ctx)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (bbr_check_drain(sk, rs, ctx)) {
-		bbr->mode = BBR_PROBE_BW;
-		bbr2_start_bw_probe_down(sk);
-	}
-}
-
-static void bbr2_update_model(struct sock *sk, const struct rate_sample *rs,
-			      struct bbr_context *ctx)
-{
-	bbr2_update_congestion_signals(sk, rs, ctx);
-	bbr_update_ack_aggregation(sk, rs);
-	bbr2_check_loss_too_high_in_startup(sk, rs);
-	bbr_check_full_bw_reached(sk, rs);
-	bbr2_check_drain(sk, rs, ctx);
-	bbr2_update_cycle_phase(sk, rs);
-	bbr_update_min_rtt(sk, rs);
-}
-
-/* Fast path for app-limited case.
- *
- * On each ack, we execute bbr state machine, which primarily consists of:
- * 1) update model based on new rate sample, and
- * 2) update control based on updated model or state change.
- *
- * There are certain workload/scenarios, e.g. app-limited case, where
- * either we can skip updating model or we can skip update of both model
- * as well as control. This provides signifcant softirq cpu savings for
- * processing incoming acks.
- *
- * In case of app-limited, if there is no congestion (loss/ecn) and
- * if observed bw sample is less than current estimated bw, then we can
- * skip some of the computation in bbr state processing:
- *
- * - if there is no rtt/mode/phase change: In this case, since all the
- *   parameters of the network model are constant, we can skip model
- *   as well control update.
- *
- * - else we can skip rest of the model update. But we still need to
- *   update the control to account for the new rtt/mode/phase.
- *
- * Returns whether we can take fast path or not.
- */
-static bool bbr2_fast_path(struct sock *sk, bool *update_model,
-		const struct rate_sample *rs, struct bbr_context *ctx)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-	u32 prev_min_rtt_us, prev_mode;
-
-	if (bbr->params.fast_path && bbr->try_fast_path &&
-	    rs->is_app_limited && ctx->sample_bw < bbr_max_bw(sk) &&
-	    !bbr->loss_in_round && !bbr->ecn_in_round) {
-		prev_mode = bbr->mode;
-		prev_min_rtt_us = bbr->min_rtt_us;
-		bbr2_check_drain(sk, rs, ctx);
-		bbr2_update_cycle_phase(sk, rs);
-		bbr_update_min_rtt(sk, rs);
-
-		if (bbr->mode == prev_mode &&
-		    bbr->min_rtt_us == prev_min_rtt_us &&
-		    bbr->try_fast_path)
-			return true;
-
-		/* Skip model update, but control still needs to be updated */
-		*update_model = false;
-	}
-	return false;
-}
-
-static void bbr2_main(struct sock *sk, const struct rate_sample *rs)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	struct bbr_context ctx = { 0 };
-	bool update_model = true;
-	u32 bw;
-	int ce_ratio = -1;
-
-	bbr->debug.event = '.';  /* init to default NOP (no event yet) */
-
-	bbr_update_round_start(sk, rs, &ctx);
-	if (bbr->round_start) {
-		bbr->rounds_since_probe =
-			min_t(s32, bbr->rounds_since_probe + 1, 0xFF);
-		ce_ratio = bbr2_update_ecn_alpha(sk);
-		tcp_plb_update_state(sk, &bbr->plb, ce_ratio);
-		tcp_plb_check_rehash(sk, &bbr->plb);
-	}
-
-	bbr->ecn_in_round  |= rs->is_ece;
-	bbr_calculate_bw_sample(sk, rs, &ctx);
-
-	if (bbr2_fast_path(sk, &update_model, rs, &ctx))
-		goto out;
-
-	if (update_model)
-		bbr2_update_model(sk, rs, &ctx);
-
-	bbr_update_gains(sk);
-	bw = bbr_bw(sk);
-	bbr_set_pacing_rate(sk, bw, bbr->pacing_gain);
-	bbr_set_cwnd(sk, rs, rs->acked_sacked, bw, bbr->cwnd_gain,
-		     tp->snd_cwnd, &ctx);
-	bbr2_bound_cwnd_for_inflight_model(sk);
-
-out:
-	bbr->prev_ca_state = inet_csk(sk)->icsk_ca_state;
-	bbr->loss_in_cycle |= rs->lost > 0;
-	bbr->ecn_in_cycle  |= rs->delivered_ce > 0;
-
-	bbr_debug(sk, rs->acked_sacked, rs, &ctx);
-}
-
-/* Module parameters that are settable by TCP_CONGESTION_PARAMS are declared
- * down here, so that the algorithm functions that use the parameters must use
- * the per-socket parameters; if they accidentally use the global version
- * then there will be a compile error.
- * TODO(ncardwell): move all per-socket parameters down to this section.
- */
-
-/* On losses, scale down inflight and pacing rate by beta scaled by BBR_SCALE.
- * No loss response when 0. Max allwed value is 255.
- */
-static u32 bbr_beta = BBR_UNIT * 30 / 100;
-
-/* Gain factor for ECN mark ratio samples, scaled by BBR_SCALE.
- * Max allowed value is 255.
- */
-static u32 bbr_ecn_alpha_gain = BBR_UNIT * 1 / 16;  /* 1/16 = 6.25% */
-
-/* The initial value for the ecn_alpha state variable. Default and max
- * BBR_UNIT (256), representing 1.0. This allows a flow to respond quickly
- * to congestion if the bottleneck is congested when the flow starts up.
- */
-static u32 bbr_ecn_alpha_init = BBR_UNIT;	/* 1.0, to respond quickly */
-
-/* On ECN, cut inflight_lo to (1 - ecn_factor * ecn_alpha) scaled by BBR_SCALE.
- * No ECN based bounding when 0. Max allwed value is 255.
- */
-static u32 bbr_ecn_factor = BBR_UNIT * 1 / 3;	    /* 1/3 = 33% */
-
-/* Estimate bw probing has gone too far if CE ratio exceeds this threshold.
- * Scaled by BBR_SCALE. Disabled when 0. Max allowed is 255.
- */
-static u32 bbr_ecn_thresh = BBR_UNIT * 1 / 2;  /* 1/2 = 50% */
-
-/* Max RTT (in usec) at which to use sender-side ECN logic.
- * Disabled when 0 (ECN allowed at any RTT).
- * Max allowed for the parameter is 524287 (0x7ffff) us, ~524 ms.
- */
-static u32 bbr_ecn_max_rtt_us = 5000;
-
-/* If non-zero, if in a cycle with no losses but some ECN marks, after ECN
- * clears then use a multiplicative increase to quickly reprobe bw by
- * starting inflight probing at the given multiple of inflight_hi.
- * Default for this experimental knob is 0 (disabled).
- * Planned value for experiments: BBR_UNIT * 1 / 2 = 128, representing 0.5.
- */
-static u32 bbr_ecn_reprobe_gain;
-
-/* Estimate bw probing has gone too far if loss rate exceeds this level. */
-static u32 bbr_loss_thresh = BBR_UNIT * 2 / 100;  /* 2% loss */
-
-/* Exit STARTUP if number of loss marking events in a Recovery round is >= N,
- * and loss rate is higher than bbr_loss_thresh.
- * Disabled if 0. Max allowed value is 15 (0xF).
- */
-static u32 bbr_full_loss_cnt = 8;
-
-/* Exit STARTUP if number of round trips with ECN mark rate above ecn_thresh
- * meets this count. Max allowed value is 3.
- */
-static u32 bbr_full_ecn_cnt = 2;
-
-/* Fraction of unutilized headroom to try to leave in path upon high loss. */
-static u32 bbr_inflight_headroom = BBR_UNIT * 15 / 100;
-
-/* Multiplier to get target inflight (as multiple of BDP) for PROBE_UP phase.
- * Default is 1.25x, as in BBR v1. Max allowed is 511.
- */
-static u32 bbr_bw_probe_pif_gain = BBR_UNIT * 5 / 4;
-
-/* Multiplier to get Reno-style probe epoch duration as: k * BDP round trips.
- * If zero, disables this BBR v2 Reno-style BDP-scaled coexistence mechanism.
- * Max allowed is 511.
- */
-static u32 bbr_bw_probe_reno_gain = BBR_UNIT;
-
-/* Max number of packet-timed rounds to wait before probing for bandwidth.  If
- * we want to tolerate 1% random loss per round, and not have this cut our
- * inflight too much, we must probe for bw periodically on roughly this scale.
- * If low, limits Reno/CUBIC coexistence; if high, limits loss tolerance.
- * We aim to be fair with Reno/CUBIC up to a BDP of at least:
- *  BDP = 25Mbps * .030sec /(1514bytes) = 61.9 packets
- */
-static u32 bbr_bw_probe_max_rounds = 63;
-
-/* Max amount of randomness to inject in round counting for Reno-coexistence.
- * Max value is 15.
- */
-static u32 bbr_bw_probe_rand_rounds = 2;
-
-/* Use BBR-native probe time scale starting at this many usec.
- * We aim to be fair with Reno/CUBIC up to an inter-loss time epoch of at least:
- *  BDP*RTT = 25Mbps * .030sec /(1514bytes) * 0.030sec = 1.9 secs
- */
-static u32 bbr_bw_probe_base_us = 2 * USEC_PER_SEC;  /* 2 secs */
-
-/* Use BBR-native probes spread over this many usec: */
-static u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC;  /* 1 secs */
-
-/* Undo the model changes made in loss recovery if recovery was spurious? */
-static bool bbr_undo = true;
-
-/* Use fast path if app-limited, no loss/ECN, and target cwnd was reached? */
-static bool bbr_fast_path = true;	/* default: enabled */
-
-/* Use fast ack mode ? */
-static int bbr_fast_ack_mode = 1;	/* default: rwnd check off */
-
-/* How much to additively increase inflight_hi when entering REFILL? */
-static u32 bbr_refill_add_inc;		/* default: disabled */
-
-module_param_named(beta,                 bbr_beta,                 uint, 0644);
-module_param_named(ecn_alpha_gain,       bbr_ecn_alpha_gain,       uint, 0644);
-module_param_named(ecn_alpha_init,       bbr_ecn_alpha_init,       uint, 0644);
-module_param_named(ecn_factor,           bbr_ecn_factor,           uint, 0644);
-module_param_named(ecn_thresh,           bbr_ecn_thresh,           uint, 0644);
-module_param_named(ecn_max_rtt_us,       bbr_ecn_max_rtt_us,       uint, 0644);
-module_param_named(ecn_reprobe_gain,     bbr_ecn_reprobe_gain,     uint, 0644);
-module_param_named(loss_thresh,          bbr_loss_thresh,          uint, 0664);
-module_param_named(full_loss_cnt,        bbr_full_loss_cnt,        uint, 0664);
-module_param_named(full_ecn_cnt,         bbr_full_ecn_cnt,         uint, 0664);
-module_param_named(inflight_headroom,    bbr_inflight_headroom,    uint, 0664);
-module_param_named(bw_probe_pif_gain,    bbr_bw_probe_pif_gain,    uint, 0664);
-module_param_named(bw_probe_reno_gain,   bbr_bw_probe_reno_gain,   uint, 0664);
-module_param_named(bw_probe_max_rounds,  bbr_bw_probe_max_rounds,  uint, 0664);
-module_param_named(bw_probe_rand_rounds, bbr_bw_probe_rand_rounds, uint, 0664);
-module_param_named(bw_probe_base_us,     bbr_bw_probe_base_us,     uint, 0664);
-module_param_named(bw_probe_rand_us,     bbr_bw_probe_rand_us,     uint, 0664);
-module_param_named(undo,                 bbr_undo,                 bool, 0664);
-module_param_named(fast_path,		 bbr_fast_path,		   bool, 0664);
-module_param_named(fast_ack_mode,	 bbr_fast_ack_mode,	   uint, 0664);
-module_param_named(refill_add_inc,       bbr_refill_add_inc,       uint, 0664);
-
-static void bbr2_init(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	const struct net *net = sock_net(sk);
-
-	bbr_init(sk);	/* run shared init code for v1 and v2 */
-
-	/* BBR v2 parameters: */
-	bbr->params.beta = min_t(u32, 0xFFU, bbr_beta);
-	bbr->params.ecn_alpha_gain = min_t(u32, 0xFFU, bbr_ecn_alpha_gain);
-	bbr->params.ecn_alpha_init = min_t(u32, BBR_UNIT, bbr_ecn_alpha_init);
-	bbr->params.ecn_factor = min_t(u32, 0xFFU, bbr_ecn_factor);
-	bbr->params.ecn_thresh = min_t(u32, 0xFFU, bbr_ecn_thresh);
-	bbr->params.ecn_max_rtt_us = min_t(u32, 0x7ffffU, bbr_ecn_max_rtt_us);
-	bbr->params.ecn_reprobe_gain = min_t(u32, 0x1FF, bbr_ecn_reprobe_gain);
-	bbr->params.loss_thresh = min_t(u32, 0xFFU, bbr_loss_thresh);
-	bbr->params.full_loss_cnt = min_t(u32, 0xFU, bbr_full_loss_cnt);
-	bbr->params.full_ecn_cnt = min_t(u32, 0x3U, bbr_full_ecn_cnt);
-	bbr->params.inflight_headroom =
-		min_t(u32, 0xFFU, bbr_inflight_headroom);
-	bbr->params.bw_probe_pif_gain =
-		min_t(u32, 0x1FFU, bbr_bw_probe_pif_gain);
-	bbr->params.bw_probe_reno_gain =
-		min_t(u32, 0x1FFU, bbr_bw_probe_reno_gain);
-	bbr->params.bw_probe_max_rounds =
-		min_t(u32, 0xFFU, bbr_bw_probe_max_rounds);
-	bbr->params.bw_probe_rand_rounds =
-		min_t(u32, 0xFU, bbr_bw_probe_rand_rounds);
-	bbr->params.bw_probe_base_us =
-		min_t(u32, (1 << 26) - 1, bbr_bw_probe_base_us);
-	bbr->params.bw_probe_rand_us =
-		min_t(u32, (1 << 26) - 1, bbr_bw_probe_rand_us);
-	bbr->params.undo = bbr_undo;
-	bbr->params.fast_path = bbr_fast_path ? 1 : 0;
-	bbr->params.refill_add_inc = min_t(u32, 0x3U, bbr_refill_add_inc);
-
-	/* BBR v2 state: */
-	bbr->initialized = 1;
-	/* Start sampling ECN mark rate after first full flight is ACKed: */
-	bbr->loss_round_delivered = tp->delivered + 1;
-	bbr->loss_round_start = 0;
-	bbr->undo_bw_lo = 0;
-	bbr->undo_inflight_lo = 0;
-	bbr->undo_inflight_hi = 0;
-	bbr->loss_events_in_round = 0;
-	bbr->startup_ecn_rounds = 0;
-	bbr2_reset_congestion_signals(sk);
-	bbr->bw_lo = ~0U;
-	bbr->bw_hi[0] = 0;
-	bbr->bw_hi[1] = 0;
-	bbr->inflight_lo = ~0U;
-	bbr->inflight_hi = ~0U;
-	bbr->bw_probe_up_cnt = ~0U;
-	bbr->bw_probe_up_acks = 0;
-	bbr->bw_probe_up_rounds = 0;
-	bbr->probe_wait_us = 0;
-	bbr->stopped_risky_probe = 0;
-	bbr->ack_phase = BBR_ACKS_INIT;
-	bbr->rounds_since_probe = 0;
-	bbr->bw_probe_samples = 0;
-	bbr->prev_probe_too_high = 0;
-	bbr->ecn_eligible = 0;
-	bbr->ecn_alpha = bbr->params.ecn_alpha_init;
-	bbr->alpha_last_delivered = 0;
-	bbr->alpha_last_delivered_ce = 0;
-
-	bbr->plb.enabled = 0;
-	bbr->plb.consec_cong_rounds = 0;
-	bbr->plb.pause_until = 0;
-	if ((tp->ecn_flags & TCP_ECN_OK) &&
-	    net->ipv4.sysctl_tcp_plb_enabled)
-		bbr->plb.enabled = 1;
-
-	tp->fast_ack_mode = min_t(u32, 0x2U, bbr_fast_ack_mode);
-
-	if ((tp->ecn_flags & TCP_ECN_OK) && bbr_ecn_enable)
-		tp->ecn_flags |= TCP_ECN_ECT_PERMANENT;
-}
-
-/* Core TCP stack informs us that the given skb was just marked lost. */
-static void bbr2_skb_marked_lost(struct sock *sk, const struct sk_buff *skb)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-	struct tcp_skb_cb *scb = TCP_SKB_CB(skb);
-	struct rate_sample rs;
-
-	/* Capture "current" data over the full round trip of loss,
-	 * to have a better chance to see the full capacity of the path.
-	*/
-	if (!bbr->loss_in_round)  /* first loss in this round trip? */
-		bbr->loss_round_delivered = tp->delivered;  /* set round trip */
-	bbr->loss_in_round = 1;
-	bbr->loss_in_cycle = 1;
-
-	if (!bbr->bw_probe_samples)
-		return;  /* not an skb sent while probing for bandwidth */
-	if (unlikely(!scb->tx.delivered_mstamp))
-		return;  /* skb was SACKed, reneged, marked lost; ignore it */
-	/* We are probing for bandwidth. Construct a rate sample that
-	 * estimates what happened in the flight leading up to this lost skb,
-	 * then see if the loss rate went too high, and if so at which packet.
-	 */
-	memset(&rs, 0, sizeof(rs));
-	rs.tx_in_flight = scb->tx.in_flight;
-	rs.lost = tp->lost - scb->tx.lost;
-	rs.is_app_limited = scb->tx.is_app_limited;
-	if (bbr2_is_inflight_too_high(sk, &rs)) {
-		rs.tx_in_flight = bbr2_inflight_hi_from_lost_skb(sk, &rs, skb);
-		bbr2_handle_inflight_too_high(sk, &rs);
-	}
-}
-
-/* Revert short-term model if current loss recovery event was spurious. */
-static u32 bbr2_undo_cwnd(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr->debug.undo = 1;
-	bbr->full_bw = 0;   /* spurious slow-down; reset full pipe detection */
-	bbr->full_bw_cnt = 0;
-	bbr->loss_in_round = 0;
-
-	if (!bbr->params.undo)
-		return tp->snd_cwnd;
-
-	/* Revert to cwnd and other state saved before loss episode. */
-	bbr->bw_lo = max(bbr->bw_lo, bbr->undo_bw_lo);
-	bbr->inflight_lo = max(bbr->inflight_lo, bbr->undo_inflight_lo);
-	bbr->inflight_hi = max(bbr->inflight_hi, bbr->undo_inflight_hi);
-	return bbr->prior_cwnd;
-}
-
-/* Entering loss recovery, so save state for when we undo recovery. */
-static u32 bbr2_ssthresh(struct sock *sk)
-{
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	bbr_save_cwnd(sk);
-	/* For undo, save state that adapts based on loss signal. */
-	bbr->undo_bw_lo		= bbr->bw_lo;
-	bbr->undo_inflight_lo	= bbr->inflight_lo;
-	bbr->undo_inflight_hi	= bbr->inflight_hi;
-	return tcp_sk(sk)->snd_ssthresh;
-}
-
-static enum tcp_bbr2_phase bbr2_get_phase(struct bbr *bbr)
-{
-	switch (bbr->mode) {
-	case BBR_STARTUP:
-		return BBR2_PHASE_STARTUP;
-	case BBR_DRAIN:
-		return BBR2_PHASE_DRAIN;
-	case BBR_PROBE_BW:
-		break;
-	case BBR_PROBE_RTT:
-		return BBR2_PHASE_PROBE_RTT;
-	default:
-		return BBR2_PHASE_INVALID;
-	}
-	switch (bbr->cycle_idx) {
-	case BBR_BW_PROBE_UP:
-		return BBR2_PHASE_PROBE_BW_UP;
-	case BBR_BW_PROBE_DOWN:
-		return BBR2_PHASE_PROBE_BW_DOWN;
-	case BBR_BW_PROBE_CRUISE:
-		return BBR2_PHASE_PROBE_BW_CRUISE;
-	case BBR_BW_PROBE_REFILL:
-		return BBR2_PHASE_PROBE_BW_REFILL;
-	default:
-		return BBR2_PHASE_INVALID;
-	}
-}
-
-static size_t bbr2_get_info(struct sock *sk, u32 ext, int *attr,
-			    union tcp_cc_info *info)
-{
-	if (ext & (1 << (INET_DIAG_BBRINFO - 1)) ||
-	    ext & (1 << (INET_DIAG_VEGASINFO - 1))) {
-		struct bbr *bbr = inet_csk_ca(sk);
-		u64 bw = bbr_bw_bytes_per_sec(sk, bbr_bw(sk));
-		u64 bw_hi = bbr_bw_bytes_per_sec(sk, bbr_max_bw(sk));
-		u64 bw_lo = bbr->bw_lo == ~0U ?
-			~0ULL : bbr_bw_bytes_per_sec(sk, bbr->bw_lo);
-
-		memset(&info->bbr2, 0, sizeof(info->bbr2));
-		info->bbr2.bbr_bw_lsb		= (u32)bw;
-		info->bbr2.bbr_bw_msb		= (u32)(bw >> 32);
-		info->bbr2.bbr_min_rtt		= bbr->min_rtt_us;
-		info->bbr2.bbr_pacing_gain	= bbr->pacing_gain;
-		info->bbr2.bbr_cwnd_gain	= bbr->cwnd_gain;
-		info->bbr2.bbr_bw_hi_lsb	= (u32)bw_hi;
-		info->bbr2.bbr_bw_hi_msb	= (u32)(bw_hi >> 32);
-		info->bbr2.bbr_bw_lo_lsb	= (u32)bw_lo;
-		info->bbr2.bbr_bw_lo_msb	= (u32)(bw_lo >> 32);
-		info->bbr2.bbr_mode		= bbr->mode;
-		info->bbr2.bbr_phase		= (__u8)bbr2_get_phase(bbr);
-		info->bbr2.bbr_version		= (__u8)2;
-		info->bbr2.bbr_inflight_lo	= bbr->inflight_lo;
-		info->bbr2.bbr_inflight_hi	= bbr->inflight_hi;
-		info->bbr2.bbr_extra_acked	= bbr_extra_acked(sk);
-		*attr = INET_DIAG_BBRINFO;
-		return sizeof(info->bbr2);
-	}
-	return 0;
-}
-
-static void bbr2_set_state(struct sock *sk, u8 new_state)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	struct bbr *bbr = inet_csk_ca(sk);
-
-	if (new_state == TCP_CA_Loss) {
-		struct rate_sample rs = { .losses = 1 };
-		struct bbr_context ctx = { 0 };
-
-		tcp_plb_update_state_upon_rto(sk, &bbr->plb);
-		bbr->prev_ca_state = TCP_CA_Loss;
-		bbr->full_bw = 0;
-		if (!bbr2_is_probing_bandwidth(sk) && bbr->inflight_lo == ~0U) {
-			/* bbr_adapt_lower_bounds() needs cwnd before
-			 * we suffered an RTO, to update inflight_lo:
-			 */
-			bbr->inflight_lo =
-				max(tp->snd_cwnd, bbr->prior_cwnd);
-		}
-		bbr_debug(sk, 0, &rs, &ctx);
-	} else if (bbr->prev_ca_state == TCP_CA_Loss &&
-		   new_state != TCP_CA_Loss) {
-		tp->snd_cwnd = max(tp->snd_cwnd, bbr->prior_cwnd);
-		bbr->try_fast_path = 0; /* bound cwnd using latest model */
-	}
-}
-
-static struct tcp_congestion_ops tcp_bbr2_cong_ops __read_mostly = {
-	.flags		= TCP_CONG_NON_RESTRICTED | TCP_CONG_WANTS_CE_EVENTS,
-	.name		= "bbr2",
-	.owner		= THIS_MODULE,
-	.init		= bbr2_init,
-	.cong_control	= bbr2_main,
-	.sndbuf_expand	= bbr_sndbuf_expand,
-	.skb_marked_lost = bbr2_skb_marked_lost,
-	.undo_cwnd	= bbr2_undo_cwnd,
-	.cwnd_event	= bbr_cwnd_event,
-	.ssthresh	= bbr2_ssthresh,
-	.tso_segs	= bbr_tso_segs,
-	.get_info	= bbr2_get_info,
-	.set_state	= bbr2_set_state,
-};
-
-static int __init bbr_register(void)
-{
-	BUILD_BUG_ON(sizeof(struct bbr) > ICSK_CA_PRIV_SIZE);
-	return tcp_register_congestion_control(&tcp_bbr2_cong_ops);
-}
-
-static void __exit bbr_unregister(void)
-{
-	tcp_unregister_congestion_control(&tcp_bbr2_cong_ops);
-}
-
-module_init(bbr_register);
-module_exit(bbr_unregister);
-
-MODULE_AUTHOR("Van Jacobson <vanj@google.com>");
-MODULE_AUTHOR("Neal Cardwell <ncardwell@google.com>");
-MODULE_AUTHOR("Yuchung Cheng <ycheng@google.com>");
-MODULE_AUTHOR("Soheil Hassas Yeganeh <soheil@google.com>");
-MODULE_AUTHOR("Priyaranjan Jha <priyarjha@google.com>");
-MODULE_AUTHOR("Yousuk Seung <ysseung@google.com>");
-MODULE_AUTHOR("Kevin Yang <yyd@google.com>");
-MODULE_AUTHOR("Arjun Roy <arjunroy@google.com>");
-
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_DESCRIPTION("TCP BBR (Bottleneck Bandwidth and RTT)");
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index f0617591fb57..38e91178ab49 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -2918,10 +2918,11 @@ static int __net_init tcp_sk_init(struct net *net)
 
 	/* Set default values for PLB */
 	net->ipv4.sysctl_tcp_plb_enabled = 0; /* Disabled by default */
-	net->ipv4.sysctl_tcp_plb_cong_thresh = 128; /* 50% congestion */
 	net->ipv4.sysctl_tcp_plb_idle_rehash_rounds = 3;
 	net->ipv4.sysctl_tcp_plb_rehash_rounds = 12;
 	net->ipv4.sysctl_tcp_plb_suspend_rto_sec = 60;
+	/* Default congestion threshold for PLB to mark a round is 50% */
+	net->ipv4.sysctl_tcp_plb_cong_thresh = (1 << TCP_PLB_SCALE) / 2;
 
 	/* Reno is always built in */
 	if (!net_eq(net, &init_net) &&
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index f0f67b25c97a..a7db9d121530 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -409,6 +409,8 @@ void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst)
 	u32 ca_key = dst_metric(dst, RTAX_CC_ALGO);
 	bool ca_got_dst = false;
 
+	tcp_set_ecn_low_from_dst(sk, dst);
+
 	if (ca_key != TCP_CA_UNSPEC) {
 		const struct tcp_congestion_ops *ca;
 
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index bda090c49050..1ab93f2755b2 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -325,10 +325,9 @@ static void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)
 	bool bpf_needs_ecn = tcp_bpf_ca_needs_ecn(sk);
 	bool use_ecn = sock_net(sk)->ipv4.sysctl_tcp_ecn == 1 ||
 		tcp_ca_needs_ecn(sk) || bpf_needs_ecn;
+	const struct dst_entry *dst = __sk_dst_get(sk);
 
 	if (!use_ecn) {
-		const struct dst_entry *dst = __sk_dst_get(sk);
-
 		if (dst && dst_feature(dst, RTAX_FEATURE_ECN))
 			use_ecn = true;
 	}
@@ -340,6 +339,9 @@ static void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)
 		tp->ecn_flags = TCP_ECN_OK;
 		if (tcp_ca_needs_ecn(sk) || bpf_needs_ecn)
 			INET_ECN_xmit(sk);
+
+		if (dst)
+			tcp_set_ecn_low_from_dst(sk, dst);
 	}
 }
 
@@ -1551,7 +1553,7 @@ int tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,
 	 * SO_SNDBUF values.
 	 * Also allow first and last skb in retransmit queue to be split.
 	 */
-	limit = sk->sk_sndbuf + 2 * SKB_TRUESIZE(GSO_MAX_SIZE);
+	limit = sk->sk_sndbuf + 2 * SKB_TRUESIZE(GSO_LEGACY_MAX_SIZE);
 	if (unlikely((sk->sk_wmem_queued >> 1) > limit &&
 		     tcp_queue != TCP_FRAG_IN_WRITE_QUEUE &&
 		     skb != tcp_rtx_queue_head(sk) &&
@@ -1613,13 +1615,28 @@ int tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,
 		if (diff)
 			tcp_adjust_pcount(sk, skb, diff);
 
-		/* Set buff tx.in_flight as if buff were sent by itself. */
 		inflight_prev = TCP_SKB_CB(skb)->tx.in_flight - old_factor;
-		if (WARN_ONCE(inflight_prev < 0,
-			      "inconsistent: tx.in_flight: %u old_factor: %d",
-			      TCP_SKB_CB(skb)->tx.in_flight, old_factor))
+		if (inflight_prev < 0) {
+			WARN_ONCE(tcp_skb_tx_in_flight_is_suspicious(
+					  old_factor,
+					  TCP_SKB_CB(skb)->sacked,
+					  TCP_SKB_CB(skb)->tx.in_flight),
+				  "inconsistent: tx.in_flight: %u "
+				  "old_factor: %d mss: %u sacked: %u "
+				  "1st pcount: %d 2nd pcount: %d "
+				  "1st len: %u 2nd len: %u ",
+				  TCP_SKB_CB(skb)->tx.in_flight, old_factor,
+				  mss_now, TCP_SKB_CB(skb)->sacked,
+				  tcp_skb_pcount(skb), tcp_skb_pcount(buff),
+				  skb->len, buff->len);
 			inflight_prev = 0;
+		}
+		/* Set 1st tx.in_flight as if 1st were sent by itself: */
+		TCP_SKB_CB(skb)->tx.in_flight = inflight_prev +
+						 tcp_skb_pcount(skb);
+		/* Set 2nd tx.in_flight with new 1st and 2nd pcounts: */
 		TCP_SKB_CB(buff)->tx.in_flight = inflight_prev +
+						 tcp_skb_pcount(skb) +
 						 tcp_skb_pcount(buff);
 	}
 
diff --git a/net/ipv4/tcp_plb.c b/net/ipv4/tcp_plb.c
index 71b02c0404ce..f4ced370acad 100644
--- a/net/ipv4/tcp_plb.c
+++ b/net/ipv4/tcp_plb.c
@@ -27,14 +27,14 @@ void tcp_plb_update_state(const struct sock *sk, struct tcp_plb_state *plb,
 {
 	struct net *net = sock_net(sk);
 
-	if (!plb->enabled)
+	if (!READ_ONCE(net->ipv4.sysctl_tcp_plb_enabled))
 		return;
 
 	if (cong_ratio >= 0) {
-		if (cong_ratio < net->ipv4.sysctl_tcp_plb_cong_thresh)
+		if (cong_ratio < READ_ONCE(net->ipv4.sysctl_tcp_plb_cong_thresh))
 			plb->consec_cong_rounds = 0;
 		else if (plb->consec_cong_rounds <
-			 net->ipv4.sysctl_tcp_plb_rehash_rounds)
+			 READ_ONCE(net->ipv4.sysctl_tcp_plb_rehash_rounds))
 			plb->consec_cong_rounds++;
 	}
 }
@@ -46,32 +46,39 @@ EXPORT_SYMBOL_GPL(tcp_plb_update_state);
 void tcp_plb_check_rehash(struct sock *sk, struct tcp_plb_state *plb)
 {
 	struct net *net = sock_net(sk);
-	bool can_idle_rehash, can_force_rehash;
+	u32 max_suspend;
+	bool forced_rehash = false, idle_rehash = false;
 
-	if (!plb->enabled)
+	if (!READ_ONCE(net->ipv4.sysctl_tcp_plb_enabled))
 		return;
 
-	/* Note that tcp_jiffies32 can wrap, so we clear pause_until
-	 * to 0 to indicate there is no recent RTO event that constrains
-	 * PLB rehashing.
+	forced_rehash = plb->consec_cong_rounds >=
+			READ_ONCE(net->ipv4.sysctl_tcp_plb_rehash_rounds);
+	/* If sender goes idle then we check whether to rehash. */
+	idle_rehash = READ_ONCE(net->ipv4.sysctl_tcp_plb_idle_rehash_rounds) &&
+		      !tcp_sk(sk)->packets_out &&
+		      plb->consec_cong_rounds >=
+		      READ_ONCE(net->ipv4.sysctl_tcp_plb_idle_rehash_rounds);
+
+	if (!forced_rehash && !idle_rehash)
+		return;
+
+	/* Note that tcp_jiffies32 can wrap; we detect wraps by checking for
+	 * cases where the max suspension end is before the actual suspension
+	 * end. We clear pause_until to 0 to indicate there is no recent
+	 * RTO event that constrains PLB rehashing.
 	 */
+	max_suspend = 2 * READ_ONCE(net->ipv4.sysctl_tcp_plb_suspend_rto_sec) * HZ;
 	if (plb->pause_until &&
-	    !before(tcp_jiffies32, plb->pause_until))
+	    (!before(tcp_jiffies32, plb->pause_until) ||
+	     before(tcp_jiffies32 + max_suspend, plb->pause_until)))
 		plb->pause_until = 0;
 
-	can_idle_rehash = net->ipv4.sysctl_tcp_plb_idle_rehash_rounds &&
-			  !tcp_sk(sk)->packets_out &&
-			  plb->consec_cong_rounds >=
-			  net->ipv4.sysctl_tcp_plb_idle_rehash_rounds;
-	can_force_rehash = plb->consec_cong_rounds >=
-			   net->ipv4.sysctl_tcp_plb_rehash_rounds;
+	if (plb->pause_until)
+		return;
 
-	if (!plb->pause_until && (can_idle_rehash || can_force_rehash)) {
-		sk_rethink_txhash(sk);
-		plb->consec_cong_rounds = 0;
-		tcp_sk(sk)->ecn_rehash++;
-		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPECNREHASH);
-	}
+	sk_rethink_txhash(sk);
+	plb->consec_cong_rounds = 0;
 }
 EXPORT_SYMBOL_GPL(tcp_plb_check_rehash);
 
@@ -84,10 +91,10 @@ void tcp_plb_update_state_upon_rto(struct sock *sk, struct tcp_plb_state *plb)
 	struct net *net = sock_net(sk);
 	u32 pause;
 
-	if (!plb->enabled)
+	if (!READ_ONCE(net->ipv4.sysctl_tcp_plb_enabled))
 		return;
 
-	pause = net->ipv4.sysctl_tcp_plb_suspend_rto_sec * HZ;
+	pause = READ_ONCE(net->ipv4.sysctl_tcp_plb_suspend_rto_sec) * HZ;
 	pause += prandom_u32_max(pause);
 	plb->pause_until = tcp_jiffies32 + pause;
 
diff --git a/net/ipv4/tcp_rate.c b/net/ipv4/tcp_rate.c
index 796fa6e5310c..c8de57c26c05 100644
--- a/net/ipv4/tcp_rate.c
+++ b/net/ipv4/tcp_rate.c
@@ -168,6 +168,10 @@ void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
 	/* delivered_ce occupies less than 32 bits in the skb control block */
 	rs->delivered_ce &= TCPCB_DELIVERED_CE_MASK;
 
+	rs->delivered_ce = tp->delivered_ce - rs->prior_delivered_ce;
+	/* delivered_ce occupies less than 32 bits in the skb control block */
+	rs->delivered_ce &= TCPCB_DELIVERED_CE_MASK;
+
 	/* Model sending data and receiving ACKs as separate pipeline phases
 	 * for a window. Usually the ACK phase is longer, but with ACK
 	 * compression the send phase can be longer. To be safe we use the
diff --git a/net/sctp/output.c b/net/sctp/output.c
index 1441eaf460bb..8e730287cfe7 100644
--- a/net/sctp/output.c
+++ b/net/sctp/output.c
@@ -133,7 +133,8 @@ void sctp_packet_config(struct sctp_packet *packet, __u32 vtag,
 		dst_hold(tp->dst);
 		sk_setup_caps(sk, tp->dst);
 	}
-	packet->max_size = sk_can_gso(sk) ? tp->dst->dev->gso_max_size
+	packet->max_size = sk_can_gso(sk) ? min(READ_ONCE(tp->dst->dev->gso_max_size),
+						GSO_LEGACY_MAX_SIZE)
 					  : asoc->pathmtu;
 	rcu_read_unlock();
 }
-- 
2.49.0

